{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import pytz\n",
    "import multiprocessing as mpp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numba\n",
    "#\n",
    "import pyspark\n",
    "import pyspark.sql.types as ps_types\n",
    "import h5py\n",
    "#\n",
    "# TODO: phase out unreferenced hpc_lib calls...\n",
    "import hpc_lib\n",
    "\n",
    "#\n",
    "#data_file_name = 'data/mazama_usage_20200506_tool8.out'\n",
    "#data_file_name = 'data/sacct_sherlock_out_serc2020_05_08.out'\n",
    "\n",
    "#data_file_name = 'data/serc_usage_20200914.out'\n",
    "data_file_name = 'data/sacct_mazama_20200820.out'\n",
    "data_file_name = 'data/sacct_owners_out_3500489.out'\n",
    "#\n",
    "pkl_name = \"{}.pkl\".format(os.path.splitext(data_file_name)[0])\n",
    "h5_name = \"{}.h5\".format(os.path.splitext(data_file_name)[0])\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark implementation of HPC_analytics (prototype)\n",
    "- Right now, a hybrid of vanilla `RDD` and `PySpark DataFrames`\n",
    "- Mixing a bit of using `SQLContext` context methods and whatever the native (??) Spark context.\n",
    "\n",
    "#### Brief sumary:\n",
    "PySpark is the Python implementation of Spark, which is a distributed data processing infrastructure. Spark should parallelize across multiple nodes, and so should be a better multi-processing option than `python.multiprocessing`. The syntax is not Pythonic. At all, so it's basically like writing another language in Python, and the workflow strategies are very different as well, so if you're a Python person, be prepared to pivot a bit.\n",
    "\n",
    "The biggest problem I'm still having is the final act of reading a very large data file into memory or transfering into a new (disk based) container. This, of course, should be simple, since it is really the fundamental and primary purpose of Spark, but alas... The problem arises when the distributed (Java) VMs exceed memory limitations. But we'll get there...\n",
    "\n",
    "#### 1. A quick look at the inpuit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  User|Group|GID|JobName|JobID|JobIDRaw|Partition|State|Timelimit|NCPUS|NNodes|Submit|Eligible|Start|End|Elapsed|SystemCPU|UserCPU|TotalCPU|NTasks|CPUTimeRAW|Suspended|ReqGRES|AllocGRES|ReqTRES|AllocTRES|\n",
      "\n",
      "**  tsy935|rubin|9133|dshap|57307350|57307350|owners|COMPLETED|2-00:00:00|12|1|2020-01-01T00:05:05|2020-01-01T00:07:06|2020-01-01T01:14:30|2020-01-01T17:18:46|16:04:16|2-21:34:59|2-16:33:26|5-14:08:26||694272|00:00:00|||billing=12,cpu=12,mem=192G,node=1|billing=12,cpu=12,energy=7143296,mem=192G,node=1|\n",
      "\n",
      "**  |||batch|57307350.batch|57307350.batch||COMPLETED||12|1|2020-01-01T01:14:30|2020-01-01T01:14:30|2020-01-01T01:14:30|2020-01-01T17:18:46|16:04:16|2-21:34:59|2-16:33:26|5-14:08:26|1|694272|00:00:00||||cpu=12,mem=192G,node=1|\n",
      "\n",
      "**  |||extern|57307350.extern|57307350.extern||COMPLETED||12|1|2020-01-01T01:14:30|2020-01-01T01:14:30|2020-01-01T01:14:30|2020-01-01T17:18:46|16:04:16|00:00.001|00:00:00|00:00.001|1|694272|00:00:00||||billing=12,cpu=12,mem=192G,node=1|\n",
      "\n",
      "**  tsy935|rubin|9133|dshap|57274801|57274801|owners|TIMEOUT|2-00:00:00|12|1|2020-01-01T00:45:43|2020-01-01T00:47:44|2020-01-01T02:32:18|2020-01-03T02:32:40|2-00:00:22|6-20:14:34|6-20:11:08|13-16:25:42||2073864|00:00:00|||billing=12,cpu=12,mem=192G,node=1|billing=12,cpu=12,mem=192G,node=1|\n",
      "\n",
      "**  |||batch|57274801.batch|57274801.batch||CANCELLED||12|1|2020-01-01T02:32:18|2020-01-01T02:32:18|2020-01-01T02:32:18|2020-01-03T02:32:42|2-00:00:24|6-20:14:34|6-20:11:08|13-16:25:42|1|2073888|00:00:00||||cpu=12,mem=192G,node=1|\n",
      "\n",
      "**  |||extern|57274801.extern|57274801.extern||COMPLETED||12|1|2020-01-01T02:32:18|2020-01-01T02:32:18|2020-01-01T02:32:18|2020-01-03T02:32:40|2-00:00:22|00:00.001|00:00:00|00:00.001|1|2073864|00:00:00||||billing=12,cpu=12,mem=192G,node=1|\n",
      "\n",
      "**  xiankong|jianq|282061|li|57395093|57395093|owners|FAILED|2-00:00:00|20|1|2020-01-01T00:47:28|2020-01-01T00:49:29|2020-01-01T00:51:21|2020-01-01T01:56:30|01:05:09|01:59:52|19:41:31|21:41:24||78180|00:00:00|||billing=20,cpu=20,mem=80000M,node=1|billing=20,cpu=20,energy=584664,mem=80000M,node=1|\n",
      "\n",
      "**  |||batch|57395093.batch|57395093.batch||FAILED||20|1|2020-01-01T00:51:21|2020-01-01T00:51:21|2020-01-01T00:51:21|2020-01-01T01:56:30|01:05:09|01:59:52|19:41:31|21:41:24|1|78180|00:00:00||||cpu=20,mem=80000M,node=1|\n",
      "\n",
      "**  |||extern|57395093.extern|57395093.extern||COMPLETED||20|1|2020-01-01T00:51:21|2020-01-01T00:51:21|2020-01-01T00:51:21|2020-01-01T01:56:30|01:05:09|00:00.001|00:00:00|00:00.001|1|78180|00:00:00||||billing=20,cpu=20,mem=80000M,node=1|\n",
      "\n",
      "**  xiankong|jianq|282061|li|57395087|57395087|owners|FAILED|2-00:00:00|20|1|2020-01-01T00:47:31|2020-01-01T00:49:32|2020-01-01T00:51:21|2020-01-01T01:48:02|00:56:41|02:09:43|16:42:36|18:52:19||68020|00:00:00|||billing=20,cpu=20,mem=80000M,node=1|billing=20,cpu=20,energy=507094,mem=80000M,node=1|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a quick look at the input data:\n",
    "#\n",
    "with open(data_file_name, 'r') as fin:\n",
    "    k=0\n",
    "    for rw in fin:\n",
    "        print('** ', rw)\n",
    "        k+=1\n",
    "        if k>10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate and configure some context handler objects.\n",
    "- There are a few...\n",
    "- The \"spark\" and \"sql\" variants seem to come from different branches of the project, or source projects, that have since merged, albeit perhaps not entirely gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build some working components:\n",
    "- Spark context objects\n",
    "- Type dictionary translator\n",
    "- Other type, etc. translator maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 6\n",
    "#\n",
    "# .config(\"spark.driver.memory\", \"15g\")\n",
    "#conf = pyspark.SparkConf('local[*]').set(\"spark.cores.max\", \"6\").set(\"spark.executor.instances\", \"4\").set(\"spark.executor.cores\",\"2\")\n",
    "conf = pyspark.SparkConf('local[{}]'.format(n_cpu)).set(\"spark.driver.memory\", \"16g\")\n",
    "#\n",
    "#conf = conf.set(\"spark.executor.memory\", \"4g\").set(\"spark.executor.pyspark.memory\", \"3g\")\n",
    "sc   = pyspark.SparkContext(conf=conf)\n",
    "#\n",
    "\n",
    "# also build a SQL context? We never ended up using this, though I think you can use\n",
    "#. it to do SQL querries on the data sets.\n",
    "sc_sql = pyspark.SQLContext(sc)\n",
    "#\n",
    "spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').master('local[{}]'.format(n_cpu)).config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "#spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').config(conf).getOrCreate()\n",
    "#\n",
    "#sc.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull some data structures, handler functions, etc. from relevant modules (ie `hpc_lib`). Note that eventually, we'll want to consolicate the `process_row()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** typex_dict:  {'User': <class 'str'>, 'JobID': <class 'str'>, 'JobName': <class 'str'>, 'Partition': <class 'str'>, 'State': <class 'str'>, 'JobID_parent': <class 'str'>, 'Timelimit': <function elapsed_time_2_day at 0x7fd091b2f700>, 'Start': <function str2date_num at 0x7fd091b2f5e0>, 'End': <function str2date_num at 0x7fd091b2f5e0>, 'Submit': <function str2date_num at 0x7fd091b2f5e0>, 'Eligible': <function str2date_num at 0x7fd091b2f5e0>, 'Elapsed': <function elapsed_time_2_day at 0x7fd091b2f700>, 'MaxRSS': <class 'str'>, 'MaxVMSize': <class 'str'>, 'NNodes': <class 'int'>, 'NCPUS': <class 'int'>, 'MinCPU': <class 'str'>, 'SystemCPU': <function elapsed_time_2_day at 0x7fd091b2f700>, 'UserCPU': <function elapsed_time_2_day at 0x7fd091b2f700>, 'TotalCPU': <function elapsed_time_2_day at 0x7fd091b2f700>, 'NTasks': <class 'int'>}\n"
     ]
    }
   ],
   "source": [
    "delim = '|'\n",
    "types_dict = hpc_lib.SACCT_data_handler.default_types_dict\n",
    "print('** typex_dict: ', types_dict)\n",
    "#\n",
    "# some numpy. (and other?) data types are not (well) supported by Spark, most pointedly some of the \n",
    "#. numpy.float{k} variants, so write a dictionary/map to re-type them. NOTE: this might need to include\n",
    "#. some numpy.int{k} types as well.\n",
    "numpy_re_typer={numpy.float64:float, numpy.float128:float, numpy.int64:int }\n",
    "#\n",
    "# python functions to use on grouped/aggregated columns:\n",
    "group_py_functions = {'End':numpy.nanmax, 'Start':numpy.nanmin, 'NCPUS':numpy.nanmax, 'NNodes':numpy.nanmax}\n",
    "#\n",
    "# Define one or more row processing functions.\n",
    "def f_rw(rw, header_names, RH_index, types_dict=types_dict, delim='|'):\n",
    "    #rws = rw[:-1].split(delim)\n",
    "    rws = rw.split(delim)\n",
    "    #\n",
    "    #if not len(rws)==0:\n",
    "    #    return rws\n",
    "    return [None if s=='' else types_dict.get(h,str)(s) for h,s in zip(header_names,rws)] + [rws[RH_index['JobID']].split('.')[0] ]\n",
    "    #return [str(s) for h,s in zip(header_names,rws)]\n",
    "#\n",
    "# this will be used to reduce() the raw data to summary data:\n",
    "def f_reduce_row(r1, r2, header_names, verbose=False):\n",
    "    if verbose:\n",
    "        print('*** DEBUG: lens: ', len(r1), len(r2), len(header_names))\n",
    "    return tuple([group_py_functions.get(hdr, lambda x: x[0] )([x1, x2]) \n",
    "             for k, (hdr,x1,x2) in enumerate(zip(header_names,\n",
    "                                *sorted([r1, r2], key=lambda X:X[RH_index['Submit']]))) ] )\n",
    "\n",
    "#\n",
    "def spark_types_to_numpy(spark_df):\n",
    "    # row1 = df1.agg({\"x\": \"max\"}).collect()[0]\n",
    "    #\n",
    "    # peel off the dtypes so it will pickle properly:\n",
    "    spark_dtypes = spark_df.dtypes\n",
    "    #\n",
    "    lens = spark_df.rdd.map(lambda rw: [max(1, len( (s or '') ) ) if spark_dtypes[k][1]=='string'\n",
    "                                else None for k,s in enumerate(rw) ] ).reduce(lambda x1,x2: max(x1,x2))\n",
    "    #\n",
    "    #print('*** stn: lens:: ', lens)\n",
    "    #\n",
    "    dtypes_out = []\n",
    "    for (nm,tp), s_len in zip(spark_dtypes, lens):\n",
    "        #print('*** DEBUG:: dtype[{},{}]: {}'.format(nm,tp,s_len))\n",
    "        if tp in ('int', 'bigint'):\n",
    "            tp_n = '>i8'\n",
    "            #tp_n = '>f8'\n",
    "            # do we need a casting function?\n",
    "            #nm_f = float\n",
    "        elif tp in ('float', 'double'):\n",
    "            tp_n = '>f8'\n",
    "        elif tp in ('string'):\n",
    "            tp_n = 'S{}'.format(s_len)\n",
    "        #\n",
    "        dtypes_out += [(nm, tp_n)]\n",
    "    #\n",
    "    return dtypes_out\n",
    "\n",
    "# numpy_to_spark_types = {str:'StringType'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminarily, read file into line-strings. Extract header row so that we\n",
    "#. can type() each column when we split() it up.\n",
    "#\n",
    "lines = sc.textFile(data_file_name)\n",
    "# header_names = (lines.take(1)[0])[:-1].split(delim)\n",
    "# RH_index = {s:k for k,s in enumerate(header_names) }\n",
    "# #\n",
    "# print('** Headers: ', header_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding header row from data collection:\n",
    "- This is surprisingly harder than it looks, and searching for solutions seems to be elusive.\n",
    "-  I think the most direct and common approach is to read text data via `textFile()`, then use filter() to exclude rows that look like the first row. This sounds kinda dumb for a single file, but it is a good option when reading multiple files.\n",
    "- *(spark) dataframes:* Use a syntactical variant of `spark.read.format('CSV')` method below to read the data into a dataframe. This provides options to exclude and catch the header row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes:\n",
    "- using the sql context, we can read the data into a dataframe\n",
    "- Nominally fast and easy, but I think really for well behaved data.\n",
    "- Getting the header row is not too tough, but I'm not so sure about excluding a false terminal column, resulting from a terminal delimeter (row string ending in a delimiter).\n",
    "- In fact, we seem to get some weird behavior from this\n",
    "- ... to the point that I would probably just err on the side of having more control and maybe burning some cycles on the filter() option (which i expect is pretty well optimized on the back end).\n",
    "- HOWEVER: Preliminary assessments just doing a `.count()` suggests that DF might be much, much faster than the standard RDD methods... Though that may also be because the `DataFramds` methods are using a context or session that is not CPU limited -- which would make sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Another way to read the file with headers. This will give an effective array of (val,ky) tuples.\n",
    "# df_rows1 = spark.read.format('CSV').option('header', 'true').option('sep', '|').load(data_file_name)\n",
    "# #\n",
    "# # another syntax:\n",
    "# df_rows = spark.read.csv(data_file_name, header=True, sep='|')\n",
    "# #\n",
    "\n",
    "# for rw in df_rows1.take(10):\n",
    "#     print('** ', rw[:])\n",
    "\n",
    "# # Another syntax to load directly into a spark dataframe (via .sql):\n",
    "# #\n",
    "# #\n",
    "# print('** type: ', type(rows_2))\n",
    "# print('** type: ', type(df_rows))\n",
    "# print('** dypes: ', df_rows.dtypes)\n",
    "# print('** header: {}'.format( df_rows.schema.names ) )\n",
    "# #\n",
    "# print('\\n*** *** ')\n",
    "# for rw in df_rows.take(5):\n",
    "#     print('** ', rw[:])\n",
    "#     #print('* * ', rw.head)\n",
    "#\n",
    "# print('*** ', df_rows.schema)\n",
    "# #\n",
    "# print('*** ', df_rows.dtypes)\n",
    "# #print('** ', set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** n_terminal:  1\n",
      "** Headers[27]: ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'ReqGRES', 'AllocGRES', 'ReqTRES', 'AllocTRES', 'JobID_parent']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# identifying the header row(s) and filtering bogus terminal characters on each row:\n",
    "n_terminal = 0\n",
    "header_string = lines.take(1)[0]\n",
    "#\n",
    "while header_string[-1] in ('\\n', delim):\n",
    "    header_string=header_string[:-1]\n",
    "    n_terminal += 1\n",
    "#\n",
    "# if necessary, trim off row-terminal characters:\n",
    "print('** n_terminal: ', n_terminal)\n",
    "if n_terminal>0:\n",
    "    lines = lines.map(lambda ln: ln[:-n_terminal])\n",
    "#\n",
    "header_names = header_string.split(delim) + ['JobID_parent']\n",
    "RH_index = {s:k for k,s in enumerate(header_names) }\n",
    "print('** Headers[{}]: {}'.format(len(header_names), header_names) )\n",
    "#\n",
    "# for c in (lines.take(2)[1]):\n",
    "#     print('{}: [{}]'.format(c, ord(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** end_max:  737608.6457986111\n"
     ]
    }
   ],
   "source": [
    "# use this to filter header rows:\n",
    "n_startswith = 15\n",
    "header_start = header_string[0:n_startswith]\n",
    "#\n",
    "rows = lines.filter(lambda s: not s.startswith(header_start) ).map(lambda x: f_rw(x, header_names,\n",
    "                                            types_dict=types_dict, RH_index=RH_index) )\n",
    "rows = rows.filter(lambda rw:rw[RH_index['JobName']]!='batch' and rw[RH_index['JobName']]!='extern' ).map(lambda rw: [None if (x is None or x=='') else numpy_re_typer.get(type(x), type(x))(x) for x in rw])\n",
    "#\n",
    "end_max = rows.map(lambda rw: rw[RH_index['End']]).filter(lambda x: x is not None).max()\n",
    "print('** end_max: ', end_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  [['tsy935', 'rubin', '9133', 'dshap', '57307350', '57307350', 'owners', 'COMPLETED', 2.0, 12, 1, 737425.0035300925, 737425.0049305556, 737425.051736111, 737425.7213657408, 0.6696296296296296, 2.8992939814814815, 2.689884259259259, 5.589189814814815, None, '694272', '00:00:00', None, None, 'billing=12,cpu=12,mem=192G,node=1', 'billing=12,cpu=12,energy=7143296,mem=192G,node=1', '57307350'], ['tsy935', 'rubin', '9133', 'dshap', '57274801', '57274801', 'owners', 'TIMEOUT', 2.0, 12, 1, 737425.0317476852, 737425.0331481481, 737425.1057638889, 737427.1060185186, 2.0002546296296297, 6.843449074074074, 6.841064814814815, 13.684513888888889, None, '2073864', '00:00:00', None, None, 'billing=12,cpu=12,mem=192G,node=1', 'billing=12,cpu=12,mem=192G,node=1', '57274801'], ['xiankong', 'jianq', '282061', 'li', '57395093', '57395093', 'owners', 'FAILED', 2.0, 20, 1, 737425.032962963, 737425.0343634259, 737425.0356597222, 737425.0809027777, 0.04524305555555556, 0.08324074074074074, 0.8204976851851852, 0.90375, None, '78180', '00:00:00', None, None, 'billing=20,cpu=20,mem=80000M,node=1', 'billing=20,cpu=20,energy=584664,mem=80000M,node=1', '57395093'], ['xiankong', 'jianq', '282061', 'li', '57395087', '57395087', 'owners', 'FAILED', 2.0, 20, 1, 737425.0329976851, 737425.0343981482, 737425.0356597222, 737425.0750231481, 0.03936342592592593, 0.09008101851851852, 0.69625, 0.7863310185185185, None, '68020', '00:00:00', None, None, 'billing=20,cpu=20,mem=80000M,node=1', 'billing=20,cpu=20,energy=507094,mem=80000M,node=1', '57395087'], ['xiankong', 'jianq', '282061', 'li', '57395099', '57395099', 'owners', 'FAILED', 2.0, 20, 1, 737425.0348958333, 737425.0362962963, 737425.0390162037, 737425.1398379629, 0.10082175925925926, 0.16931712962962964, 1.8453472222222222, 2.014664351851852, None, '174220', '00:00:00', None, None, 'billing=20,cpu=20,mem=80000M,node=1', 'billing=20,cpu=20,energy=1246721,mem=80000M,node=1', '57395099'], ['sahiner', 'pilanci', '306675', 'run_cifar100.sh', '57225527', '57225527', 'owners', 'COMPLETED', 0.5, 1, 1, 737425.0595138889, 737425.0609143518, 737425.1762152778, 737425.2790625, 0.10284722222222223, 0.003997303240740741, 0.09858796296296296, 0.10258101851851852, None, '8886', '00:00:00', 'gpu:4', 'gpu:4', 'billing=1,cpu=1,gres/gpu=4,mem=64G,node=1', 'billing=1,cpu=1,energy=851658,gres/gpu=4,mem=64G,node=1', '57225527'], ['davidama', 'euan', '11886', 'HC404_vs_HC164.sh', '57397256', '57397256', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.1504861111, 0.007847222222222222, 3.824074074074074e-05, 0.007383298611111111, 0.007421550925925926, None, '678', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=116914,mem=4000M,node=1', '57397256'], ['davidama', 'euan', '11886', 'HC132_vs_Cystatin_C.sh', '57397257', '57397257', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.1479166667, 0.005277777777777778, 4.012731481481482e-05, 0.004848298611111111, 0.004888425925925926, None, '456', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=77047,mem=4000M,node=1', '57397257'], ['davidama', 'euan', '11886', 'HC404_vs_HC132.sh', '57397258', '57397258', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.152662037, 0.010023148148148147, 9.020833333333334e-05, 0.009535694444444444, 0.00962591435185185, None, '866', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=148047,mem=4000M,node=1', '57397258'], ['davidama', 'euan', '11886', 'HC132_vs_HC221.sh', '57397259', '57397259', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.1587962963, 0.01615740740740741, 0.00018601851851851852, 0.015446678240740742, 0.015632696759259258, None, '1396', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=207179,mem=4000M,node=1', '57397259']]\n"
     ]
    }
   ],
   "source": [
    "rows = rows.map(lambda rw: \n",
    "                rw[0:RH_index['End']] + [end_max] + rw[RH_index['End']+1:] \n",
    "                if rw[RH_index['End']] is None else rw  )\n",
    "print('** ', rows.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** lens:  27 27\n",
      "** header_names:  [(0, 'User'), (1, 'Group'), (2, 'GID'), (3, 'JobName'), (4, 'JobID'), (5, 'JobIDRaw'), (6, 'Partition'), (7, 'State'), (8, 'Timelimit'), (9, 'NCPUS'), (10, 'NNodes'), (11, 'Submit'), (12, 'Eligible'), (13, 'Start'), (14, 'End'), (15, 'Elapsed'), (16, 'SystemCPU'), (17, 'UserCPU'), (18, 'TotalCPU'), (19, 'NTasks'), (20, 'CPUTimeRAW'), (21, 'Suspended'), (22, 'ReqGRES'), (23, 'AllocGRES'), (24, 'ReqTRES'), (25, 'AllocTRES'), (26, 'JobID_parent')]\n",
      "** row1:  [(0, 'tsy935'), (1, 'rubin'), (2, '9133'), (3, 'dshap'), (4, '57307350'), (5, '57307350'), (6, 'owners'), (7, 'COMPLETED'), (8, 2.0), (9, 12), (10, 1), (11, 737425.0035300925), (12, 737425.0049305556), (13, 737425.051736111), (14, 737425.7213657408), (15, 0.6696296296296296), (16, 2.8992939814814815), (17, 2.689884259259259), (18, 5.589189814814815), (19, None), (20, '694272'), (21, '00:00:00'), (22, None), (23, None), (24, 'billing=12,cpu=12,mem=192G,node=1'), (25, 'billing=12,cpu=12,energy=7143296,mem=192G,node=1'), (26, '57307350')]\n",
      "** lines hdr:  User|Group|GID|JobName|JobID|JobIDRaw|Partition|State|Timelimit|NCPUS|NNodes|Submit|Eligible|Start|End|Elapsed|SystemCPU|UserCPU|TotalCPU|NTasks|CPUTimeRAW|Suspended|ReqGRES|AllocGRES|ReqTRES|AllocTRES\n",
      "** lines1:  tsy935|rubin|9133|dshap|57307350|57307350|owners|COMPLETED|2-00:00:00|12|1|2020-01-01T00:05:05|2020-01-01T00:07:06|2020-01-01T01:14:30|2020-01-01T17:18:46|16:04:16|2-21:34:59|2-16:33:26|5-14:08:26||694272|00:00:00|||billing=12,cpu=12,mem=192G,node=1|billing=12,cpu=12,energy=7143296,mem=192G,node=1\n",
      "**  26\n",
      "**  26\n"
     ]
    }
   ],
   "source": [
    "print('** lens: ', len(rows.take(1)[0]), len(header_names))\n",
    "print('** header_names: ' , [(k,h) for k,h in enumerate(header_names) ])\n",
    "print('** row1: ', [(k,c) for k,c in enumerate(rows.take(1)[0])])\n",
    "l1,l2 = lines.take(2)\n",
    "print('** lines hdr: ', l1)\n",
    "print('** lines1: ', l2)\n",
    "print('** ', len(lines.take(1)[0].split('|')) )\n",
    "print('** ', len(lines.take(2)[1].split('|')) )\n",
    "#\n",
    "# we can either group and reduce using the RH{} index, or convert to a DF first:\n",
    "#rows_df = spark.createDataFrame( rows, header_names ).sort('JobID')\n",
    "#\n",
    "# print('** rows_df schema[{}]: {}'.format(len(rows_df.schema), rows_df.schema))\n",
    "# print('** dytpes[{}]: {}'.format(len(rows_df.dtypes), rows_df.dtypes))\n",
    "# print('** header_names[{}]: {}'.format(len(header_names), header_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  ['tsy935', 'rubin', '9133', 'dshap', '57307350', '57307350', 'owners', 'COMPLETED', 2.0, 12, 1, 737425.0035300925, 737425.0049305556, 737425.051736111, 737425.7213657408, 0.6696296296296296, 2.8992939814814815, 2.689884259259259, 5.589189814814815, None, '694272', '00:00:00', None, None, 'billing=12,cpu=12,mem=192G,node=1', 'billing=12,cpu=12,energy=7143296,mem=192G,node=1', '57307350']\n",
      "**  ['tsy935', 'rubin', '9133', 'dshap', '57274801', '57274801', 'owners', 'TIMEOUT', 2.0, 12, 1, 737425.0317476852, 737425.0331481481, 737425.1057638889, 737427.1060185186, 2.0002546296296297, 6.843449074074074, 6.841064814814815, 13.684513888888889, None, '2073864', '00:00:00', None, None, 'billing=12,cpu=12,mem=192G,node=1', 'billing=12,cpu=12,mem=192G,node=1', '57274801']\n",
      "**  ['xiankong', 'jianq', '282061', 'li', '57395093', '57395093', 'owners', 'FAILED', 2.0, 20, 1, 737425.032962963, 737425.0343634259, 737425.0356597222, 737425.0809027777, 0.04524305555555556, 0.08324074074074074, 0.8204976851851852, 0.90375, None, '78180', '00:00:00', None, None, 'billing=20,cpu=20,mem=80000M,node=1', 'billing=20,cpu=20,energy=584664,mem=80000M,node=1', '57395093']\n",
      "**  ['xiankong', 'jianq', '282061', 'li', '57395087', '57395087', 'owners', 'FAILED', 2.0, 20, 1, 737425.0329976851, 737425.0343981482, 737425.0356597222, 737425.0750231481, 0.03936342592592593, 0.09008101851851852, 0.69625, 0.7863310185185185, None, '68020', '00:00:00', None, None, 'billing=20,cpu=20,mem=80000M,node=1', 'billing=20,cpu=20,energy=507094,mem=80000M,node=1', '57395087']\n",
      "**  ['xiankong', 'jianq', '282061', 'li', '57395099', '57395099', 'owners', 'FAILED', 2.0, 20, 1, 737425.0348958333, 737425.0362962963, 737425.0390162037, 737425.1398379629, 0.10082175925925926, 0.16931712962962964, 1.8453472222222222, 2.014664351851852, None, '174220', '00:00:00', None, None, 'billing=20,cpu=20,mem=80000M,node=1', 'billing=20,cpu=20,energy=1246721,mem=80000M,node=1', '57395099']\n",
      "**  ['sahiner', 'pilanci', '306675', 'run_cifar100.sh', '57225527', '57225527', 'owners', 'COMPLETED', 0.5, 1, 1, 737425.0595138889, 737425.0609143518, 737425.1762152778, 737425.2790625, 0.10284722222222223, 0.003997303240740741, 0.09858796296296296, 0.10258101851851852, None, '8886', '00:00:00', 'gpu:4', 'gpu:4', 'billing=1,cpu=1,gres/gpu=4,mem=64G,node=1', 'billing=1,cpu=1,energy=851658,gres/gpu=4,mem=64G,node=1', '57225527']\n",
      "**  ['davidama', 'euan', '11886', 'HC404_vs_HC164.sh', '57397256', '57397256', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.1504861111, 0.007847222222222222, 3.824074074074074e-05, 0.007383298611111111, 0.007421550925925926, None, '678', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=116914,mem=4000M,node=1', '57397256']\n",
      "**  ['davidama', 'euan', '11886', 'HC132_vs_Cystatin_C.sh', '57397257', '57397257', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.1479166667, 0.005277777777777778, 4.012731481481482e-05, 0.004848298611111111, 0.004888425925925926, None, '456', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=77047,mem=4000M,node=1', '57397257']\n",
      "**  ['davidama', 'euan', '11886', 'HC404_vs_HC132.sh', '57397258', '57397258', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.152662037, 0.010023148148148147, 9.020833333333334e-05, 0.009535694444444444, 0.00962591435185185, None, '866', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=148047,mem=4000M,node=1', '57397258']\n",
      "**  ['davidama', 'euan', '11886', 'HC132_vs_HC221.sh', '57397259', '57397259', 'owners', 'COMPLETED', 0.20833333333333334, 1, 1, 737425.127349537, 737425.127349537, 737425.1426388889, 737425.1587962963, 0.01615740740740741, 0.00018601851851851852, 0.015446678240740742, 0.015632696759259258, None, '1396', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=207179,mem=4000M,node=1', '57397259']\n",
      "reduced: \n",
      "**  ('57399044_375', ['pjwomble', 'gorelick', '26961', '21c362e8b66f1', '57399044_375', '57399423', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2385879629, 737425.2386111111, 737425.2450578704, 737425.2563773148, 0.011319444444444444, 9.321759259259259e-05, 0.010857499999999999, 0.010950729166666668, None, '978', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=171454,mem=4000M,node=1', '57399044_375'])\n",
      "**  ('57399898_430', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_430', '57400329', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2818287037, 737425.2933680556, 0.011539351851851851, 9.383101851851851e-05, 0.01118962962962963, 0.011283460648148148, None, '997', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=174940,mem=4000M,node=1', '57399898_430'])\n",
      "**  ('57399898_505', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_505', '57400404', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.282337963, 737425.2937847222, 0.011446759259259259, 9.872685185185185e-05, 0.011193981481481482, 0.011292708333333333, None, '989', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=121431,mem=4000M,node=1', '57399898_505'])\n",
      "**  ('57399898_566', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_566', '57400466', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2838541666, 737425.2955092592, 0.011655092592592592, 0.00010068287037037036, 0.011054791666666668, 0.01115548611111111, None, '1007', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=164776,mem=4000M,node=1', '57399898_566'])\n",
      "**  ('57399898_597', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_597', '57400498', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2843402778, 737425.2960300926, 0.011689814814814814, 0.00014809027777777778, 0.011178576388888889, 0.011326666666666667, None, '1010', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=107528,mem=4000M,node=1', '57399898_597'])\n",
      "**  ('57399898_665', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_665', '57400566', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2848032408, 737425.2971412037, 0.012337962962962964, 0.00010297453703703704, 0.011976006944444445, 0.012078981481481482, None, '1066', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=165654,mem=4000M,node=1', '57399898_665'])\n",
      "**  ('57400920_481', ['pjwomble', 'gorelick', '26961', '21c366c54dbcc', '57400920_481', '57401404', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3160763889, 737425.3160995371, 737425.3176620371, 737425.329363426, 0.01170138888888889, 9.726851851851852e-05, 0.011243877314814814, 0.011341157407407407, None, '1011', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=178752,mem=4000M,node=1', '57400920_481'])\n",
      "**  ('57400920_646', ['pjwomble', 'gorelick', '26961', '21c366c54dbcc', '57400920_646', '57401569', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3160763889, 737425.3160995371, 737425.3191319444, 737425.3306597222, 0.011527777777777777, 9.556712962962962e-05, 0.011154699074074074, 0.011250277777777777, None, '996', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=116407,mem=4000M,node=1', '57400920_646'])\n",
      "**  ('57401745_644', ['pjwomble', 'gorelick', '26961', '21c364517c49e', '57401745_644', '57402466', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3483333334, 737425.3483564815, 737425.3508217592, 737425.3627314814, 0.011909722222222223, 9.947916666666667e-05, 0.011283043981481482, 0.011382523148148149, None, '1029', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=183716,mem=4000M,node=1', '57401745_644'])\n",
      "**  ('57401745_684', ['pjwomble', 'gorelick', '26961', '21c364517c49e', '57401745_684', '57402506', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3483333334, 737425.3483564815, 737425.3508217592, 737425.3626273149, 0.011805555555555555, 9.159722222222222e-05, 0.011156493055555557, 0.011248090277777779, None, '1020', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=164552,mem=4000M,node=1', '57401745_684'])\n"
     ]
    }
   ],
   "source": [
    "# Here, let's try a RDD_pair, then reduce_by_key() function:\n",
    "# (this appears to work, but needs to be validated )\n",
    "row_pairs = rows.map(lambda x: (x[RH_index['JobID_parent']], list(x[:])))\n",
    "#\n",
    "summary_rdd = row_pairs.reduceByKey(lambda rw1, rw2: f_reduce_row(rw1, rw2, header_names))\n",
    "#\n",
    "\n",
    "for rw in rows.take(10):\n",
    "    print('** ', rw)\n",
    "#\n",
    "print('reduced: ')\n",
    "for rw in summary_rdd.take(10):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1, r2 = rows.take(2)\n",
    "# print('** ', r1)\n",
    "# print('** ', r2)\n",
    "# #\n",
    "# rr = f_reduce_row(r1, r2, header_names, True)\n",
    "# print('** ** ', rr)\n",
    "\n",
    "\n",
    "# N=1000\n",
    "# #rows_df = spark.createDataFrame( rows.takeSample(withReplacement=True, num=N), header_names )\n",
    "# rows_df = spark.createDataFrame( rows.take(num=N), header_names )\n",
    "\n",
    "# typex_dict:  {'User': <class 'str'>, 'JobID': <class 'str'>, 'JobName': <class 'str'>, \n",
    "#               'Partition': <class 'str'>, 'State': <class 'str'>, 'JobID_parent': <class 'str'>, \n",
    "#               'Timelimit': <function elapsed_time_2_day at 0x7f035ef83790>, \n",
    "#               'Start': <function str2date_num at 0x7f035ef83670>, \n",
    "#               'End': <function str2date_num at 0x7f035ef83670>, \n",
    "#               'Submit': <function str2date_num at 0x7f035ef83670>, \n",
    "#               'Eligible': <function str2date_num at 0x7f035ef83670>, \n",
    "#               'Elapsed': <function elapsed_time_2_day at 0x7f035ef83790>, \n",
    "#               'MaxRSS': <class 'str'>, 'MaxVMSize': <class 'str'>, 'NNodes': <class 'int'>, \n",
    "#               'NCPUS': <class 'int'>, 'MinCPU': <class 'str'>, \n",
    "#               'SystemCPU': <function elapsed_time_2_day at 0x7f035ef83790>, \n",
    "#               'UserCPU': <function elapsed_time_2_day at 0x7f035ef83790>, \n",
    "#               'TotalCPU': <function elapsed_time_2_day at 0x7f035ef83790>, 'NTasks': <class 'int'>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  StructType(List(StructField(User,StringType,true),StructField(Group,StringType,true),StructField(GID,StringType,true),StructField(JobName,StringType,true),StructField(JobID,StringType,true),StructField(JobIDRaw,StringType,true),StructField(Partition,StringType,true),StructField(State,StringType,true),StructField(Timelimit,DoubleType,true),StructField(NCPUS,LongType,true),StructField(NNodes,LongType,true),StructField(Submit,DoubleType,true),StructField(Eligible,DoubleType,true),StructField(Start,DoubleType,true),StructField(End,DoubleType,true),StructField(Elapsed,DoubleType,true),StructField(SystemCPU,DoubleType,true),StructField(UserCPU,DoubleType,true),StructField(TotalCPU,DoubleType,true),StructField(NTasks,LongType,true),StructField(CPUTimeRAW,StringType,true),StructField(Suspended,StringType,true),StructField(ReqGRES,StringType,true),StructField(AllocGRES,StringType,true),StructField(ReqTRES,StringType,true),StructField(AllocTRES,StringType,true),StructField(JobID_parent,StringType,true)))\n",
      "**  ('57399044_375', ['pjwomble', 'gorelick', '26961', '21c362e8b66f1', '57399044_375', '57399423', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2385879629, 737425.2386111111, 737425.2450578704, 737425.2563773148, 0.011319444444444444, 9.321759259259259e-05, 0.010857499999999999, 0.010950729166666668, None, '978', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=171454,mem=4000M,node=1', '57399044_375'])\n",
      "**  ('57399898_430', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_430', '57400329', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2818287037, 737425.2933680556, 0.011539351851851851, 9.383101851851851e-05, 0.01118962962962963, 0.011283460648148148, None, '997', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=174940,mem=4000M,node=1', '57399898_430'])\n",
      "**  ('57399898_505', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_505', '57400404', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.282337963, 737425.2937847222, 0.011446759259259259, 9.872685185185185e-05, 0.011193981481481482, 0.011292708333333333, None, '989', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=121431,mem=4000M,node=1', '57399898_505'])\n",
      "**  ('57399898_566', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_566', '57400466', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2838541666, 737425.2955092592, 0.011655092592592592, 0.00010068287037037036, 0.011054791666666668, 0.01115548611111111, None, '1007', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=164776,mem=4000M,node=1', '57399898_566'])\n",
      "**  ('57399898_597', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_597', '57400498', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2843402778, 737425.2960300926, 0.011689814814814814, 0.00014809027777777778, 0.011178576388888889, 0.011326666666666667, None, '1010', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=107528,mem=4000M,node=1', '57399898_597'])\n",
      "**  ('57399898_665', ['pjwomble', 'gorelick', '26961', '21c366f3edba2', '57399898_665', '57400566', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.2802314815, 737425.2802430555, 737425.2848032408, 737425.2971412037, 0.012337962962962964, 0.00010297453703703704, 0.011976006944444445, 0.012078981481481482, None, '1066', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=165654,mem=4000M,node=1', '57399898_665'])\n",
      "**  ('57400920_481', ['pjwomble', 'gorelick', '26961', '21c366c54dbcc', '57400920_481', '57401404', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3160763889, 737425.3160995371, 737425.3176620371, 737425.329363426, 0.01170138888888889, 9.726851851851852e-05, 0.011243877314814814, 0.011341157407407407, None, '1011', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=178752,mem=4000M,node=1', '57400920_481'])\n",
      "**  ('57400920_646', ['pjwomble', 'gorelick', '26961', '21c366c54dbcc', '57400920_646', '57401569', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3160763889, 737425.3160995371, 737425.3191319444, 737425.3306597222, 0.011527777777777777, 9.556712962962962e-05, 0.011154699074074074, 0.011250277777777777, None, '996', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=116407,mem=4000M,node=1', '57400920_646'])\n",
      "**  ('57401745_644', ['pjwomble', 'gorelick', '26961', '21c364517c49e', '57401745_644', '57402466', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3483333334, 737425.3483564815, 737425.3508217592, 737425.3627314814, 0.011909722222222223, 9.947916666666667e-05, 0.011283043981481482, 0.011382523148148149, None, '1029', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=183716,mem=4000M,node=1', '57401745_644'])\n",
      "**  ('57401745_684', ['pjwomble', 'gorelick', '26961', '21c364517c49e', '57401745_684', '57402506', 'owners', 'COMPLETED', 0.04097222222222222, 1, 1, 737425.3483333334, 737425.3483564815, 737425.3508217592, 737425.3626273149, 0.011805555555555555, 9.159722222222222e-05, 0.011156493055555557, 0.011248090277777779, None, '1020', '00:00:00', None, None, 'billing=1,cpu=1,mem=4000M,node=1', 'billing=1,cpu=1,energy=164552,mem=4000M,node=1', '57401745_684'])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# make a sampling DataFrame() to automagically determine the schema (this breaks when we do just \n",
    "#. the summary... which suggests maybe a problem -- NTasks is not populating correctly maybe?)\n",
    "# otherwise, we need to write some more translator dictionaries to map numpy types to Spark types\n",
    "#\n",
    "# for n in range(2,7):\n",
    "#     N=int(10**n)\n",
    "#     try:\n",
    "#         rows_df = spark.createDataFrame( rows.take(N), header_names )\n",
    "#         break\n",
    "#     except:\n",
    "#         # try a bigger sample.\n",
    "#         print('broke for [{}]. trying a bigger sample.'.format(N))\n",
    "#\n",
    "#sp_string='string'\n",
    "sp_string = ps_types.StringType\n",
    "#sp_float  = ps_types.FloatType()\n",
    "sp_float  = ps_types.DoubleType\n",
    "#sp_int    = ps_types.IntegerType()\n",
    "sp_int    = ps_types.LongType\n",
    "#\n",
    "spark_data_types={'User':sp_string, 'Group':sp_string, 'GID':sp_string, 'JobName':sp_string, 'JobID':sp_string,\n",
    "              'JobIDRaw':sp_string, 'Partition':sp_string, 'State':sp_string, 'Timelimit':sp_float,\n",
    "              'NCPUS':sp_int, 'NNodes':sp_int, 'Submit':sp_float, 'Eligible':sp_float, 'Start':sp_float,\n",
    "              'End':sp_float, 'Elapsed':sp_float, 'SystemCPU':sp_float, \n",
    "              'UserCPU':sp_float, 'TotalCPU':sp_float, 'NTasks':sp_int, 'CPUTimeRAW':sp_string,\n",
    "              'Suspended':sp_string, 'ReqGRES':sp_string, 'AllocGRES':sp_string, 'JobID_parent':sp_string}\n",
    "#\n",
    "#df_schema = [(col, spark_dtypes.get(col,  sp_string)) for col in header_names]\n",
    "df_schema = ps_types.StructType([ps_types.StructField(col, spark_data_types.get(col,  sp_string)() ) \n",
    "                                 for col in header_names])\n",
    "\n",
    "print('** ', df_schema)\n",
    "\n",
    "# rows_df = spark.createDataFrame( rows, header_names ).sort('JobID')\n",
    "summary_df = spark.createDataFrame(summary_rdd.values().map(lambda rw: \n",
    "                            [None if (x is None or x=='') else numpy_re_typer.get(type(x),\n",
    "                                lambda a:a)(x) for x in rw]),\n",
    "                                   df_schema)\n",
    "#\n",
    "for rw in summary_rdd.take(10):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rw in summary_df.take(10):\n",
    "#     print('** ', rw[:])\n",
    "\n",
    "# dt_dict = dict(summary_df.dtypes)\n",
    "# sdf_dtypes = summary_df.dtypes\n",
    "# #\n",
    "# print('** ', sdf_dtypes)\n",
    "# #lens = summary_df.rdd.map(lambda rw: [len(s) for k,s in enumerate(rw) if sdf_dtypes[k][1]=='string'] ).reduce(lambda x1,x2: max(x1,x2))\n",
    "# lens = summary_df.rdd.map(lambda rw: [len(s) if sdf_dtypes[k][1]=='string' else None for k,s in enumerate(rw) ] ).reduce(lambda x1,x2: max(x1,x2))\n",
    "# print('** lens: ', lens)\n",
    "\n",
    "\n",
    "\n",
    "# # for rw in lens.take(10):\n",
    "# #     print('** ', rw)\n",
    "# print('** ')\n",
    "# for (cl,n),l in zip(summary_df.dtypes, lens):\n",
    "#     print('** ', cl,n,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  [('User', 'string'), ('Group', 'string'), ('GID', 'string'), ('JobName', 'string'), ('JobID', 'string'), ('JobIDRaw', 'string'), ('Partition', 'string'), ('State', 'string'), ('Timelimit', 'double'), ('NCPUS', 'bigint'), ('NNodes', 'bigint'), ('Submit', 'double'), ('Eligible', 'double'), ('Start', 'double'), ('End', 'double'), ('Elapsed', 'double'), ('SystemCPU', 'double'), ('UserCPU', 'double'), ('TotalCPU', 'double'), ('NTasks', 'bigint'), ('CPUTimeRAW', 'string'), ('Suspended', 'string'), ('ReqGRES', 'string'), ('AllocGRES', 'string'), ('ReqTRES', 'string'), ('AllocTRES', 'string'), ('JobID_parent', 'string')]\n",
      "**  ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'ReqGRES', 'AllocGRES', 'ReqTRES', 'AllocTRES', 'JobID_parent']\n",
      "** \n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c362e8b66f1', JobID='57399044_375', JobIDRaw='57399423', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.2385879629, Eligible=737425.2386111111, Start=737425.2450578704, End=737425.2563773148, Elapsed=0.011319444444444444, SystemCPU=9.321759259259259e-05, UserCPU=0.010857499999999999, TotalCPU=0.010950729166666668, NTasks=None, CPUTimeRAW='978', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=171454,mem=4000M,node=1', JobID_parent='57399044_375')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366f3edba2', JobID='57399898_430', JobIDRaw='57400329', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.2802314815, Eligible=737425.2802430555, Start=737425.2818287037, End=737425.2933680556, Elapsed=0.011539351851851851, SystemCPU=9.383101851851851e-05, UserCPU=0.01118962962962963, TotalCPU=0.011283460648148148, NTasks=None, CPUTimeRAW='997', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=174940,mem=4000M,node=1', JobID_parent='57399898_430')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366f3edba2', JobID='57399898_505', JobIDRaw='57400404', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.2802314815, Eligible=737425.2802430555, Start=737425.282337963, End=737425.2937847222, Elapsed=0.011446759259259259, SystemCPU=9.872685185185185e-05, UserCPU=0.011193981481481482, TotalCPU=0.011292708333333333, NTasks=None, CPUTimeRAW='989', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=121431,mem=4000M,node=1', JobID_parent='57399898_505')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366f3edba2', JobID='57399898_566', JobIDRaw='57400466', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.2802314815, Eligible=737425.2802430555, Start=737425.2838541666, End=737425.2955092592, Elapsed=0.011655092592592592, SystemCPU=0.00010068287037037036, UserCPU=0.011054791666666668, TotalCPU=0.01115548611111111, NTasks=None, CPUTimeRAW='1007', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=164776,mem=4000M,node=1', JobID_parent='57399898_566')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366f3edba2', JobID='57399898_597', JobIDRaw='57400498', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.2802314815, Eligible=737425.2802430555, Start=737425.2843402778, End=737425.2960300926, Elapsed=0.011689814814814814, SystemCPU=0.00014809027777777778, UserCPU=0.011178576388888889, TotalCPU=0.011326666666666667, NTasks=None, CPUTimeRAW='1010', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=107528,mem=4000M,node=1', JobID_parent='57399898_597')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366f3edba2', JobID='57399898_665', JobIDRaw='57400566', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.2802314815, Eligible=737425.2802430555, Start=737425.2848032408, End=737425.2971412037, Elapsed=0.012337962962962964, SystemCPU=0.00010297453703703704, UserCPU=0.011976006944444445, TotalCPU=0.012078981481481482, NTasks=None, CPUTimeRAW='1066', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=165654,mem=4000M,node=1', JobID_parent='57399898_665')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366c54dbcc', JobID='57400920_481', JobIDRaw='57401404', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.3160763889, Eligible=737425.3160995371, Start=737425.3176620371, End=737425.329363426, Elapsed=0.01170138888888889, SystemCPU=9.726851851851852e-05, UserCPU=0.011243877314814814, TotalCPU=0.011341157407407407, NTasks=None, CPUTimeRAW='1011', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=178752,mem=4000M,node=1', JobID_parent='57400920_481')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c366c54dbcc', JobID='57400920_646', JobIDRaw='57401569', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.3160763889, Eligible=737425.3160995371, Start=737425.3191319444, End=737425.3306597222, Elapsed=0.011527777777777777, SystemCPU=9.556712962962962e-05, UserCPU=0.011154699074074074, TotalCPU=0.011250277777777777, NTasks=None, CPUTimeRAW='996', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=116407,mem=4000M,node=1', JobID_parent='57400920_646')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c364517c49e', JobID='57401745_644', JobIDRaw='57402466', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.3483333334, Eligible=737425.3483564815, Start=737425.3508217592, End=737425.3627314814, Elapsed=0.011909722222222223, SystemCPU=9.947916666666667e-05, UserCPU=0.011283043981481482, TotalCPU=0.011382523148148149, NTasks=None, CPUTimeRAW='1029', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=183716,mem=4000M,node=1', JobID_parent='57401745_644')\n",
      "**  Row(User='pjwomble', Group='gorelick', GID='26961', JobName='21c364517c49e', JobID='57401745_684', JobIDRaw='57402506', Partition='owners', State='COMPLETED', Timelimit=0.04097222222222222, NCPUS=1, NNodes=1, Submit=737425.3483333334, Eligible=737425.3483564815, Start=737425.3508217592, End=737425.3626273149, Elapsed=0.011805555555555555, SystemCPU=9.159722222222222e-05, UserCPU=0.011156493055555557, TotalCPU=0.011248090277777779, NTasks=None, CPUTimeRAW='1020', Suspended='00:00:00', ReqGRES=None, AllocGRES=None, ReqTRES='billing=1,cpu=1,mem=4000M,node=1', AllocTRES='billing=1,cpu=1,energy=164552,mem=4000M,node=1', JobID_parent='57401745_684')\n"
     ]
    }
   ],
   "source": [
    "print('** ', summary_df.dtypes)\n",
    "#spark_to_numpy_types('double':float, 'float':float, 'bigint':int, 'int':int, 'string':str)\n",
    "#\n",
    "print('** ', header_names)\n",
    "#\n",
    "print('** ')\n",
    "for rw in summary_df.take(10):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "numpy_dtypes = spark_types_to_numpy(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** numpy_dfs:  [('User', 'S8'), ('Group', 'S8'), ('GID', 'S6'), ('JobName', 'S39'), ('JobID', 'S8'), ('JobIDRaw', 'S8'), ('Partition', 'S6'), ('State', 'S9'), ('Timelimit', '>f8'), ('NCPUS', '>i8'), ('NNodes', '>i8'), ('Submit', '>f8'), ('Eligible', '>f8'), ('Start', '>f8'), ('End', '>f8'), ('Elapsed', '>f8'), ('SystemCPU', '>f8'), ('UserCPU', '>f8'), ('TotalCPU', '>f8'), ('NTasks', '>i8'), ('CPUTimeRAW', 'S3'), ('Suspended', 'S8'), ('ReqGRES', 'S1'), ('AllocGRES', 'S1'), ('ReqTRES', 'S30'), ('AllocTRES', 'S43'), ('JobID_parent', 'S8')]\n"
     ]
    }
   ],
   "source": [
    "array_len = summary_df.count()\n",
    "#\n",
    "print('** numpy_dfs: ', numpy_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** col: User :: S8\n",
      "** col: Group :: S8\n",
      "** col: GID :: S6\n",
      "** col: JobName :: S39\n",
      "** col: JobID :: S8\n",
      "** col: JobIDRaw :: S8\n",
      "** col: Partition :: S6\n",
      "** col: State :: S9\n",
      "** col: Timelimit :: >f8\n",
      "** col: NCPUS :: >i8\n",
      "** col: NNodes :: >i8\n",
      "** col: Submit :: >f8\n",
      "** col: Eligible :: >f8\n",
      "** col: Start :: >f8\n",
      "** col: End :: >f8\n",
      "** col: Elapsed :: >f8\n",
      "** col: SystemCPU :: >f8\n",
      "** col: UserCPU :: >f8\n",
      "** col: TotalCPU :: >f8\n",
      "** col: NTasks :: >i8\n",
      "** col: CPUTimeRAW :: S3\n",
      "** col: Suspended :: S8\n",
      "** col: ReqGRES :: S1\n",
      "** col: AllocGRES :: S1\n",
      "** col: ReqTRES :: S30\n",
      "** col: AllocTRES :: S43\n",
      "** col: JobID_parent :: S8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_types = {'i8':'>f8', 'i16':'>f16'}\n",
    "foutname = 'data/serc_spark_summary.h5'\n",
    "#\n",
    "#os.remove(foutname)\n",
    "with h5py.File(foutname, 'w') as fout:\n",
    "    ds = fout.create_dataset('summary', (array_len, ),\n",
    "                            dtype=numpy_dtypes)\n",
    "    #\n",
    "    #ds[...] = numpy.array(summary_df.collect())[:]\n",
    "    for cl,tp in numpy_dtypes:\n",
    "        print('** col: {} :: {}'.format(cl, tp))\n",
    "        #if not cl=='NTasks': continue\n",
    "#        ds[cl] = numpy.array([numpy.nan if x is None else x for x in numpy.reshape(summary_df.select(cl).collect(),\n",
    "#                                                                     (array_len,))])\n",
    "        # None types for integer types are badly handled by numpy and HDF5; we have to convert them \\\n",
    "        #   basically manually\n",
    "        if tp in ('>i8', '>i16'):\n",
    "            ds[cl] = numpy.array([numpy.nan if x is None else x for x in numpy.reshape(summary_df.select(cl).collect(),\n",
    "                                                                     (array_len,))])[:]\n",
    "        else:\n",
    "            ds[cl] = numpy.reshape(summary_df.select(cl).collect(), (array_len,)).astype(tp)[:] \n",
    "            #ds[cl] = numpy.array(numpy.reshape(summary_df.select(cl).collect(), (array_len,)), dtype=tp)\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** NTasks\n",
      "[-9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808]\n",
      "** JobID\n",
      "[b'57399044' b'57399898' b'57399898' b'57399898' b'57399898' b'57399898'\n",
      " b'57400920' b'57400920' b'57401745' b'57401745' b'57401745' b'57401745'\n",
      " b'57402775' b'57403173' b'57403223' b'57403323' b'57404945' b'57404947'\n",
      " b'57404965' b'57406142']\n",
      "** NCPUS\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1 16 16 16 16 16 16 16  7]\n",
      "** Elapsed\n",
      "[0.01131944 0.01153935 0.01144676 0.01165509 0.01168981 0.01233796\n",
      " 0.01170139 0.01152778 0.01190972 0.01180556 0.01152778 0.01033565\n",
      " 0.01005787 0.02511574 0.02111111 0.02663194 0.01047454 0.01123843\n",
      " 0.01094907 0.00012731]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(foutname, 'r') as fin:\n",
    "    #print('NTasks:')\n",
    "    for cl in ['NTasks', 'JobID', 'NCPUS', 'Elapsed']:\n",
    "        print('** {}'.format(cl))\n",
    "        print(fin['summary'][cl][0:20])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #col = 'NCPUS'\n",
    "# col = 'NTasks'\n",
    "# #\n",
    "# with h5py.File('summary.h5', 'a') as fin:\n",
    "#     print('** ', fin['summary'][col][0:10])\n",
    "#     #\n",
    "#     print(fin['summary']['NTasks'][0], fin['summary']['NTasks'][0]+1)\n",
    "#     #fin['summary']['NTasks'][0:5] = numpy.array([7 for _ in range(5)][:])\n",
    "#     #fin['summary']['NTasks'][0:5] = (numpy.ones(5)*7)[:]\n",
    "#     #fin['summary']['NTasks'][0]=42.\n",
    "#     fin['summary'][col,0:5] = numpy.ones(5)*43\n",
    "#     fin['summary'][col,5:10] = numpy.array([numpy.nan for _ in range(5)])[:]\n",
    "#     fin['summary'][col,5:10] = numpy.array([7 for _ in range(5)])[:]\n",
    "#     #\n",
    "#     # Not sure write_direct() will work with named columns.\n",
    "#     #fin['summary'].write_direct(numpy.ones(5)*42., source_sel=numpy.s_[0:5], dest_sel=numpy.s_[5:10, RH_index[col]])\n",
    "#     #\n",
    "#     #fin['summary'].write_direct(numpy.ones(5, dtype='>i8')[:]*42, source_sel=numpy.s_[0:5], \n",
    "#     #                            dest_sel=numpy.s_[0:5] )\n",
    "    \n",
    "#     #\n",
    "#     print('** ', fin['summary'][col][0:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** jids:  ['57225527' '57225616' '57225628' '57225637' '57225639' '57274400'\n",
      " '57274801' '57307118' '57307228' '57307243']\n"
     ]
    }
   ],
   "source": [
    "# Let's do some validation...\n",
    "#\n",
    "#summary_sample = summary_df.take(100)\n",
    "\n",
    "\n",
    "rows_sample = rows.take(500)\n",
    "\n",
    "j_ids  = numpy.unique([rw[-1] for rw in rows_sample])\n",
    "#\n",
    "print('** jids: ', j_ids[0:10])\n",
    "\n",
    "summary_sample = summary_df.filter(summary_df.JobID_parent.isin(list(j_ids)) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('** summary[{}]'.format(len(summary_sample)))\n",
    "# for rw in summary_sample[0:10]:\n",
    "#     print('** ', rw[:])\n",
    "# #\n",
    "# print('*** ***\\n*** ***')\n",
    "# #\n",
    "# print('** rows[{}]: '.format(len(rows_sample)))\n",
    "# for rw in rows_sample[0:10]:\n",
    "#     print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jobid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-70b47486f8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n**** **** **** \\nheaders[{}]: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#jobid = '62339523'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjobid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jobid' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n**** **** **** \\nheaders[{}]: {}\\n'.format(jobid, header_names) )\n",
    "#jobid = '62339523'\n",
    "for rw in summary_sample:\n",
    "    jobid = rw[-1]\n",
    "    #\n",
    "    for rw in summary_sample:\n",
    "        if rw[-1]==jobid:\n",
    "            print('** sample: ', rw[:])\n",
    "        #\n",
    "    #\n",
    "    print('*** ***\\n\\n*** ***')\n",
    "    #\n",
    "    k=0\n",
    "    for rw in rows_sample:\n",
    "        if rw[-1]==jobid and not (rw[3] in ('batch', 'extern') ):\n",
    "            print('** row[{}]: {}'.format(k, rw))\n",
    "            k+=1\n",
    "        #\n",
    "    #\n",
    "    print('**\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneill_summ = summary_df.filter(summary_df.Group == 'oneillm').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', oneill_summ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_counts = row_pairs.groupByKey().filter(lambda rw:rw[3]!='extern').filter(lambda rw: rw[3]!='batch').count()\n",
    "row_counts = rows.filter(lambda rw:rw[3]!='extern').filter(lambda rw: rw[3]!='batch')\\\n",
    ".groupBy(lambda rw:rw[-1]).countByValue()\n",
    "#print('** ', row_counts.take(100))\n",
    "\n",
    "# .agg(psf.collect_list('JobID_parent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,(jid,n) in enumerate(row_counts.items()):\n",
    "    if n<=1:\n",
    "        continue\n",
    "    #\n",
    "    print('** [{}] {}: {}'.format(k,jid,n) )\n",
    "#print('** ', numpy.array([(k,n) for k,(jid,n) \n",
    "#                          in enumerate(row_counts.items())])[numpy.array(row_counts.values())>1])\n",
    "#print('** ', row_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

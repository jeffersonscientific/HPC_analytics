{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import pytz\n",
    "import multiprocessing as mpp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numba\n",
    "#\n",
    "import pyspark\n",
    "import h5py\n",
    "#\n",
    "# TODO: phase out unreferenced hpc_lib calls...\n",
    "import hpc_lib\n",
    "\n",
    "#\n",
    "#data_file_name = 'data/mazama_usage_20200506_tool8.out'\n",
    "#data_file_name = 'data/sacct_sherlock_out_serc2020_05_08.out'\n",
    "data_file_name = 'data/serc_usage_20200914.out'\n",
    "#\n",
    "pkl_name = \"{}.pkl\".format(os.path.splitext(data_file_name)[0])\n",
    "h5_name = \"{}.h5\".format(os.path.splitext(data_file_name)[0])\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark implementation of HPC_analytics (prototype)\n",
    "- Right now, a hybrid of vanilla `RDD` and `PySpark DataFrames`\n",
    "- Mixing a bit of using `SQLContext` context methods and whatever the native (??) Spark context.\n",
    "\n",
    "#### Brief sumary:\n",
    "PySpark is the Python implementation of Spark, which is a distributed data processing infrastructure. Spark should parallelize across multiple nodes, and so should be a better multi-processing option than `python.multiprocessing`. The syntax is not Pythonic. At all, so it's basically like writing another language in Python, and the workflow strategies are very different as well, so if you're a Python person, be prepared to pivot a bit.\n",
    "\n",
    "The biggest problem I'm still having is the final act of reading a very large data file into memory or transfering into a new (disk based) container. This, of course, should be simple, since it is really the fundamental and primary purpose of Spark, but alas... The problem arises when the distributed (Java) VMs exceed memory limitations. But we'll get there...\n",
    "\n",
    "#### 1. A quick look at the inpuit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  User|Group|GID|JobName|JobID|JobIDRaw|Partition|State|Timelimit|NCPUS|NNodes|Submit|Eligible|Start|End|Elapsed|SystemCPU|UserCPU|TotalCPU|NTasks|CPUTimeRAW|Suspended|\n",
      "\n",
      "**  saipb|oneillm|328022|hovmuller|62339523|62339523|serc|COMPLETED|4-00:00:00|16|1|2020-03-01T00:10:36|2020-03-01T00:10:36|2020-03-01T00:11:24|2020-03-01T00:12:05|00:00:41|00:02.369|00:17.315|00:19.685||656|00:00:00|\n",
      "\n",
      "**  |||batch|62339523.batch|62339523.batch||COMPLETED||16|1|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:12:05|00:00:41|00:02.367|00:17.315|00:19.683|1|656|00:00:00|\n",
      "\n",
      "**  |||extern|62339523.extern|62339523.extern||COMPLETED||16|1|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:12:05|00:00:41|00:00.001|00:00:00|00:00.001|1|656|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_0|62339659|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.982|21:27.659|21:38.642||1327|00:00:00|\n",
      "\n",
      "**  |||batch|62339657_0.batch|62339659.batch||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.981|21:27.659|21:38.641|1|1327|00:00:00|\n",
      "\n",
      "**  |||extern|62339657_0.extern|62339659.extern||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:00:00|00:00:00|00:00:00|1|1327|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_1|62339660|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.977|21:27.939|21:38.917||1327|00:00:00|\n",
      "\n",
      "**  |||batch|62339657_1.batch|62339660.batch||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.977|21:27.938|21:38.916|1|1327|00:00:00|\n",
      "\n",
      "**  |||extern|62339657_1.extern|62339660.extern||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:00:00|00:00:00|00:00:00|1|1327|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_2|62339661|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:11.013|21:27.938|21:38.951||1327|00:00:00|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a quick look at the input data:\n",
    "#\n",
    "with open(data_file_name, 'r') as fin:\n",
    "    k=0\n",
    "    for rw in fin:\n",
    "        print('** ', rw)\n",
    "        k+=1\n",
    "        if k>10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate and configure some context handler objects.\n",
    "- There are a few...\n",
    "- The \"spark\" and \"sql\" variants seem to come from different branches of the project, or source projects, that have since merged, albeit perhaps not entirely gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build some working components:\n",
    "- Spark context objects\n",
    "- Type dictionary translator\n",
    "- Other type, etc. translator maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 8\n",
    "#\n",
    "# .config(\"spark.driver.memory\", \"15g\")\n",
    "#conf = pyspark.SparkConf('local[*]').set(\"spark.cores.max\", \"6\").set(\"spark.executor.instances\", \"4\").set(\"spark.executor.cores\",\"2\")\n",
    "conf = pyspark.SparkConf('local[{}]'.format(n_cpu)).set(\"spark.driver.memory\", \"15g\")\n",
    "#\n",
    "#conf = conf.set(\"spark.executor.memory\", \"4g\").set(\"spark.executor.pyspark.memory\", \"3g\")\n",
    "sc   = pyspark.SparkContext(conf=conf)\n",
    "#\n",
    "\n",
    "# also build a SQL context? We never ended up using this, though I think you can use\n",
    "#. it to do SQL querries on the data sets.\n",
    "sc_sql = pyspark.SQLContext(sc)\n",
    "#\n",
    "spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').master('local[{}]'.format(n_cpu)).config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "#spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').config(conf).getOrCreate()\n",
    "#\n",
    "#sc.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull some data structures, handler functions, etc. from relevant modules (ie `hpc_lib`). Note that eventually, we'll want to consolicate the `process_row()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** typex_dict:  {'User': <class 'str'>, 'JobID': <class 'str'>, 'JobName': <class 'str'>, 'Partition': <class 'str'>, 'State': <class 'str'>, 'JobID_parent': <class 'str'>, 'Timelimit': <function elapsed_time_2_day at 0x7fa40335b820>, 'Start': <function str2date_num at 0x7fa40335b700>, 'End': <function str2date_num at 0x7fa40335b700>, 'Submit': <function str2date_num at 0x7fa40335b700>, 'Eligible': <function str2date_num at 0x7fa40335b700>, 'Elapsed': <function elapsed_time_2_day at 0x7fa40335b820>, 'MaxRSS': <class 'str'>, 'MaxVMSize': <class 'str'>, 'NNodes': <class 'int'>, 'NCPUS': <class 'int'>, 'MinCPU': <class 'str'>, 'SystemCPU': <function elapsed_time_2_day at 0x7fa40335b820>, 'UserCPU': <function elapsed_time_2_day at 0x7fa40335b820>, 'TotalCPU': <function elapsed_time_2_day at 0x7fa40335b820>, 'NTasks': <class 'int'>}\n"
     ]
    }
   ],
   "source": [
    "delim = '|'\n",
    "types_dict = hpc_lib.SACCT_data_handler.default_types_dict\n",
    "print('** typex_dict: ', types_dict)\n",
    "#\n",
    "# some numpy. (and other?) data types are not (well) supported by Spark, most pointedly some of the \n",
    "#. numpy.float{k} variants, so write a dictionary/map to re-type them. NOTE: this might need to include\n",
    "#. some numpy.int{k} types as well.\n",
    "numpy_re_typer={numpy.float64:float, numpy.float128:float, numpy.int64:int }\n",
    "#\n",
    "# python functions to use on grouped/aggregated columns:\n",
    "group_py_functions = {'End':numpy.nanmax, 'Start':numpy.nanmin, 'NCPUS':numpy.nanmax, 'NNodes':numpy.nanmax}\n",
    "#\n",
    "# Define one or more row processing functions.\n",
    "def f_rw(rw, header_names, RH_index, types_dict=types_dict, delim='|'):\n",
    "    rws = rw[:-1].split(delim)\n",
    "    #\n",
    "    #if not len(rws)==0:\n",
    "    #    return rws\n",
    "    return [None if s=='' else types_dict.get(h,str)(s) for h,s in zip(header_names,rws)] + [rws[RH_index['JobID']].split('.')[0] ]\n",
    "    #return [str(s) for h,s in zip(header_names,rws)]\n",
    "#\n",
    "# this will be used to reduce() the raw data to summary data:\n",
    "def f_reduce_row(r1, r2):\n",
    "     return tuple([group_py_functions.get(hdr, lambda x: x[0] )([x1, x2]) \n",
    "             for k, (hdr,x1,x2) in enumerate(zip(header_names,\n",
    "                                *sorted([r1, r2], key=lambda X:X[RH_index['Submit']]))) ] )\n",
    "\n",
    "#\n",
    "def spark_types_to_numpy(spark_df):\n",
    "    # row1 = df1.agg({\"x\": \"max\"}).collect()[0]\n",
    "    #\n",
    "    # peel off the dtypes so it will pickle properly:\n",
    "    spark_dtypes = spark_df.dtypes\n",
    "    #\n",
    "    lens = spark_df.rdd.map(lambda rw: [len(s) if spark_dtypes[k][1]=='string' else None for k,s in enumerate(rw) ] ).reduce(lambda x1,x2: max(x1,x2))\n",
    "\n",
    "    dtypes_out = []\n",
    "    for (nm,tp), s_len in zip(spark_dtypes, lens):\n",
    "        if tp in ('int', 'bigint'):\n",
    "            tp_n = '>i8'\n",
    "            #tp_n = '>f8'\n",
    "            # do we need a casting function?\n",
    "            #nm_f = float\n",
    "        elif tp in ('float', 'double'):\n",
    "            tp_n = '>f8'\n",
    "        elif tp in ('string'):\n",
    "            tp_n = 'S{}'.format(s_len)\n",
    "        #\n",
    "        dtypes_out += [(nm, tp_n)]\n",
    "    #\n",
    "    return dtypes_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Headers:  ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended']\n"
     ]
    }
   ],
   "source": [
    "# Preliminarily, read file into line-strings. Extract header row so that we\n",
    "#. can type() each column when we split() it up.\n",
    "#\n",
    "lines = sc.textFile(data_file_name)\n",
    "header_names = (lines.take(1)[0])[:-1].split(delim)\n",
    "RH_index = {s:k for k,s in enumerate(header_names) }\n",
    "#\n",
    "print('** Headers: ', header_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding header row from data collection:\n",
    "- This is surprisingly harder than it looks, and searching for solutions seems to be elusive.\n",
    "-  I think the most direct and common approach is to read text data via `textFile()`, then use filter() to exclude rows that look like the first row. This sounds kinda dumb for a single file, but it is a good option when reading multiple files.\n",
    "- *(spark) dataframes:* Use a syntactical variant of `spark.read.format('CSV')` method below to read the data into a dataframe. This provides options to exclude and catch the header row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes:\n",
    "- using the sql context, we can read the data into a dataframe\n",
    "- Nominally fast and easy, but I think really for well behaved data.\n",
    "- Getting the header row is not too tough, but I'm not so sure about excluding a false terminal column, resulting from a terminal delimeter (row string ending in a delimiter).\n",
    "- In fact, we seem to get some weird behavior from this\n",
    "- ... to the point that I would probably just err on the side of having more control and maybe burning some cycles on the filter() option (which i expect is pretty well optimized on the back end).\n",
    "- HOWEVER: Preliminary assessments just doing a `.count()` suggests that DF might be much, much faster than the standard RDD methods... Though that may also be because the `DataFramds` methods are using a context or session that is not CPU limited -- which would make sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Another way to read the file with headers. This will give an effective array of (val,ky) tuples.\n",
    "# df_rows1 = spark.read.format('CSV').option('header', 'true').option('sep', '|').load(data_file_name)\n",
    "# #\n",
    "# # another syntax:\n",
    "# df_rows = spark.read.csv(data_file_name, header=True, sep='|')\n",
    "# #\n",
    "\n",
    "# for rw in df_rows1.take(10):\n",
    "#     print('** ', rw[:])\n",
    "\n",
    "# # Another syntax to load directly into a spark dataframe (via .sql):\n",
    "# #\n",
    "# #\n",
    "# print('** type: ', type(rows_2))\n",
    "# print('** type: ', type(df_rows))\n",
    "# print('** dypes: ', df_rows.dtypes)\n",
    "# print('** header: {}'.format( df_rows.schema.names ) )\n",
    "# #\n",
    "# print('\\n*** *** ')\n",
    "# for rw in df_rows.take(5):\n",
    "#     print('** ', rw[:])\n",
    "#     #print('* * ', rw.head)\n",
    "#\n",
    "# print('*** ', df_rows.schema)\n",
    "# #\n",
    "# print('*** ', df_rows.dtypes)\n",
    "# #print('** ', set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** n_terminal:  1\n",
      "** Headers[23]: \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# identifying the header row(s) and filtering bogus terminal characters on each row:\n",
    "n_terminal = 0\n",
    "header_string = lines.take(1)[0]\n",
    "#\n",
    "while header_string[-1] in ('\\n', delim):\n",
    "    header_string=header_string[:-1]\n",
    "    n_terminal += 1\n",
    "#\n",
    "# if necessary, trim off row-terminal characters:\n",
    "print('** n_terminal: ', n_terminal)\n",
    "if n_terminal>0:\n",
    "    lines = lines.map(lambda ln: ln[:-n_terminal])\n",
    "#\n",
    "header_names = header_string.split(delim) + ['JobID_parent']\n",
    "RH_index = {s:k for k,s in enumerate(header_names) }\n",
    "print('** Headers[{}]: '.format(len(header_names), header_names) )\n",
    "#\n",
    "# for c in (lines.take(2)[1]):\n",
    "#     print('{}: [{}]'.format(c, ord(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to filter header rows:\n",
    "n_startswith = 15\n",
    "header_start = header_string[0:n_startswith]\n",
    "#\n",
    "rows = lines.filter(lambda s: not s.startswith(header_start) ).map(lambda x: f_rw(x, header_names,\n",
    "                                            types_dict=types_dict, RH_index=RH_index) )\n",
    "rows = rows.map(lambda rw: [None if (x is None or x=='') else numpy_re_typer.get(type(x), type(x))(x) for x in rw])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we can either group and reduce using the RH{} index, or convert to a DF first:\n",
    "#rows_df = spark.createDataFrame( rows, header_names ).sort('JobID')\n",
    "#\n",
    "# print('** rows_df schema[{}]: {}'.format(len(rows_df.schema), rows_df.schema))\n",
    "# print('** dytpes[{}]: {}'.format(len(rows_df.dtypes), rows_df.dtypes))\n",
    "# print('** header_names[{}]: {}'.format(len(header_names), header_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, let's try a RDD_pair, then reduce_by_key() function:\n",
    "# (this appears to work, but needs to be validated )\n",
    "row_pairs = rows.map(lambda x: (x[RH_index['JobID_parent']], list(x[:])))\n",
    "#\n",
    "summary_rdd = row_pairs.reduceByKey(f_reduce_row)\n",
    "#\n",
    "\n",
    "# for rw in summary_rdd.take(10):\n",
    "#     print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=100\n",
    "# rows_df = spark.createDataFrame( rows.takeSample(withReplacement=True, num=N), header_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# make a sampling DataFrame() to automagically determine the schema (this breaks when we do just \n",
    "#. the summary... which suggests maybe a problem -- NTasks is not populating correctly maybe?)\n",
    "# otherwise, we need to write some more translator dictionaries to map numpy types to Spark types\n",
    "#\n",
    "for n in range(2,7):\n",
    "    N=int(10**n)\n",
    "    try:\n",
    "        rows_df = spark.createDataFrame( rows.take(N), header_names )\n",
    "        break\n",
    "    except:\n",
    "        # try a bigger sample.\n",
    "        print('broke for [{}]. trying a bigger sample.'.format(N))\n",
    "    #\n",
    "        \n",
    "# rows_df = spark.createDataFrame( rows, header_names ).sort('JobID')\n",
    "summary_df = spark.createDataFrame(summary_rdd.values().map(lambda rw: \n",
    "                            [None if (x is None or x=='') else numpy_re_typer.get(type(x),\n",
    "                                lambda a:a)(x) for x in rw]),\n",
    "                                   rows_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rw in summary_df.take(10):\n",
    "#     print('** ', rw[:])\n",
    "\n",
    "# dt_dict = dict(summary_df.dtypes)\n",
    "# sdf_dtypes = summary_df.dtypes\n",
    "# #\n",
    "# print('** ', sdf_dtypes)\n",
    "# #lens = summary_df.rdd.map(lambda rw: [len(s) for k,s in enumerate(rw) if sdf_dtypes[k][1]=='string'] ).reduce(lambda x1,x2: max(x1,x2))\n",
    "# lens = summary_df.rdd.map(lambda rw: [len(s) if sdf_dtypes[k][1]=='string' else None for k,s in enumerate(rw) ] ).reduce(lambda x1,x2: max(x1,x2))\n",
    "# print('** lens: ', lens)\n",
    "\n",
    "\n",
    "\n",
    "# # for rw in lens.take(10):\n",
    "# #     print('** ', rw)\n",
    "# print('** ')\n",
    "# for (cl,n),l in zip(summary_df.dtypes, lens):\n",
    "#     print('** ', cl,n,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  [('User', 'string'), ('Group', 'string'), ('GID', 'string'), ('JobName', 'string'), ('JobID', 'string'), ('JobIDRaw', 'string'), ('Partition', 'string'), ('State', 'string'), ('Timelimit', 'double'), ('NCPUS', 'bigint'), ('NNodes', 'bigint'), ('Submit', 'double'), ('Eligible', 'double'), ('Start', 'double'), ('End', 'double'), ('Elapsed', 'double'), ('SystemCPU', 'double'), ('UserCPU', 'double'), ('TotalCPU', 'double'), ('NTasks', 'bigint'), ('CPUTimeRAW', 'string'), ('Suspended', 'string'), ('JobID_parent', 'string')]\n",
      "** \n",
      "** output_dfs:  [('User', 'S8'), ('Group', 'S8'), ('GID', 'S5'), ('JobName', 'S24'), ('JobID', 'S8'), ('JobIDRaw', 'S8'), ('Partition', 'S4'), ('State', 'S18'), ('Timelimit', '>f8'), ('NCPUS', '>i8'), ('NNodes', '>i8'), ('Submit', '>f8'), ('Eligible', '>f8'), ('Start', '>f8'), ('End', '>f8'), ('Elapsed', '>f8'), ('SystemCPU', '>f8'), ('UserCPU', '>f8'), ('TotalCPU', '>f8'), ('NTasks', '>i8'), ('CPUTimeRAW', 'S5'), ('Suspended', 'S7'), ('JobID_parent', 'S8')]\n"
     ]
    }
   ],
   "source": [
    "print('** ', summary_df.dtypes)\n",
    "#spark_to_numpy_types('double':float, 'float':float, 'bigint':int, 'int':int, 'string':str)\n",
    "#\n",
    "print('** ')\n",
    "#\n",
    "numpy_dtypes = spark_types_to_numpy(summary_df)\n",
    "array_len = summary_df.count()\n",
    "#\n",
    "print('** output_dfs: ', numpy_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** col: User :: S8\n",
      "** col: Group :: S8\n",
      "** col: GID :: S5\n",
      "** col: JobName :: S24\n",
      "** col: JobID :: S8\n",
      "** col: JobIDRaw :: S8\n",
      "** col: Partition :: S4\n",
      "** col: State :: S18\n",
      "** col: Timelimit :: >f8\n",
      "** col: NCPUS :: >i8\n",
      "** col: NNodes :: >i8\n",
      "** col: Submit :: >f8\n",
      "** col: Eligible :: >f8\n",
      "** col: Start :: >f8\n",
      "** col: End :: >f8\n",
      "** col: Elapsed :: >f8\n",
      "** col: SystemCPU :: >f8\n",
      "** col: UserCPU :: >f8\n",
      "** col: TotalCPU :: >f8\n",
      "** col: NTasks :: >i8\n",
      "** col: CPUTimeRAW :: S5\n",
      "** col: Suspended :: S7\n",
      "** col: JobID_parent :: S8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_types = {'i8':'>f8', 'i16':'>f16'}\n",
    "foutname = 'data/serc_spark_summary.h5'\n",
    "#\n",
    "#os.remove(foutname)\n",
    "with h5py.File(foutname, 'w') as fout:\n",
    "    ds = fout.create_dataset('summary', (array_len, ),\n",
    "                            dtype=numpy_dtypes)\n",
    "    #\n",
    "    #ds[...] = numpy.array(summary_df.collect())[:]\n",
    "    for cl,tp in numpy_dtypes:\n",
    "        print('** col: {} :: {}'.format(cl, tp))\n",
    "        #if not cl=='NTasks': continue\n",
    "#        ds[cl] = numpy.array([numpy.nan if x is None else x for x in numpy.reshape(summary_df.select(cl).collect(),\n",
    "#                                                                     (array_len,))])\n",
    "        # None types for integer types are badly handled by numpy and HDF5; we have to convert them \\\n",
    "        #   basically manually\n",
    "        if tp in ('>i8', '>i16'):\n",
    "            ds[cl] = numpy.array([numpy.nan if x is None else x for x in numpy.reshape(summary_df.select(cl).collect(),\n",
    "                                                                     (array_len,))])[:]\n",
    "        else:\n",
    "            ds[cl] = numpy.reshape(summary_df.select(cl).collect(), (array_len,)).astype(tp)[:] \n",
    "            #ds[cl] = numpy.array(numpy.reshape(summary_df.select(cl).collect(), (array_len,)), dtype=tp)\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** NTasks\n",
      "[-9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808]\n",
      "** JobID\n",
      "[b'62339657' b'62339657' b'62339657' b'62339657' b'62339657' b'62339657'\n",
      " b'62344473' b'62344473' b'62344473' b'62344473' b'62344473' b'62344473'\n",
      " b'62344473' b'62346892' b'62346892' b'62346892' b'62346892' b'62346892'\n",
      " b'62350507' b'62350507']\n",
      "** NCPUS\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "** Elapsed\n",
      "[1.53009259e-02 1.52430556e-02 1.53703704e-02 1.50810185e-02\n",
      " 1.51388889e-02 1.51620370e-02 1.53703704e-02 1.42592593e-02\n",
      " 9.25925926e-05 1.54513889e-02 1.54745370e-02 1.41898148e-02\n",
      " 1.42013889e-02 1.54513889e-02 1.51851852e-02 1.43171296e-02\n",
      " 1.50462963e-04 1.54861111e-02 1.56018519e-02 1.58333333e-02]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(foutname, 'r') as fin:\n",
    "    #print('NTasks:')\n",
    "    for cl in ['NTasks', 'JobID', 'NCPUS', 'Elapsed']:\n",
    "        print('** {}'.format(cl))\n",
    "        print(fin['summary'][cl][0:20])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #col = 'NCPUS'\n",
    "# col = 'NTasks'\n",
    "# #\n",
    "# with h5py.File('summary.h5', 'a') as fin:\n",
    "#     print('** ', fin['summary'][col][0:10])\n",
    "#     #\n",
    "#     print(fin['summary']['NTasks'][0], fin['summary']['NTasks'][0]+1)\n",
    "#     #fin['summary']['NTasks'][0:5] = numpy.array([7 for _ in range(5)][:])\n",
    "#     #fin['summary']['NTasks'][0:5] = (numpy.ones(5)*7)[:]\n",
    "#     #fin['summary']['NTasks'][0]=42.\n",
    "#     fin['summary'][col,0:5] = numpy.ones(5)*43\n",
    "#     fin['summary'][col,5:10] = numpy.array([numpy.nan for _ in range(5)])[:]\n",
    "#     fin['summary'][col,5:10] = numpy.array([7 for _ in range(5)])[:]\n",
    "#     #\n",
    "#     # Not sure write_direct() will work with named columns.\n",
    "#     #fin['summary'].write_direct(numpy.ones(5)*42., source_sel=numpy.s_[0:5], dest_sel=numpy.s_[5:10, RH_index[col]])\n",
    "#     #\n",
    "#     #fin['summary'].write_direct(numpy.ones(5, dtype='>i8')[:]*42, source_sel=numpy.s_[0:5], \n",
    "#     #                            dest_sel=numpy.s_[0:5] )\n",
    "    \n",
    "#     #\n",
    "#     print('** ', fin['summary'][col][0:15])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import pytz\n",
    "import multiprocessing as mpp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numba\n",
    "#\n",
    "import pyspark\n",
    "#\n",
    "# TODO: phase out unreferenced hpc_lib calls...\n",
    "import hpc_lib\n",
    "\n",
    "#\n",
    "#data_file_name = 'data/mazama_usage_20200506_tool8.out'\n",
    "#data_file_name = 'data/sacct_sherlock_out_serc2020_05_08.out'\n",
    "data_file_name = 'data/serc_usage_20200914.out'\n",
    "#\n",
    "pkl_name = \"{}.pkl\".format(os.path.splitext(data_file_name)[0])\n",
    "h5_name = \"{}.h5\".format(os.path.splitext(data_file_name)[0])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf('local[*]').set(\"spark.cores.max\", \"6\").set(\"spark.executor.instances\", \"4\").set(\"spark.executor.cores\",\"2\")\n",
    "conf = conf.set(\"spark.executor.memory\", \"4g\").set(\"spark.executor.pyspark.memory\", \"3g\")\n",
    "sc   = pyspark.SparkContext(conf=conf)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also build a SQL context?\n",
    "sc_sql = pyspark.SQLContext(sc)\n",
    "spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_dict = hpc_lib.SACCT_data_handler.default_types_dict\n",
    "\n",
    "@numba.jit\n",
    "def process_row(rw, delim='|'):\n",
    "    # use this with MPP processing:\n",
    "    # ... but TODO: it looks like this is 1) inefficient and 2) breaks with large data inputs because I think it pickles the entire\n",
    "    #  class object... so we need to move the MPP object out of class.\n",
    "    #\n",
    "    # use this for MPP processing:\n",
    "    rws = rw.split(delim)\n",
    "    #return [None if vl=='' else self.types_dict.get(col,str)(vl)\n",
    "    #            for k,(col,vl) in enumerate(zip(self.headers, rw.split(self.delim)[:-1]))]\n",
    "    return [None if vl=='' else types_dict.get(col,str)(vl)\n",
    "                for k,(col,vl) in enumerate(zip(self.headers, rws[:-1]))] + [rws[self.RH['JobID']].split('.')[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SparkSession, we can load the csv directly to a spark Data Frame:\n",
    "df_rows = spark.read.csv(data_file_name, header=True, sep='|')\n",
    "#\n",
    "print('** ', df_rows.dtypes)\n",
    "print('** ', df_rows.head)\n",
    "#\n",
    "print('*** *** ')\n",
    "for rw in df_rows.take(5):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rw(rw):\n",
    "    rws = rw.split('|')\n",
    "    #\n",
    "    if not len(rws)==0:\n",
    "        return rws\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(data_file_name)\n",
    "\n",
    "delim = '|'\n",
    "\n",
    "rows = lines.map(f_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StorageLevel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f6c0a0e4e024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEMORY_AND_DISK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#grouped = rows.filter(lambda rw:rw[0]!='User').groupBy(lambda rw: rw[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StorageLevel' is not defined"
     ]
    }
   ],
   "source": [
    "rows.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "all_rows = rows.collect()\n",
    "rows.unpersist()\n",
    "\n",
    "#grouped = rows.filter(lambda rw:rw[0]!='User').groupBy(lambda rw: rw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rws = rows.count()\n",
    "print('** n_rws: {}'.format( n_rws) )\n",
    "#\n",
    "n=1000\n",
    "xx = rows.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(grouped))\n",
    "xx = grouped.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('** lines: ')\n",
    "# for ln in lines.take(10):\n",
    "# \tprint('** **: ', ln)\n",
    "#\n",
    "print('rows: ')\n",
    "for rw in rows.take(10):\n",
    "\tprint('** **: ', rw)\n",
    "#\n",
    "#\n",
    "# print('** rows again: ')\n",
    "# for rw in rows.take(10):\n",
    "#         print('** **: ', rw)\n",
    "#\n",
    "print('** groups: ')\n",
    "for rw in grouped.take(10):\n",
    "    print('* *: ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin all_rows[]: ')\n",
    "t0 = time.time()\n",
    "#\n",
    "all_rows = rows.collect()\n",
    "#\n",
    "print('** time: {}'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin all_lines: ')\n",
    "t0 = time.time()\n",
    "#\n",
    "all_lines = lines.collect()\n",
    "#\n",
    "print('** time: {}'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** sc: ', sc.defaultParallelism)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import pytz\n",
    "import multiprocessing as mpp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numba\n",
    "#\n",
    "import pyspark\n",
    "#\n",
    "# TODO: phase out unreferenced hpc_lib calls...\n",
    "import hpc_lib\n",
    "\n",
    "#\n",
    "#data_file_name = 'data/mazama_usage_20200506_tool8.out'\n",
    "#data_file_name = 'data/sacct_sherlock_out_serc2020_05_08.out'\n",
    "data_file_name = 'data/serc_usage_20200914.out'\n",
    "#\n",
    "pkl_name = \"{}.pkl\".format(os.path.splitext(data_file_name)[0])\n",
    "h5_name = \"{}.h5\".format(os.path.splitext(data_file_name)[0])\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark tutorial and examples\n",
    "- PySpark implementation of HPC_analytics SACCT data parsing script\n",
    "- Discussion of different reading methods and data reading/management classes\n",
    "    - Vanilla `RDD`\n",
    "    - `PySpark DataFrames` object (and `SQLContext` context methods).\n",
    "\n",
    "#### Brief sumary:\n",
    "PySpark is the Python implementation of Spark, which is a distributed data processing infrastructure. Spark should parallelize across multiple nodes, and so should be a better multi-processing option than `python.multiprocessing`. The syntax is not Pythonic. At all, so it's basically like writing another language in Python, and the workflow strategies are very different as well, so if you're a Python person, be prepared to pivot a bit.\n",
    "\n",
    "The biggest problem I'm still having is the final act of reading a very large data file into memory or transfering into a new (disk based) container. This, of course, should be simple, since it is really the fundamental and primary purpose of Spark, but alas... The problem arises when the distributed (Java) VMs exceed memory limitations. But we'll get there...\n",
    "\n",
    "#### 1. A quick look at the inpuit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  User|Group|GID|JobName|JobID|JobIDRaw|Partition|State|Timelimit|NCPUS|NNodes|Submit|Eligible|Start|End|Elapsed|SystemCPU|UserCPU|TotalCPU|NTasks|CPUTimeRAW|Suspended|\n",
      "\n",
      "**  saipb|oneillm|328022|hovmuller|62339523|62339523|serc|COMPLETED|4-00:00:00|16|1|2020-03-01T00:10:36|2020-03-01T00:10:36|2020-03-01T00:11:24|2020-03-01T00:12:05|00:00:41|00:02.369|00:17.315|00:19.685||656|00:00:00|\n",
      "\n",
      "**  |||batch|62339523.batch|62339523.batch||COMPLETED||16|1|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:12:05|00:00:41|00:02.367|00:17.315|00:19.683|1|656|00:00:00|\n",
      "\n",
      "**  |||extern|62339523.extern|62339523.extern||COMPLETED||16|1|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:11:24|2020-03-01T00:12:05|00:00:41|00:00.001|00:00:00|00:00.001|1|656|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_0|62339659|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.982|21:27.659|21:38.642||1327|00:00:00|\n",
      "\n",
      "**  |||batch|62339657_0.batch|62339659.batch||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.981|21:27.659|21:38.641|1|1327|00:00:00|\n",
      "\n",
      "**  |||extern|62339657_0.extern|62339659.extern||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:00:00|00:00:00|00:00:00|1|1327|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_1|62339660|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.977|21:27.939|21:38.917||1327|00:00:00|\n",
      "\n",
      "**  |||batch|62339657_1.batch|62339660.batch||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:10.977|21:27.938|21:38.916|1|1327|00:00:00|\n",
      "\n",
      "**  |||extern|62339657_1.extern|62339660.extern||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:00:00|00:00:00|00:00:00|1|1327|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_2|62339661|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:11.013|21:27.938|21:38.951||1327|00:00:00|\n",
      "\n",
      "**  |||batch|62339657_2.batch|62339661.batch||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:11.013|21:27.937|21:38.951|1|1327|00:00:00|\n",
      "\n",
      "**  |||extern|62339657_2.extern|62339661.extern||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:00:00|00:00:00|00:00:00|1|1327|00:00:00|\n",
      "\n",
      "**  pjwomble|gorelick|26961|6dff71d6eaf0c|62339657_3|62339662|serc|COMPLETED|00:59:00|1|1|2020-03-01T00:17:07|2020-03-01T00:17:08|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:11.626|21:25.533|21:37.160||1327|00:00:00|\n",
      "\n",
      "**  |||batch|62339657_3.batch|62339662.batch||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:11.626|21:25.532|21:37.159|1|1327|00:00:00|\n",
      "\n",
      "**  |||extern|62339657_3.extern|62339662.extern||COMPLETED||1|1|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:17:14|2020-03-01T00:39:21|00:22:07|00:00:00|00:00.001|00:00.001|1|1327|00:00:00|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a quick look at the input data:\n",
    "#\n",
    "with open(data_file_name, 'r') as fin:\n",
    "    k=0\n",
    "    for rw in fin:\n",
    "        print('** ', rw)\n",
    "        k+=1\n",
    "        if k>15: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate and configure some context handler objects.\n",
    "- There are a few...\n",
    "- The \"spark\" and \"sql\" variants seem to come from different branches of the project, or source projects, that have since merged, albeit perhaps not entirely gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 8\n",
    "#\n",
    "# .config(\"spark.driver.memory\", \"15g\")\n",
    "#conf = pyspark.SparkConf('local[*]').set(\"spark.cores.max\", \"6\").set(\"spark.executor.instances\", \"4\").set(\"spark.executor.cores\",\"2\")\n",
    "conf = pyspark.SparkConf('local[{}]'.format(n_cpu)).set(\"spark.driver.memory\", \"15g\")\n",
    "#conf = conf.set(\"spark.executor.memory\", \"4g\").set(\"spark.executor.pyspark.memory\", \"3g\")\n",
    "sc   = pyspark.SparkContext(conf=conf)\n",
    "#\n",
    "\n",
    "# also build a SQL context?\n",
    "sc_sql = pyspark.SQLContext(sc)\n",
    "spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').master('local[{}]'.format(n_cpu)).config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "#spark = pyspark.sql.SparkSession.builder.appName('HPC_loader').config(conf).getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull some data structures, handler functions, etc. from relevant modules (ie `hpc_lib`). Note that eventually, we'll want to consolicate the `process_row()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** typex_dict:  {'User': <class 'str'>, 'JobID': <class 'str'>, 'JobName': <class 'str'>, 'Partition': <class 'str'>, 'State': <class 'str'>, 'JobID_parent': <class 'str'>, 'Timelimit': <function elapsed_time_2_day at 0x7fa38eb05550>, 'Start': <function str2date_num at 0x7fa38eb05430>, 'End': <function str2date_num at 0x7fa38eb05430>, 'Submit': <function str2date_num at 0x7fa38eb05430>, 'Eligible': <function str2date_num at 0x7fa38eb05430>, 'Elapsed': <function elapsed_time_2_day at 0x7fa38eb05550>, 'MaxRSS': <class 'str'>, 'MaxVMSize': <class 'str'>, 'NNodes': <class 'int'>, 'NCPUS': <class 'int'>, 'MinCPU': <class 'str'>, 'SystemCPU': <function elapsed_time_2_day at 0x7fa38eb05550>, 'UserCPU': <function elapsed_time_2_day at 0x7fa38eb05550>, 'TotalCPU': <function elapsed_time_2_day at 0x7fa38eb05550>, 'NTasks': <class 'int'>}\n"
     ]
    }
   ],
   "source": [
    "types_dict = hpc_lib.SACCT_data_handler.default_types_dict\n",
    "print('** typex_dict: ', types_dict)\n",
    "\n",
    "@numba.jit\n",
    "def process_row(rw, delim='|'):\n",
    "    # use this with MPP processing:\n",
    "    # ... but TODO: it looks like this is 1) inefficient and 2) breaks with large data inputs because I think it pickles the entire\n",
    "    #  class object... so we need to move the MPP object out of class.\n",
    "    #\n",
    "    # use this for MPP processing:\n",
    "    rws = rw.split(delim)\n",
    "    #\n",
    "    # NOTE: SACCT returns data with a terminal delimiter. We can assume this or trap for it. Here, we assume\n",
    "    #. it and exclude the last value in each row. Note that this also handles the terminal `\\n` character.\n",
    "    #. a more rigorous solution would be to confirm that the `\\n` is being treated like an actual character\n",
    "    #  (ie, not automatically parsed since it's an EoL delimeter), and then check the rw[-2] character.\n",
    "    #. We probably want to avoid diagnosing this on a row-by-row basis, for performance. We could add \n",
    "    #. an input pram to beter handle this.\n",
    "    #\n",
    "    #return [None if vl=='' else self.types_dict.get(col,str)(vl)\n",
    "    #            for k,(col,vl) in enumerate(zip(self.headers, rw.split(self.delim)[:-1]))]\n",
    "    return [None if vl=='' else types_dict.get(col,str)(vl)\n",
    "                for k,(col,vl) in enumerate(zip(self.headers, rws[:-1]))] + [rws[self.RH['JobID']].split('.')[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Direct Approach:\n",
    "- Use a sequence of RDD operations to (pseudo-)directly compute jobs_summary from the raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = '|'\n",
    "# Define one or more row processing functions.\n",
    "def f_rw(rw, header_names, RH_index, types_dict=types_dict, delim='|'):\n",
    "    rws = rw[:-1].split(delim)\n",
    "    #\n",
    "    #if not len(rws)==0:\n",
    "    #    return rws\n",
    "    return [None if s=='' else types_dict.get(h,str)(s) for h,s in zip(header_names,rws)] + [rws[RH_index['JobID']].split('.')[0] ]\n",
    "    #return [str(s) for h,s in zip(header_names,rws)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic RDD operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Headers:  ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended']\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(data_file_name)\n",
    "header_names = (lines.take(1)[0])[:-1].split(delim)\n",
    "RH_index = {s:k for k,s in enumerate(header_names) }\n",
    "print('** Headers: ', header_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding header row from data collection:\n",
    "- This is surprisingly harder than it looks, and searching for solutions seems to be elusive.\n",
    "- A very direct approach like: read first row, then read the rest of the file, skipping the first row, etc. do not really seem to be an option\n",
    "- We have to find a _sparkonic_ way\n",
    "- But there are a few:\n",
    "    - Using the most common (I think) and direct approach to read text data, `textFile()`, a filter() can be used. This is also a good option when reading multiple files. For example, a script to consilidate many small data files into a single (HDF5) container might ust this, since the header row will be encountered many times.\n",
    "    - *(spark) dataframes:* Use a syntactical variant of `spark.read.format('CSV')` method below to read the data into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  ['saipb', 'oneillm', '328022', 'hovmuller', '62339523', '62339523', 'serc', 'COMPLETED', 4.0, 16, 1, 737485.0073611111, 737485.0073611111, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 2.7418981481481484e-05, 0.0002004050925925926, 0.00022783564814814812, None, '656', '00:00:00', '62339523']\n",
      "**  [None, None, None, 'batch', '62339523.batch', '62339523.batch', None, 'COMPLETED', None, 16, 1, 737485.0079166667, 737485.0079166667, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 2.7395833333333333e-05, 0.0002004050925925926, 0.0002278125, 1, '656', '00:00:00', '62339523']\n",
      "**  [None, None, None, 'extern', '62339523.extern', '62339523.extern', None, 'COMPLETED', None, 16, 1, 737485.0079166667, 737485.0079166667, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 1.1574074074074074e-08, 0.0, 1.1574074074074074e-08, 1, '656', '00:00:00', '62339523']\n",
      "**  ['pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_0', '62339659', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012710648148148146, 0.01490346064814815, 0.015030578703703704, None, '1327', '00:00:00', '62339657_0']\n",
      "**  [None, None, None, 'batch', '62339657_0.batch', '62339659.batch', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012709490740740742, 0.01490346064814815, 0.01503056712962963, 1, '1327', '00:00:00', '62339657_0']\n",
      "**  [None, None, None, 'extern', '62339657_0.extern', '62339659.extern', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.0, 0.0, 0.0, 1, '1327', '00:00:00', '62339657_0']\n",
      "**  ['pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_1', '62339660', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012704861111111112, 0.01490670138888889, 0.015033761574074073, None, '1327', '00:00:00', '62339657_1']\n",
      "**  [None, None, None, 'batch', '62339657_1.batch', '62339660.batch', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012704861111111112, 0.014906689814814815, 0.015033749999999999, 1, '1327', '00:00:00', '62339657_1']\n",
      "**  [None, None, None, 'extern', '62339657_1.extern', '62339660.extern', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.0, 0.0, 0.0, 1, '1327', '00:00:00', '62339657_1']\n",
      "**  ['pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_2', '62339661', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012746527777777778, 0.014906689814814815, 0.015034155092592592, None, '1327', '00:00:00', '62339657_2']\n"
     ]
    }
   ],
   "source": [
    "# is there a smart way to skip the header row? I\"m not finding it, and this was actually recommended.\n",
    "#. obviously, it's expensive and will be part of the reason to reorganize this to start with the \\\n",
    "#. spark DF clase.\n",
    "# that said, if we are taking multiple files, the filer() approach tentatively makes a lot of sense.\n",
    "#\n",
    "rows = lines.filter(lambda s: not s.startswith('User') ).map(lambda x: f_rw(x, header_names, RH_index) )\n",
    "#\n",
    "for rw in rows.take(10):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes:\n",
    "- using the sql context, we can read the data into a dataframe\n",
    "- Nominally fast and easy, but I think really for well behaved data.\n",
    "- Getting the header row is not too tough, but I'm not so sure about excluding a false terminal column, resulting from a terminal delimeter (row string ending in a delimiter).\n",
    "- In fact, we seem to get some weird behavior from this\n",
    "- ... to the point that I would probably just err on the side of having more control and maybe burning some cycles on the filter() option (which i expect is pretty well optimized on the back end).\n",
    "- HOWEVER: Preliminary assessments just doing a `.count()` suggests that DF might be much, much faster than the standard RDD methods... Though that may also be because the `DataFramds` methods are using a context or session that is not CPU limited -- which would make sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** type:  <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "**  ('saipb', 'oneillm', '328022', 'hovmuller', '62339523', '62339523', 'serc', 'COMPLETED', '4-00:00:00', '16', '1', '2020-03-01T00:10:36', '2020-03-01T00:10:36', '2020-03-01T00:11:24', '2020-03-01T00:12:05', '00:00:41', '00:02.369', '00:17.315', '00:19.685', None, '656', '00:00:00', None)\n",
      "**  (None, None, None, 'batch', '62339523.batch', '62339523.batch', None, 'COMPLETED', None, '16', '1', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:12:05', '00:00:41', '00:02.367', '00:17.315', '00:19.683', '1', '656', '00:00:00', None)\n",
      "**  (None, None, None, 'extern', '62339523.extern', '62339523.extern', None, 'COMPLETED', None, '16', '1', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:12:05', '00:00:41', '00:00.001', '00:00:00', '00:00.001', '1', '656', '00:00:00', None)\n",
      "**  ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_0', '62339659', 'serc', 'COMPLETED', '00:59:00', '1', '1', '2020-03-01T00:17:07', '2020-03-01T00:17:08', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:10.982', '21:27.659', '21:38.642', None, '1327', '00:00:00', None)\n",
      "**  (None, None, None, 'batch', '62339657_0.batch', '62339659.batch', None, 'COMPLETED', None, '1', '1', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:10.981', '21:27.659', '21:38.641', '1', '1327', '00:00:00', None)\n",
      "**  (None, None, None, 'extern', '62339657_0.extern', '62339659.extern', None, 'COMPLETED', None, '1', '1', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:00:00', '00:00:00', '00:00:00', '1', '1327', '00:00:00', None)\n",
      "**  ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_1', '62339660', 'serc', 'COMPLETED', '00:59:00', '1', '1', '2020-03-01T00:17:07', '2020-03-01T00:17:08', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:10.977', '21:27.939', '21:38.917', None, '1327', '00:00:00', None)\n",
      "**  (None, None, None, 'batch', '62339657_1.batch', '62339660.batch', None, 'COMPLETED', None, '1', '1', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:10.977', '21:27.938', '21:38.916', '1', '1327', '00:00:00', None)\n",
      "**  (None, None, None, 'extern', '62339657_1.extern', '62339660.extern', None, 'COMPLETED', None, '1', '1', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:00:00', '00:00:00', '00:00:00', '1', '1327', '00:00:00', None)\n",
      "**  ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_2', '62339661', 'serc', 'COMPLETED', '00:59:00', '1', '1', '2020-03-01T00:17:07', '2020-03-01T00:17:08', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:11.013', '21:27.938', '21:38.951', None, '1327', '00:00:00', None)\n"
     ]
    }
   ],
   "source": [
    "# Another way to read the file with headers. This will give an effective array of (val,ky) tuples.\n",
    "rows_2 = spark.read.format('CSV').option('header', 'true').option('sep', '|').load(data_file_name)\n",
    "print('** type: ', type(rows_2))\n",
    "#\n",
    "\n",
    "for rw in rows_2.take(10):\n",
    "    print('** ', rw[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** type:  <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "** dypes:  [('User', 'string'), ('Group', 'string'), ('GID', 'string'), ('JobName', 'string'), ('JobID', 'string'), ('JobIDRaw', 'string'), ('Partition', 'string'), ('State', 'string'), ('Timelimit', 'string'), ('NCPUS', 'string'), ('NNodes', 'string'), ('Submit', 'string'), ('Eligible', 'string'), ('Start', 'string'), ('End', 'string'), ('Elapsed', 'string'), ('SystemCPU', 'string'), ('UserCPU', 'string'), ('TotalCPU', 'string'), ('NTasks', 'string'), ('CPUTimeRAW', 'string'), ('Suspended', 'string'), ('_c22', 'string')]\n",
      "** header: ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', '_c22']\n",
      "\n",
      "*** *** \n",
      "**  ('saipb', 'oneillm', '328022', 'hovmuller', '62339523', '62339523', 'serc', 'COMPLETED', '4-00:00:00', '16', '1', '2020-03-01T00:10:36', '2020-03-01T00:10:36', '2020-03-01T00:11:24', '2020-03-01T00:12:05', '00:00:41', '00:02.369', '00:17.315', '00:19.685', None, '656', '00:00:00', None)\n",
      "**  (None, None, None, 'batch', '62339523.batch', '62339523.batch', None, 'COMPLETED', None, '16', '1', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:12:05', '00:00:41', '00:02.367', '00:17.315', '00:19.683', '1', '656', '00:00:00', None)\n",
      "**  (None, None, None, 'extern', '62339523.extern', '62339523.extern', None, 'COMPLETED', None, '16', '1', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:11:24', '2020-03-01T00:12:05', '00:00:41', '00:00.001', '00:00:00', '00:00.001', '1', '656', '00:00:00', None)\n",
      "**  ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_0', '62339659', 'serc', 'COMPLETED', '00:59:00', '1', '1', '2020-03-01T00:17:07', '2020-03-01T00:17:08', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:10.982', '21:27.659', '21:38.642', None, '1327', '00:00:00', None)\n",
      "**  (None, None, None, 'batch', '62339657_0.batch', '62339659.batch', None, 'COMPLETED', None, '1', '1', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:17:14', '2020-03-01T00:39:21', '00:22:07', '00:10.981', '21:27.659', '21:38.641', '1', '1327', '00:00:00', None)\n"
     ]
    }
   ],
   "source": [
    "# Another syntax to load directly into a spark dataframe (via .sql):\n",
    "#\n",
    "df_rows = spark.read.csv(data_file_name, header=True, sep='|')\n",
    "#\n",
    "print('** type: ', type(df_rows))\n",
    "print('** dypes: ', df_rows.dtypes)\n",
    "print('** header: {}'.format( df_rows.schema.names ) )\n",
    "#\n",
    "print('\\n*** *** ')\n",
    "for rw in df_rows.take(5):\n",
    "    print('** ', rw[:])\n",
    "    #print('* * ', rw.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can count rows like:\n",
    "#\n",
    "# NOTE: the DF.count() instances may be much faster (if they are) because they are not\n",
    "#. configured with CPU constraints (which would be good news that the cpu constraints are working)\n",
    "# for rr in (rows, rows_2, df_rows, rows, rows_2, df_rows):\n",
    "#     t0 = time.time()\n",
    "#     n_rws = rr.count()\n",
    "#     print('** time: {}'.format(time.time()-t0))\n",
    "# #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  Serialized 1x Replicated MapPartitionsRDD[34] at mapPartitions at PythonRDD.scala:133\n"
     ]
    }
   ],
   "source": [
    "# this is how to fetch all the rows, but it always breaks for a large array.\n",
    "#all_rows = rows.collect()\n",
    "print('** ', rows.getStorageLevel(), rows.partitionBy(20))\n",
    "\n",
    "#grouped = rows.filter(lambda rw:rw[0]!='User').groupBy(lambda rw: rw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Pseudo-) production(ish) PySpark prodcessing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***  StructType(List(StructField(User,StringType,true),StructField(Group,StringType,true),StructField(GID,StringType,true),StructField(JobName,StringType,true),StructField(JobID,StringType,true),StructField(JobIDRaw,StringType,true),StructField(Partition,StringType,true),StructField(State,StringType,true),StructField(Timelimit,StringType,true),StructField(NCPUS,StringType,true),StructField(NNodes,StringType,true),StructField(Submit,StringType,true),StructField(Eligible,StringType,true),StructField(Start,StringType,true),StructField(End,StringType,true),StructField(Elapsed,StringType,true),StructField(SystemCPU,StringType,true),StructField(UserCPU,StringType,true),StructField(TotalCPU,StringType,true),StructField(NTasks,StringType,true),StructField(CPUTimeRAW,StringType,true),StructField(Suspended,StringType,true),StructField(_c22,StringType,true)))\n",
      "***  [('User', 'string'), ('Group', 'string'), ('GID', 'string'), ('JobName', 'string'), ('JobID', 'string'), ('JobIDRaw', 'string'), ('Partition', 'string'), ('State', 'string'), ('Timelimit', 'string'), ('NCPUS', 'string'), ('NNodes', 'string'), ('Submit', 'string'), ('Eligible', 'string'), ('Start', 'string'), ('End', 'string'), ('Elapsed', 'string'), ('SystemCPU', 'string'), ('UserCPU', 'string'), ('TotalCPU', 'string'), ('NTasks', 'string'), ('CPUTimeRAW', 'string'), ('Suspended', 'string'), ('_c22', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print('*** ', df_rows.schema)\n",
    "#\n",
    "print('*** ', df_rows.dtypes)\n",
    "#print('** ', set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  [None, None, None, 'batch', '62339523.batch', '62339523.batch', None, 'COMPLETED', None, 16, 1, 737485.0079166667, 737485.0079166667, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 2.7395833333333333e-05, 0.0002004050925925926, 0.0002278125, 1, '656', '00:00:00', '62339523']\n",
      "**  62339523.batch\n",
      "** x1_prime:  [None, None, None, 'batch', '62339523.batch', '62339523.batch', None, 'COMPLETED', None, 16, 1, 737485.0079166667, 737485.0079166667, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 2.7395833333333333e-05, 0.0002004050925925926, 0.0002278125, 1, '656', '00:00:00', '62339523']\n",
      "** x2_p types:  [<class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'NoneType'>, <class 'str'>, <class 'NoneType'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "re_typer={numpy.float64:float, numpy.float128:float}\n",
    "\n",
    "\n",
    "\n",
    "x1 = rows.take(2)[1]\n",
    "print('** ', x1)\n",
    "print('** ', type(x1[4])(x1[4]))\n",
    "#\n",
    "x1_prime = [None if x is None else re_typer.get(type(x), type(x))(x) for x in x1]\n",
    "#\n",
    "print('** x1_prime: ', x1_prime)\n",
    "print('** x2_p types: ', [type(x) for x in x1_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** n_terminal:  1\n",
      "** Headers[23]: \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "\n",
    "# raw, preliminary access to data:\n",
    "delim = '|'\n",
    "lines = sc.textFile(data_file_name)\n",
    "#\n",
    "n_terminal = 0\n",
    "header_string = lines.take(1)[0]\n",
    "#for c in header_string:\n",
    "# for c in (lines.take(2)[1])[:-1]:\n",
    "#     print('{}: [{}]'.format(c, ord(c)))\n",
    "\n",
    "while header_string[-1] in ('\\n', delim):\n",
    "    header_string=header_string[:-1]\n",
    "    n_terminal += 1\n",
    "print('** n_terminal: ', n_terminal)\n",
    "if n_terminal>0:\n",
    "    lines = lines.map(lambda ln: ln[:-n_terminal])\n",
    "#\n",
    "header_names = header_string.split(delim) + ['JobID_parent']\n",
    "RH_index = {s:k for k,s in enumerate(header_names) }\n",
    "print('** Headers[{}]: '.format(len(header_names), header_names) )\n",
    "#\n",
    "# for c in (lines.take(2)[1]):\n",
    "#     print('{}: [{}]'.format(c, ord(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to filter header rows:\n",
    "n_startswith = 15\n",
    "header_start = header_string[0:n_startswith]\n",
    "#\n",
    "rows = lines.filter(lambda s: not s.startswith(header_start) ).map(lambda x: f_rw(x, header_names,\n",
    "                                            types_dict=types_dict, RH_index=RH_index) )\n",
    "rows = rows.map(lambda rw: [None if (x is None or x=='') else re_typer.get(type(x), type(x))(x) for x in rw])\n",
    "#\n",
    "#sample_row = f_rw(lines.take(2)[1], header_names, RH_index)\n",
    "#print('*** ', [(s, type(s) )for s in sample_row])\n",
    "\n",
    "#my_dtypes = \n",
    "# we can either group and reduce using the RH{} index, or convert to a DF first:\n",
    "rows_df = spark.createDataFrame( rows, header_names ).sort('JobID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** rows_df schema[23]: StructType(List(StructField(User,StringType,true),StructField(Group,StringType,true),StructField(GID,StringType,true),StructField(JobName,StringType,true),StructField(JobID,StringType,true),StructField(JobIDRaw,StringType,true),StructField(Partition,StringType,true),StructField(State,StringType,true),StructField(Timelimit,DoubleType,true),StructField(NCPUS,LongType,true),StructField(NNodes,LongType,true),StructField(Submit,DoubleType,true),StructField(Eligible,DoubleType,true),StructField(Start,DoubleType,true),StructField(End,DoubleType,true),StructField(Elapsed,DoubleType,true),StructField(SystemCPU,DoubleType,true),StructField(UserCPU,DoubleType,true),StructField(TotalCPU,DoubleType,true),StructField(NTasks,LongType,true),StructField(CPUTimeRAW,StringType,true),StructField(Suspended,StringType,true),StructField(JobID_parent,StringType,true)))\n",
      "** dytpes[23]: [('User', 'string'), ('Group', 'string'), ('GID', 'string'), ('JobName', 'string'), ('JobID', 'string'), ('JobIDRaw', 'string'), ('Partition', 'string'), ('State', 'string'), ('Timelimit', 'double'), ('NCPUS', 'bigint'), ('NNodes', 'bigint'), ('Submit', 'double'), ('Eligible', 'double'), ('Start', 'double'), ('End', 'double'), ('Elapsed', 'double'), ('SystemCPU', 'double'), ('UserCPU', 'double'), ('TotalCPU', 'double'), ('NTasks', 'bigint'), ('CPUTimeRAW', 'string'), ('Suspended', 'string'), ('JobID_parent', 'string')]\n",
      "** header_names[23]: ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'JobID_parent']\n"
     ]
    }
   ],
   "source": [
    "print('** rows_df schema[{}]: {}'.format(len(rows_df.schema), rows_df.schema))\n",
    "print('** dytpes[{}]: {}'.format(len(rows_df.dtypes), rows_df.dtypes))\n",
    "print('** header_names[{}]: {}'.format(len(header_names), header_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows_df.show()\n",
    "print(rows_df.rdd.getNumPartitions())\n",
    "rows_df = rows_df.repartition('JobID_parent').orderBy('Submit')\n",
    "print(rows_df.rdd.getNumPartitions())\n",
    "#\n",
    "\n",
    "for rw in rows_df.take(10):\n",
    "    print('** ', rw[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df_rows.withColumn(\"row\",row_number.over(w2))\n",
    "      .where($\"row\" === 1).drop(\"row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The end (of usefulness?)\n",
    "- And this might be where usefullness ends.\n",
    "- Looks like a real-programming language approach, of grouping/aggegating/reducing by grouping, pulling the first row by default, then substituting and aggregate for selected columns (but not really caring if some of the other columns are not identical in value) is not possible\n",
    "- Looks like it needs to be much more SQL `group by` like. Can we write our own functions? Can we pass functions programmatically? Not seeing that so far...\n",
    "- *Maybe!* I think we want to do it this way: https://sparkbyexamples.com/spark/spark-dataframe-how-to-select-the-first-row-of-each-group/\n",
    "- Which I think is the DF method. There are ways to (maybe) do it in an RDD with or without converting to a key-value array (like {JobID_parent: full_row} ), but those are supposed to be much less compute efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, let's try a RDD_pair, then reduce_by_key() function:\n",
    "# (this appears to work, but needs to be validated )\n",
    "row_pairs = rows.map(lambda x: (x[RH_index['JobID_parent']], list(x[:])))\n",
    "#\n",
    "group_py_functions = {'End':numpy.nanmax, 'Start':numpy.nanmin, 'NCPUS':numpy.nanmax, 'NNodes':numpy.nanmax}\n",
    "def f_reduce_row(r1, r2):\n",
    "     return tuple([group_py_functions.get(hdr, lambda x: x[0] )([x1, x2]) \n",
    "             for k, (hdr,x1,x2) in enumerate(zip(header_names, r1, r2)) ] )\n",
    "summary_rdd = row_pairs.reduceByKey(f_reduce_row)\n",
    "#summary_rdd = row_pairs.reduceByKey(lambda x1,x2: x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  ('62339657_34', ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_34', '62339848', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0134027777, 737485.0287037037, 0.015300925925925926, 0.00012969907407407407, 0.014705127314814813, 0.014834837962962963, None, '1322', '00:00:0', '62339657_34'))\n",
      "**  ('62339657_252', ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_252', '62341000', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0169097222, 737485.0321759259, 0.015243055555555555, 0.0001260648148148148, 0.014826122685185187, 0.014952199074074073, None, '1317', '00:00:0', '62339657_252'))\n",
      "**  ('62339657_381', ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_381', '62342165', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0274652778, 737485.0428356482, 0.015370370370370371, 0.00012980324074074074, 0.014864444444444444, 0.014994259259259258, None, '1328', '00:00:0', '62339657_381'))\n",
      "**  ('62339657_439', ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_439', '62342305', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0292939815, 737485.0443865741, 0.015081018518518518, 0.00012501157407407407, 0.014665844907407407, 0.014790856481481483, None, '1303', '00:00:0', '62339657_439'))\n",
      "**  ('62339657_441', ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_441', '62342380', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0301736111, 737485.0453125, 0.01513888888888889, 0.0001263310185185185, 0.014724548611111112, 0.014850891203703703, None, '1308', '00:00:0', '62339657_441'))\n",
      "**  ('62339657_388', ('pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_388', '62342172', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0336226852, 737485.0350231482, 737485.0356597222, 737485.0508217593, 0.015162037037037036, 0.00012931712962962962, 0.014790347222222222, 0.014919664351851852, None, '1310', '00:00:0', '62339657_388'))\n",
      "**  ('62344473_22', ('pjwomble', 'gorelick', '26961', '6dff7437940be', '62344473_22', '62344501', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0677546297, 737485.0678472222, 737485.0682291667, 737485.0839120371, 0.015370370370370371, 0.00015361111111111112, 0.014749409722222221, 0.014903020833333334, None, '1328', '00:00:0', '62344473_22'))\n",
      "**  ('62344473_28', ('pjwomble', 'gorelick', '26961', '6dff7437940be', '62344473_28', '62344507', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0677546297, 737485.0678472222, 737485.0682291667, 737485.0824884259, 0.01425925925925926, 0.00011451388888888889, 0.013884953703703705, 0.013999467592592593, None, '1232', '00:00:0', '62344473_28'))\n",
      "**  ('62344473_33', ('pjwomble', 'gorelick', '26961', '6dff7437940be', '62344473_33', '62344515', 'serc', 'REQUEUED', 0.04097222222222222, 1, 1, 737485.0677546297, 737485.0678472222, 737485.0684143519, 737485.0685185185, 9.259259259259259e-05, 0.0, 0.0, 0.0, None, '8', '00:00:0', '62344473_33'))\n",
      "**  ('62344473_158', ('pjwomble', 'gorelick', '26961', '6dff7437940be', '62344473_158', '62344997', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0677546297, 737485.0678472222, 737485.0839814815, 737485.0994328704, 0.01545138888888889, 0.0001347337962962963, 0.015062986111111112, 0.015197731481481482, None, '1335', '00:00:0', '62344473_158'))\n"
     ]
    }
   ],
   "source": [
    "for rw in summary_rdd.take(10):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  ('62339523', ['saipb', 'oneillm', '328022', 'hovmuller', '62339523', '62339523', 'serc', 'COMPLETED', 4.0, 16, 1, 737485.0073611111, 737485.0073611111, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 2.7418981481481484e-05, 0.0002004050925925926, 0.00022783564814814812, None, '656', '00:00:0', '62339523'])\n",
      "**  ('62339523', [None, None, None, 'batch', '62339523.batch', '62339523.batch', None, 'COMPLETED', None, 16, 1, 737485.0079166667, 737485.0079166667, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 2.7395833333333333e-05, 0.0002004050925925926, 0.0002278125, 1, '656', '00:00:0', '62339523'])\n",
      "**  ('62339523', [None, None, None, 'extern', '62339523.extern', '62339523.extern', None, 'COMPLETED', None, 16, 1, 737485.0079166667, 737485.0079166667, 737485.0079166667, 737485.0083912037, 0.00047453703703703704, 1.1574074074074074e-08, 0.0, 1.1574074074074074e-08, 1, '656', '00:00:0', '62339523'])\n",
      "**  ('62339657_0', ['pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_0', '62339659', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012710648148148146, 0.01490346064814815, 0.015030578703703704, None, '1327', '00:00:0', '62339657_0'])\n",
      "**  ('62339657_0', [None, None, None, 'batch', '62339657_0.batch', '62339659.batch', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012709490740740742, 0.01490346064814815, 0.01503056712962963, 1, '1327', '00:00:0', '62339657_0'])\n",
      "**  ('62339657_0', [None, None, None, 'extern', '62339657_0.extern', '62339659.extern', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.0, 0.0, 0.0, 1, '1327', '00:00:0', '62339657_0'])\n",
      "**  ('62339657_1', ['pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_1', '62339660', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012704861111111112, 0.01490670138888889, 0.015033761574074073, None, '1327', '00:00:0', '62339657_1'])\n",
      "**  ('62339657_1', [None, None, None, 'batch', '62339657_1.batch', '62339660.batch', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012704861111111112, 0.014906689814814815, 0.015033749999999999, 1, '1327', '00:00:0', '62339657_1'])\n",
      "**  ('62339657_1', [None, None, None, 'extern', '62339657_1.extern', '62339660.extern', None, 'COMPLETED', None, 1, 1, 737485.0119675926, 737485.0119675926, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.0, 0.0, 0.0, 1, '1327', '00:00:0', '62339657_1'])\n",
      "**  ('62339657_2', ['pjwomble', 'gorelick', '26961', '6dff71d6eaf0c', '62339657_2', '62339661', 'serc', 'COMPLETED', 0.04097222222222222, 1, 1, 737485.0118865741, 737485.0118981481, 737485.0119675926, 737485.0273263889, 0.015358796296296296, 0.00012746527777777778, 0.014906689814814815, 0.015034155092592592, None, '1327', '00:00:0', '62339657_2'])\n"
     ]
    }
   ],
   "source": [
    "for rw in row_pairs.take(10):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  {'User': 0, 'Group': 1, 'GID': 2, 'JobName': 3, 'JobID': 4, 'JobIDRaw': 5, 'Partition': 6, 'State': 7, 'Timelimit': 8, 'NCPUS': 9, 'NNodes': 10, 'Submit': 11, 'Eligible': 12, 'Start': 13, 'End': 14, 'Elapsed': 15, 'SystemCPU': 16, 'UserCPU': 17, 'TotalCPU': 18, 'NTasks': 19, 'CPUTimeRAW': 20, 'Suspended': 21, 'JobID_parent': 22}\n",
      "**  ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'JobID_parent']\n"
     ]
    }
   ],
   "source": [
    "print('** ', RH_index)\n",
    "print('** ', header_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples using group(). Basically, not very useful for what we want to do, since we want to keep the\n",
    "#.  non-grouped rows and we can't guarantee uniqueness of the extra col values.\n",
    "\n",
    "# dummy(-ish) functions to handle grouping operations.\n",
    "def grp_first(X):\n",
    "    return X[0]\n",
    "#\n",
    "# can we use regular numpy functions, or do we need to use PySpark SQL functions?\n",
    "import pyspark.sql.functions as psf\n",
    "group_functions = {'End':psf.max, 'Start':psf.min, 'NCPUS':psf.max, 'NNodes':psf.max}\n",
    "group_function_names = {'End':'max', 'Start':'min', 'NCPUS':'max', 'NNodes':'max'}\n",
    "#group_functions = {'End':numpy.nanmax, 'Start':numpy.nanmin, 'NCPUS':numpy.nanmax, 'NNodes':numpy.nanmax}\n",
    "#\n",
    "# maybe this:?:\n",
    "#jobs_summary =  rows_df.groupBy('JobID_parent').agg({cl:group_functions.get(cl, grp_first)\n",
    "#                                                          for cl in header_names})\n",
    "\n",
    "# jobs_summary =  rows_df.groupBy('JobID_parent').agg({cl:group_functions.get(cl, grp_first)\n",
    "#                                                           for cl in header_names})\n",
    "jobs_summary =  rows_df.groupBy('JobID_parent').agg({cl:f for cl,f in group_function_names.items()})\n",
    "#jobs_summary = rows_df.groupBy('JobID_parent').agg({'End':'max'})\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_group = rows_df.groupBy('JobID_parent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rw in js_group.count().take(5) :\n",
    "    print('*** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = js_group.agg(psf.collect_list('JobID_parent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rw in collected.take(5):\n",
    "    print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('** lines: ')\n",
    "# for ln in lines.take(10):\n",
    "# \tprint('** **: ', ln)\n",
    "#\n",
    "print('rows: ')\n",
    "for rw in rows.take(10):\n",
    "\tprint('** **: ', rw)\n",
    "#\n",
    "#\n",
    "# print('** rows again: ')\n",
    "# for rw in rows.take(10):\n",
    "#         print('** **: ', rw)\n",
    "#\n",
    "print('** groups: ')\n",
    "for rw in grouped.take(10):\n",
    "    print('* *: ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin all_rows[]: ')\n",
    "t0 = time.time()\n",
    "#\n",
    "all_rows = rows.collect()\n",
    "#\n",
    "print('** time: {}'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin all_lines: ')\n",
    "t0 = time.time()\n",
    "#\n",
    "all_lines = lines.collect()\n",
    "#\n",
    "print('** time: {}'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** sc: ', sc.defaultParallelism)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** platform:  UIT-C02YT0LBLVDP.local\n",
      "hostname:  UIT-C02YT0LBLVDP.local\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import pytz\n",
    "import multiprocessing as mpp\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import socket\n",
    "import h5py\n",
    "#import lmod\n",
    "# lmod.load('system')\n",
    "# lmod.load('texlive')\n",
    "# lmod.\n",
    "#\n",
    "# TODO: phase out unreferenced hpc_lib calls...\n",
    "import hpc_lib\n",
    "import hpc_reports\n",
    "#\n",
    "# def running_mean(X,n=10):\n",
    "#     return (numpy.cumsum(numpy.insert(X,0,0))[n:] - numpy.cumsum(numpy.insert(X,0,0))[:-n])/n\n",
    "# #\n",
    "import platform\n",
    "print('** platform: ', platform.node() )\n",
    "\n",
    "import socket\n",
    "print('hostname: ', socket.gethostname() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Count integration\n",
    "\n",
    "Integrate GPU counts into jobs_summary, so instead of having a seperate method to extract and cound GPUs, we count them during calc_jobs_summary(). This should also give more accurate count (???), in the event that it is possible to miss a GPU request for  a job where the request occurs in a later step. I actually don't think this is possible; the first line of the job group should always have the full allocation, which will include the GPUs, but just in case something strange happens...\n",
    "\n",
    "NOTE: Since this involves actual computation of jobs_summary, it has to be run on Sherlock (or similar)... or could be done from a downloaded data file, but we don't really want to do that...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** epoch: 1970-01-01T00:00:00\n"
     ]
    }
   ],
   "source": [
    "n_cpus = 4\n",
    "print('** epoch: {}'.format(mpd.get_epoch()))\n",
    "print('*** ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** dates: 2022-01-07 - 2022-07-06\n",
      "*** sacct_h5_file: sacct_data/sacct_pSERC_gNONE_uNone_20220107_20220706.h5\n",
      "*** h5_file exists: True\n"
     ]
    }
   ],
   "source": [
    "N_report_len = 180\n",
    "end_dtm = dtm.datetime(2022,7,6)\n",
    "end_date = end_dtm.date()\n",
    "start_date = end_date - dtm.timedelta(days=N_report_len)\n",
    "print('*** dates: {} - {}'.format(start_date, end_date))\n",
    "partition='serc'\n",
    "#partition=None\n",
    "group=None\n",
    "s_user=None\n",
    "verbose=0\n",
    "# group='oneillm'\n",
    "# group='edunham'\n",
    "#s_user = 'labraha2'\n",
    "#\n",
    "sacct_h5_file = f'sacct_data/sacct_p{str(partition).upper()}_g{str(group).upper()}_u{s_user}_{start_date.year:04}{start_date.month:02}{start_date.day:02}_{end_dtm.year:04}{end_dtm.month:02}{end_dtm.day:02}.h5'\n",
    "print(f'*** sacct_h5_file: {sacct_h5_file}')\n",
    "print(f'*** h5_file exists: {os.path.isfile(sacct_h5_file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Creating SACCT object from HDF5 sacct_data/sacct_pSERC_gNONE_uNone_20220107_20220706.h5\n",
      "**  ('index', 'User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'ReqTRES', 'AllocTRES', 'MaxRSS', 'AveRSS', 'AveVMSize', 'MaxVMSize', 'MaxDiskWrite', 'MaxDiskRead', 'AveDiskWrite', 'AveDiskRead', 'JobID_parent')\n"
     ]
    }
   ],
   "source": [
    "# NOTE: to get individual users, eg to get Eric's group usage:\n",
    "#. sacct --allusers --user=labraha2 --start=2022-01-01 --end=2022-06-15\n",
    "#\n",
    "# NOTE: additional options can be passed in the more_options=[] arrary, or just as sacct_{option-name}={val}\n",
    "#\n",
    "if os.path.isfile(sacct_h5_file):\n",
    "    print('*** Creating SACCT object from HDF5 {}'.format(sacct_h5_file))\n",
    "    SACCT_obj = hpc_lib.SACCT_data_from_h5(sacct_h5_file, keep_raw_data=False, n_cpu=n_cpus)\n",
    "    #\n",
    "    \n",
    "else:\n",
    "    print('*** Fetching SACCT data directly')\n",
    "    SACCT_obj = hpc_lib.SACCT_data_direct(group=group, partition=partition, start_date=str(start_date),\n",
    "                                          n_cpu=n_cpus, verbose=verbose,\n",
    "                                          end_date=str(end_date), keep_raw_data=False)\n",
    "    # , sacct_user=s_user\n",
    "    print(f'** writing HDF5: {sacct_h5_file}')\n",
    "    SACCT_obj.write_hdf5(sacct_h5_file)\n",
    "    #\n",
    "#\n",
    "print('** ', SACCT_obj.jobs_summary.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***  [('index', '<i8'), ('User', 'S8'), ('Group', 'S8'), ('GID', 'S6'), ('JobName', 'S81'), ('JobID', 'S67'), ('JobIDRaw', 'S8'), ('Partition', 'S4'), ('State', 'S19'), ('Timelimit', '<f8'), ('NCPUS', '<i8'), ('NNodes', '<i8'), ('Submit', '<f8'), ('Eligible', '<f8'), ('Start', '<f8'), ('End', '<f8'), ('Elapsed', '<f8'), ('SystemCPU', '<f8'), ('UserCPU', '<f8'), ('TotalCPU', '<f8'), ('NTasks', '<f8'), ('CPUTimeRAW', 'S9'), ('Suspended', 'S8'), ('ReqTRES', 'S49'), ('AllocTRES', 'S61'), ('MaxRSS', '<f8'), ('AveRSS', '<f8'), ('AveVMSize', '<f8'), ('MaxVMSize', '<f8'), ('MaxDiskWrite', '<f8'), ('MaxDiskRead', '<f8'), ('AveDiskWrite', '<f8'), ('AveDiskRead', '<f8'), ('JobID_parent', 'S67')]\n"
     ]
    }
   ],
   "source": [
    "print('*** ', SACCT_obj.jobs_summary.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** my_js2:  [(0, b'haofu', b'oneillm', b'328022', b'coldpool_LES', b'41480792_1', b'41480792', b'serc', b'TIMEOUT', 6., 96, 6, 18993.88233796, 18993.88238426, 18993.88241898, 18999.88261574, 6.00015046, 85.3362963 , 160.51142361, 245.84771991, 6., b'49767648', b'00:00:00', b'billing=283,cpu=96,mem=750G,node=6', b'billing=283,cpu=96,mem=750G,node=6', 6.529478e+09, 6.257395e+09, 1.8476052e+10, 1.8476052e+10, 1.36872348e+13, 1.76223064e+13, 1.31590362e+13, 1.76223064e+13, b'41480792_1', 42)\n",
      " (4, b'haofu', b'oneillm', b'328022', b'coldpool_LES', b'41480918_1', b'41480918', b'serc', b'TIMEOUT', 6., 96, 6, 18993.88387731, 18993.88390046, 18993.88394676, 18999.88402778, 6.00003472, 74.95368056, 146.86747685, 221.82115741, 6., b'49766688', b'00:00:00', b'billing=283,cpu=96,mem=750G,node=6', b'billing=283,cpu=96,mem=750G,node=6', 6.533113e+09, 6.258025e+09, 1.8475916e+10, 1.8475916e+10, 1.39023542e+13, 1.77343609e+13, 1.33263408e+13, 1.77343609e+13, b'41480918_1', 27)\n",
      " (0, b'', b'', b'', b'', b'', b'', b'', b'', 0.,  0, 0,     0.        ,     0.        ,     0.        ,     0.        , 0.        ,  0.        ,   0.        ,   0.        , 0., b'', b'', b'', b'', 0.000000e+00, 0.000000e+00, 0.0000000e+00, 0.0000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, b'',  0)\n",
      " (0, b'', b'', b'', b'', b'', b'', b'', b'', 0.,  0, 0,     0.        ,     0.        ,     0.        ,     0.        , 0.        ,  0.        ,   0.        ,   0.        , 0., b'', b'', b'', b'', 0.000000e+00, 0.000000e+00, 0.0000000e+00, 0.0000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, b'',  0)\n",
      " (0, b'', b'', b'', b'', b'', b'', b'', b'', 0.,  0, 0,     0.        ,     0.        ,     0.        ,     0.        , 0.        ,  0.        ,   0.        ,   0.        , 0., b'', b'', b'', b'', 0.000000e+00, 0.000000e+00, 0.0000000e+00, 0.0000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, b'',  0)]\n"
     ]
    }
   ],
   "source": [
    "my_js  = SACCT_obj.jobs_summary.copy()\n",
    "print('** size: {}, shape: {}, len: {}'.format( my_js.size, my_js.shape, len(my_js) ) )\n",
    "#\n",
    "dtp= numpy.dtype([(n, t[0] if isinstance(t,tuple) else t) for n,t in my_js.dtype.descr ] + [('NGPUS', '<i8')])\n",
    "#dtp = SACCT_obj.jobs_summary.dtype.descr\n",
    "#+ [('NGPUS', '<i8')])\n",
    "\n",
    "print('* *: ', dtp)\n",
    "print('\\n** **: ', SACCT_obj.jobs_summary.dtype.descr )\n",
    "\n",
    "my_js2 = numpy.zeros(shape=(len(my_js), ), dtype=dtp)\n",
    "#\n",
    "for x in my_js2.dtype.descr:\n",
    "    print('** ', x)\n",
    "\n",
    "my_js2[list(my_js.dtype.names)][0] = my_js[0]\n",
    "my_js2[list(my_js.dtype.names)][1] = tuple(my_js[list(my_js.dtype.names)][1])\n",
    "#\n",
    "my_js2['NGPUS'][0] = 42\n",
    "my_js2['NGPUS'][1] = 27\n",
    "\n",
    "print('** my_js2: ', my_js2[0:5])\n",
    "\n",
    "\n",
    "CF = [('End', 'End',  numpy.nanmax), ('Start', 'Start', numpy.nanmax), ('NCPUS', 'NCPUS', numpy.nanmax),\n",
    "      ('NNodes', 'NNodes', lambda x: 1+numpy.nanmax(x) ), \n",
    "      ('NGPUs', 'AllocTRES', lambda x: numpy.nanmax(hpc_lib.get_NGPUs(x)).astype(int) )]\n",
    "\n",
    "print('** ', my_js[cls][2])\n",
    "\n",
    "cls = [n1 for (n1, n2, f) in CF]\n",
    "cls2 = [n2 for (n1, n2, f) in CF]\n",
    "my_js2[cls][2] = tuple([f(x) for (n1, n2, f),x in zip(CF, my_js[cls2][2])])\n",
    "\n",
    "print('** my_js2: ', my_js2[0:5])\n",
    "\n",
    "print('** ', my_js[cls][2])\n",
    "print('*** ', my_js2[cls][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('** ', SACCT_obj.jobs_summary.dtype.descr)\n",
    "# dtp = [(n, t[0] if isinstance(t,tuple) else t) for n,t in SACCT_obj.jobs_summary.dtype.descr ]\n",
    "\n",
    "# #\n",
    "# ZZ = numpy.zeros( shape=(100, ), dtype=dtp)\n",
    "# ZZ = SACCT_obj[0:100]\n",
    "# print('** ', ZZ[0:10])\n",
    "\n",
    "# # check date epoch:\n",
    "# dt_test = SACCT_obj.jobs_summary['Start'][0]\n",
    "# yr_test = mpd.num2date(dt_test).year\n",
    "# print(f'** yr_test: {yr_test}')\n",
    "# #print('** ', mpd.num2epoch(dt_test))\n",
    "# if yr_test>3000:\n",
    "#     mpd.set_epoch('0000-12-31T00:00:00')\n",
    "# yr_test = mpd.num2date(dt_test)\n",
    "# print(f'** yr_test: {yr_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuh_layers = SACCT_obj.get_cpu_hours_layer_cake(bin_size=1., layer_field='Group',\n",
    "                                                 t_max=mpd.date2num(end_date) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU-Hours, Jobs layer cake + Pie charts.\n",
    "- Fist, the semi-manual construction. Uses some consolidated functions (eg, _layer_cake() and plot_pie() )\n",
    "- Then, these all wrapped up into one line or data + figures setup, then one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(20,16))\n",
    "ax1 = fg.add_subplot(2,2,1)\n",
    "ax2 = fg.add_subplot(2,2,2)\n",
    "ax3 = fg.add_subplot(2,2,3)\n",
    "ax4 = fg.add_subplot(2,2,4)\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "#\n",
    "ax1.set_title('CPU Hours (per day)', size=16)\n",
    "ax2.set_title('Jobs (per day?)', size=16)\n",
    "#\n",
    "#cpuh_layers = SACCT_obj.get_cpu_hours_layer_cake(bin_size=1., layer_field='Group')\n",
    "group_by = 'Group'\n",
    "cpuh = cpuh_layers['cpu_hours']\n",
    "jobs = cpuh_layers['jobs']\n",
    "T = cpuh['time']\n",
    "#\n",
    "z_cpuh = hpc_lib.plot_layer_cake(data=cpuh, layers=cpuh.dtype.names[1:], time_col='time', ax=ax1)\n",
    "z_jobs = hpc_lib.plot_layer_cake(data=jobs, layers=cpuh.dtype.names[1:], time_col='time', ax=ax2)\n",
    "#\n",
    "pie_cpuh_data = hpc_lib.plot_pie(sum_data=SACCT_obj['Elapsed']*SACCT_obj['NCPUS'],\n",
    "                                slice_data=SACCT_obj[group_by], ax=ax3\n",
    "                                )\n",
    "#pi_slices = ax.pie(pi_slices[:,1], labels=pi_slices[:,0])\n",
    "pie_jobs_data = hpc_lib.plot_pie(sum_data=SACCT_obj['Elapsed'], slice_data=SACCT_obj[group_by], \n",
    "                            ax=ax4)\n",
    "#\n",
    "ax1.legend(loc=0)\n",
    "ax2.legend(loc=0)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(20,16))\n",
    "ax1 = fg.add_subplot(2,2,1)\n",
    "ax2 = fg.add_subplot(2,2,2)\n",
    "ax3 = fg.add_subplot(2,2,3)\n",
    "ax4 = fg.add_subplot(2,2,4)\n",
    "\n",
    "zz = SACCT_obj.report_cpuhours_jobs_layercake_and_pie(fg=fg, cpuh_jobs=cpuh_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ep = hpc_lib.compute_mpd_epoch_dt(738150)\n",
    "print('** ', dt_ep)\n",
    "print('** ', SACCT_obj.compute_mpd_epoch_dt())\n",
    "print('** *', hpc_lib.compute_mpd_epoch_dt(dtm.datetime.now()))\n",
    "#\n",
    "#print('** ', isinstance(numpy.datetime64(dtm.datetime.now())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acpu_layer_cake = SACCT_obj.get_active_cpus_layer_cake(layer_field='Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** ', SACCT_obj.dt_mpd_epoch)\n",
    "print('*** ', mpd.date2num(dtm.datetime.now()), numpy.min(SACCT_obj['Start']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qq = SACCT_obj.report_activecpus_jobs_layercake_and_CDFs(n_points=5000,\n",
    "                        group_by='Group', acpu_layer_cake=None)\n",
    "# acpu_layer_cake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = numpy.array([SACCT_obj['Start'], SACCT_obj['End']])\n",
    "\n",
    "print('*** ', len(ZZ.ravel()) )\n",
    "\n",
    "\n",
    "#print('** ', numpy.nanmin([SACCT_obj['Start'], SACCT_obj['End'], mpd.date2num(dtm.datetime.now())] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = SACCT_obj.active_cpu_jobs_per_day_hour_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGPUS = [s.split('gpu=')[1].split(',')[0] if 'gpu=' in s else 0 \n",
    "         for s in SACCT_obj.jobs_summary['AllocTRES'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpus=SACCT_obj.get_NGPUs()\n",
    "ngpus=ngpus[ngpus>0]\n",
    "#\n",
    "hh = plt.hist(ngpus, bins=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mazama GPU activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maz_h5 = 'sacct_data/sacct_pNONE_gNONE_uNone_20211222_20220620.h5'\n",
    "# SACCT_maz = SACCT_obj = hpc_lib.SACCT_data_from_h5(maz_h5, keep_raw_data=False, n_cpu=n_cpus)\n",
    "\n",
    "# # serc_gpu_activity = SACCT_serc.active_jobs_cpu(jobs_summary=None, bin_size=1., \n",
    "# #                                                NCPUs=SACCT_serc.get_NGPUs())\n",
    "\n",
    "# gpu_layers = SACCT_maz.get_active_cpus_layer_cake(layer_field='Partition', NCPUs=SACCT_serc.get_NGPUs())\n",
    "\n",
    "# # For MAZAMA\n",
    "# #\n",
    "# pi_gpu_grps = [s for s in gpu_layers['N_cpu'].dtype.names[1:]]\n",
    "# print(f'** {pi_gpu_lbls}' )\n",
    "# #\n",
    "# NGPU = SACCT_serc.get_NGPUs()\n",
    "# #\n",
    "# #pi_gpu_vals = numpy.zeros(len(pi_gpu_lbls))\n",
    "# pi_gpu_vals = []\n",
    "# pi_gpu_lbls = []\n",
    "# for k,g in enumerate(pi_gpu_grps):\n",
    "#     ix = SACCT_serc.jobs_summary['Group'].astype(str)==g\n",
    "#     #\n",
    "#     n_gpus = numpy.sum(SACCT_serc.jobs_summary['Elapsed'][ix] * NGPU[ix])\n",
    "#     if n_gpus <= 0.:\n",
    "#         continue\n",
    "#     #\n",
    "#     pi_gpu_vals += [n_gpus]\n",
    "#     pi_gpu_lbls += [f'{g}: {pi_gpu_vals[-1]:.1f}']\n",
    "# #\n",
    "# print('** vals: ', pi_gpu_vals)\n",
    "# #pi_gpu_lbls = [f'{s}: {v:.1f}' for s,v in zip(pi_gpu_lbls, pi_gpu_vals) ]\n",
    "\n",
    "\n",
    "# fg = plt.figure(figsize=(18,16))\n",
    "# ax1 = fg.add_subplot(2,1,1)\n",
    "# ax2 = fg.add_subplot(2,2,3)\n",
    "# ax3 = fg.add_subplot(2,2,4)\n",
    "# ax1.grid()\n",
    "# #ax2.grid()\n",
    "# ax3.grid()\n",
    "# #\n",
    "# hpc_lib.plot_layer_cake(gpu_layers['N_cpu'], ax=ax1)\n",
    "# z_gpus = ax1.lines[-1].get_ydata()\n",
    "# qs = [.5, .75, .9]\n",
    "# qs_gpu = numpy.quantile(z_gpus, qs)\n",
    "# #\n",
    "# print('*** keys(): ', gpu_layers.keys())\n",
    "# ax2.pie(pi_gpu_vals, labels=pi_gpu_lbls) \n",
    "# ax2.legend(loc=0)\n",
    "# #\n",
    "# hh_cpus = ax3.hist(z_gpus, bins=100, cumulative=True, density=True, histtype='step', lw=3.)\n",
    "# for x,y in zip(qs_gpu, qs):\n",
    "#     #ax3.plot([0., qs_cpus[-1], qs_cpus[-1]], [qs[-1], qs[-1], 0.], ls='--', color='r', lw=2. )\n",
    "#     ax3.plot([0., x, x], [y, y, 0.], ls='--', lw=2., label=f'{y*100.}th %: {x:.0f} gpus' )\n",
    "\n",
    "# ax1.set_title('Active GPUs', size=16)\n",
    "# ax3.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SERC GPU activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serc_h5 = 'sacct_data/serc_sacct_20211126_20220525.h5'\n",
    "# SACCT_serc = hpc_lib.SACCT_data_from_h5(serc_h5, keep_raw_data=False, n_cpu=n_cpus)\n",
    "\n",
    "serc_gpu_activity = SACCT_obj.active_jobs_cpu(jobs_summary=None, bin_size=1., \n",
    "                                               NCPUs=SACCT_obj.get_NGPUs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_layers = SACCT_obj.get_active_cpus_layer_cake(layer_field='Group',\n",
    "                        NCPUs=SACCT_obj.get_NGPUs(), t_max=mpd.date2num(dtm.datetime(2022,7,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_gpu_grps = [s for s in gpu_layers['N_cpu'].dtype.names[1:]]\n",
    "#\n",
    "NGPU = SACCT_obj.get_NGPUs()\n",
    "#\n",
    "#pi_gpu_vals = numpy.zeros(len(pi_gpu_lbls))\n",
    "pi_gpu_vals = []\n",
    "pi_gpu_lbls = []\n",
    "for k,g in enumerate(pi_gpu_grps):\n",
    "    ix = SACCT_obj.jobs_summary['Group'].astype(str)==g\n",
    "    #\n",
    "    n_gpus = numpy.sum(SACCT_obj.jobs_summary['Elapsed'][ix] * NGPU[ix])\n",
    "    if n_gpus <= 0.:\n",
    "        continue\n",
    "    #\n",
    "    pi_gpu_vals += [n_gpus]\n",
    "    pi_gpu_lbls += [f'{g}: {pi_gpu_vals[-1]:.1f}']\n",
    "#\n",
    "print('** vals: ', pi_gpu_vals)\n",
    "print(f'** {pi_gpu_lbls}' )\n",
    "#pi_gpu_lbls = [f'{s}: {v:.1f}' for s,v in zip(pi_gpu_lbls, pi_gpu_vals) ]\n",
    "\n",
    "\n",
    "fg = plt.figure(figsize=(18,16))\n",
    "ax1 = fg.add_subplot(2,1,1)\n",
    "ax2 = fg.add_subplot(2,2,3)\n",
    "ax3 = fg.add_subplot(2,2,4)\n",
    "ax1.grid()\n",
    "#ax2.grid()\n",
    "ax3.grid()\n",
    "#\n",
    "hpc_lib.plot_layer_cake(gpu_layers['N_cpu'], ax=ax1)\n",
    "z_gpus = ax1.lines[-1].get_ydata()\n",
    "qs = [.5, .75, .9]\n",
    "qs_gpu = numpy.quantile(z_gpus, qs)\n",
    "#\n",
    "print('*** keys(): ', gpu_layers.keys())\n",
    "ax2.pie(pi_gpu_vals, labels=pi_gpu_lbls) \n",
    "ax2.legend(loc=0)\n",
    "#\n",
    "hh_cpus = ax3.hist(z_gpus, bins=100, cumulative=True, density=True, histtype='step', lw=3.)\n",
    "for x,y in zip(qs_gpu, qs):\n",
    "    #ax3.plot([0., qs_cpus[-1], qs_cpus[-1]], [qs[-1], qs[-1], 0.], ls='--', color='r', lw=2. )\n",
    "    ax3.plot([0., x, x], [y, y, 0.], ls='--', lw=2., label=f'{y*100.}th %: {x:.0f} gpus' )\n",
    "\n",
    "ax1.set_title('Active GPUs', size=16)\n",
    "ax3.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', SACCT_obj.start_date, SACCT_obj.end_date)\n",
    "print('** ', type(SACCT_obj.start_date), type(SACCT_obj.end_date))\n",
    "# print('*** ', start_date, type(start_date))\n",
    "# #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('** ', gpu_layers['N_cpu'].dtype.names)\n",
    "ngpus = SACCT_obj.get_NGPUs()\n",
    "ix_g = ngpus>0\n",
    "#\n",
    "for k, (n,s) in enumerate(zip(ngpus[ix_g],SACCT_obj['AllocTRES'][ix_g])):\n",
    "    #print('** ', k, n, s)\n",
    "    #if k>15:  break\n",
    "    if 'v100' in s.decode().lower():\n",
    "        print('** ', k, n, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_jobs_serc = SACCT_serc.active_jobs_cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serc_cpu_qs = numpy.quantile(cpu_jobs_serc['N_cpu'], [.5, .75, .9])\n",
    "print('** qs: ', serc_cpu_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acpu = acpu_layer_cake['N_cpu']\n",
    "fg = plt.figure(figsize=(18,16))\n",
    "ax1 = fg.add_subplot(1,1,1)\n",
    "#\n",
    "ax1.grid()\n",
    "ax1.plot(acpu['time'], acpu['tgp'])\n",
    "\n",
    "fg.canvas.draw()\n",
    "#lbls = [hpc_lib.simple_date_string(mpd.num2date(float(hpc_lib.fix_to_ascii(str(s.get_text()))) - hpc_lib.dt_mpd_epoch) ) \n",
    "#         for s in ax1.get_xticklabels()]\n",
    "lbls = [hpc_lib.simple_date_string(mpd.num2date(float(hpc_lib.fix_to_ascii(str(s.get_text()))) + SACCT_obj.dt_mpd_epoch) ) \n",
    "         for s in ax1.get_xticklabels()]\n",
    "ax1.set_xticklabels(lbls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** ', SACCT_serc.jobs_summary.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** ', hpc_lib.kmg_to_num('105k'))\n",
    "print('** ', [hpc_lib.kmg_to_num(s) for s in ['11k', '11m', '11M', '11g']])\n",
    "print('** ', [hpc_lib.kmg_to_num(s) for s in ['11', '11.3', '', 's']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', SACCT_obj.jobs_summary.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(13,8))\n",
    "ax = fg.add_subplot(1,1,1)\n",
    "#\n",
    "print('** ', numpy.sum([not numpy.isnan(x) for x in SACCT_obj.jobs_summary['MaxRSS']]))\n",
    "hh = ax.hist(SACCT_obj.jobs_summary['MaxRSS'], bins=100, histtype='step', cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('hostname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', SACCT_obj.h5in_file)\n",
    "print('** ', mpd.date2num(dtm.datetime(2022,7,6)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import multiprocessing as mpp\n",
    "#\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpc_lib import *\n",
    "#\n",
    "# get header, print a few rows:\n",
    "#data_file_name = 'data/sacct_out.out'\n",
    "#data_file_name = 'data/sacct_mazama_out.out'\n",
    "data_file_name = 'data/sacct_sherlock_out_hns.out'\n",
    "#data_file_name = 'data/sacct_sherlock_out_oneillm.out'\n",
    "#\n",
    "# translation dictionary. Note the column input parameters (aka, --format=...) are not the same as\n",
    "#.  the colunm names.\n",
    "#\n",
    "# with -p --delimiter='|', we get:\n",
    "#. ['User', 'JobID', 'JobName', 'Partition', 'State', 'Timelimit', 'Start', 'End',\n",
    "#'Elapsed', 'MaxRSS', 'MaxVMSize', 'NNodes', 'NCPUS', '']\n",
    "#\n",
    "# how well does PANDAS automagically handle types? Maybe we should just add all available columns to this:\n",
    "# NOTE: System-,User-,Total-CPU is a string with multiple values (time, energy, etc.)\n",
    "types_dict={'User':str, 'JobID':str, 'JobName':str, 'Partition':str, 'State':str,\n",
    "            'Timelimit':str,'Start':str2date, 'End':str2date, 'Submit':str2date, 'Elapsed':elapsed_time_2_sec, 'MaxRSS':str,\n",
    "            'MaxVMSize':str, 'NNodes':int, 'NCPUS':int, 'MinCPU':str, 'SystemCPU':str, 'UserCPU':str,\n",
    "            'TotalCPU':str}\n",
    "delim = '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How dtypes work in PANDAS:\n",
    "#\n",
    "# #data_df2['Submit']=mpd.datestr2num(data_df2['Submit'])\n",
    "# print('** ', data_df2.dtypes)\n",
    "# print('** ', data_df2.dtypes.index, data_df2.dtypes.values)\n",
    "# for s in data_df2.dtypes.index: print('* ', s)\n",
    "# print('** ** ', 'State' in data_df2.dtypes.index)\n",
    "#\n",
    "# print('** ', data_df2.columns )\n",
    "# print('** ', len(data_df2.columns))\n",
    "# #\n",
    "# for s in data_df2.columns:\n",
    "#     print('** ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing data in PANDAS DF, as opposed to sequentially in a list:\n",
    "#\n",
    "# TODO: can we make this work? it will be faster, but 1) we need to reformat some... most? of the data,\n",
    "#. and 2) we might need to throw out some extra header rows, depending on how we collect our data.\n",
    "#\n",
    "# NOTE: One would think this is the much faster way to do this, as opposed to spinning through a list, but \n",
    "#. it turns out it really is not... or at least not obviously. This is probably because PANDAS, despite what\n",
    "#. you might read on the DS blogs, is not inherently fast. In fact, standard indexing, search, etc. operations\n",
    "#. in PANDAS are MUCH slower than using a plain List(), let alone numpy arrays. Also, it makes more complex\n",
    "#. parsing functions much more difficult.\n",
    "#\n",
    "if False:\n",
    "    print('load data into dataframe...')\n",
    "    data_df2 = pandas.read_csv(data_file_name, sep='|')\n",
    "    #\n",
    "    print('Data loaded to df2. do types, casting, and conversions...')\n",
    "    for cl,f in types_dict.items():\n",
    "        print('column: ', cl)\n",
    "        if not cl in data_df2.dtypes.index:\n",
    "            print('skipping...')\n",
    "            continue\n",
    "        #\n",
    "        data_df2[cl] = f(data_df2[cl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC Usage data\n",
    "- Presently focused on Mazama\n",
    "- Usage data drawn via:\n",
    "```\n",
    "--format=User,JobID,Jobname,partition,state,time,start,end,elapsed,MaxRss,MaxVMSize,nnodes,ncpus\n",
    "```\n",
    "- Generally, type conversions will need to be performed manually\n",
    "- But since we might change the columns of interest, let's code up a dictionary of functions to do the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers:  ['User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', '']\n",
      "0: |||extern|47861384.extern|47861384.extern||COMPLETED||20|1|2019-08-10T00:57:02|2019-08-10T00:57:02|2019-08-10T00:57:02|2019-08-17T00:57:29|7-00:00:27|00:00.001|00:00:00|00:00.001|1|12096540|00:00:00\n",
      "1: ippoliti|vkhemani|357497|test|48057259_930|48215392|hns|COMPLETED|2-00:00:00|1|1|2019-08-12T15:03:44|2019-08-12T15:03:45|2019-08-14T02:04:17|2019-08-16T01:31:51|1-23:27:34|24:49.090|1-23:00:48|1-23:25:37||170854|00:00:00\n",
      "2: |||batch|48057259_930.batch|48215392.batch||COMPLETED||1|1|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-16T01:31:51|1-23:27:34|24:49.089|1-23:00:48|1-23:25:37|1|170854|00:00:00\n",
      "3: |||extern|48057259_930.extern|48215392.extern||COMPLETED||1|1|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-16T01:31:51|1-23:27:34|00:00.001|00:00:00|00:00.001|1|170854|00:00:00\n",
      "4: ippoliti|vkhemani|357497|test|48057259_931|48215393|hns|COMPLETED|2-00:00:00|1|1|2019-08-12T15:03:44|2019-08-12T15:03:45|2019-08-14T02:04:17|2019-08-16T01:32:09|1-23:27:52|24:13.588|1-23:01:25|1-23:25:39||170872|00:00:00\n",
      "5: |||batch|48057259_931.batch|48215393.batch||COMPLETED||1|1|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-16T01:32:09|1-23:27:52|24:13.586|1-23:01:25|1-23:25:39|1|170872|00:00:00\n",
      "6: |||extern|48057259_931.extern|48215393.extern||COMPLETED||1|1|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-16T01:32:10|1-23:27:53|00:00.001|00:00:00|00:00.001|1|170873|00:00:00\n",
      "7: ippoliti|vkhemani|357497|test|48057259_932|48215394|hns|COMPLETED|2-00:00:00|1|1|2019-08-12T15:03:44|2019-08-12T15:03:45|2019-08-14T02:04:17|2019-08-16T00:37:58|1-22:33:41|23:27.040|1-22:08:24|1-22:31:51||167621|00:00:00\n",
      "8: |||batch|48057259_932.batch|48215394.batch||COMPLETED||1|1|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-16T00:37:58|1-22:33:41|23:27.039|1-22:08:24|1-22:31:51|1|167621|00:00:00\n",
      "9: |||extern|48057259_932.extern|48215394.extern||COMPLETED||1|1|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-14T02:04:17|2019-08-16T00:38:03|1-22:33:46|00:00.001|00:00:00|00:00.001|1|167626|00:00:00\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "with open(data_file_name, 'r') as fin:\n",
    "    # NOTE: lines end with a delimiter, so dump the last value.\n",
    "    #\n",
    "    headers = fin.readline()[:-1].split(delim)\n",
    "    delim_lines=fin.readline()\n",
    "    #\n",
    "    #print('*** headers: ', headers)\n",
    "    #\n",
    "    teasers = []\n",
    "    for k in range(10):\n",
    "        #print('**')\n",
    "        #teasers += [fin.readline()[:-1].split()]\n",
    "        teasers += [fin.readline()[:-2]]\n",
    "#\n",
    "print('headers: ', headers)\n",
    "for k,rw in enumerate(teasers):\n",
    "    print('{}: {}'.format(k,rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read the whole data file:\n",
    "class SACCT_data_handler(object):\n",
    "    def __init__(self, data_file_name, delim='|', max_rows=None, types_dict=None):\n",
    "        #\n",
    "        if types_dict is None:\n",
    "            types_dict={'User':str, 'JobID':str, 'JobName':str, 'Partition':str, 'State':str,\n",
    "                'Timelimit':str,'Start':str2date, 'End':str2date, 'Submit':str2date, 'Elapsed':elapsed_time_2_sec, 'MaxRSS':str,\n",
    "                'MaxVMSize':str, 'NNodes':int, 'NCPUS':int, 'MinCPU':str, 'SystemCPU':str, 'UserCPU':str,\n",
    "                'TotalCPU':str}\n",
    "        #\n",
    "        self.__dict__.update({key:val for key,val in locals().items() if not key in ['self', '__class__']})\n",
    "        #\n",
    "        # especially for dev, allow to pass the DF object itself...\n",
    "        if isinstance(data_file_name, str):\n",
    "            self.load_sacct_data()\n",
    "        else:\n",
    "            self.data_df = data_file_name\n",
    "            self.data = self.data_df.values.to_list()\n",
    "        self.headers = self.data_df.columns\n",
    "        self.RH = {h:k for k,h in enumerate(self.headers)}\n",
    "        #\n",
    "        index_job_id = numpy.argsort(self.data_df['JobID'])\n",
    "        index_start  = numpy.argsort(self.data_df['Start'])\n",
    "        index_endf   = numpy.argsort(self.data_df['End'])\n",
    "        #\n",
    "        # group jobs; compute summary table:\n",
    "        ix_user_jobs = numpy.array([not ('.batch' in s or '.extern' in s) for s in data_df['JobID']])\n",
    "\n",
    "        #group_index = {s:numpy.array([]) for s in numpy.unique([s.split('.')[0] for s in data_df['JobID'][ix_user_jobs] ])}\n",
    "        job_ID_index = {s:[] for s in numpy.unique([s.split('.')[0] \n",
    "                                                    for s in data_df['JobID'][ix_user_jobs] ])}\n",
    "        #\n",
    "        for k,s in enumerate(data_df['JobID']):\n",
    "            job_ID_index[s.split('.')[0]] += [k]\n",
    "            #group_index[s.split('.')[0]] = numpy.append(group_index[s.split('.')[0]], [k])\n",
    "        for ky,vl in group_index.items():\n",
    "            group_index[ky] = numpy.array(vl)\n",
    "        #\n",
    "        # Compute a job summary table. What columns do we need to aggregate? NCPUS, all the times: Start,\n",
    "        #. End, Submit, Eligible. Note that the End time of the parent job often terminates before (only\n",
    "        #. by a few seconds????) the End time of the last step, so we should probably just compute the\n",
    "        #. Elapsed time. Start -> min(Start[]), End -> max(End[]), NCPU -> (NCPU of parent job or \n",
    "        #. max(NCPU)). For performance, we'll do well to not have to do logical, string-like operations --\n",
    "        #  aka, do algebraic type operations.\n",
    "        #\n",
    "        #jobs_summary = numpy.array()\n",
    "        #\n",
    "        #\n",
    "        self.__dict__.update({key:val for key,val in locals().items() if not key in ['self', '__class__']})\n",
    "    #\n",
    "    def load_sacct_data(self, data_file_name=None, delim=None, verbose=0, max_rows=None):\n",
    "        if data_file_name is None:\n",
    "            data_file_name = self.data_file_name\n",
    "        if delim is None:\n",
    "            delim = self.delim\n",
    "        max_rows = max_rows or self.max_rows\n",
    "        #\n",
    "        #\n",
    "        with open(data_file_name, 'r') as fin:\n",
    "            headers_rw = fin.readline()\n",
    "            if verbose:\n",
    "                print('*** headers_rw: ', headers_rw)\n",
    "            headers = headers_rw[:-1].split(delim)[:-1]\n",
    "            RH = {h:k for k,h in enumerate(headers)}\n",
    "            #\n",
    "            #self.RH=RH\n",
    "            #self.headers=headers\n",
    "            #\n",
    "            #delim_lines=fin.readline()\n",
    "            #\n",
    "            if verbose:\n",
    "                print('*** headers: ', headers)\n",
    "            active_headers=headers\n",
    "            # TODO: it would be a nice performance boost to only work through a subset of the headers...\n",
    "            #active_headers = [cl for cl in headers if cl in types_dict.keys()]\n",
    "            #\n",
    "            #\n",
    "            \n",
    "            n_rws = 0\n",
    "            # TODO: multithread this. A pool() class should do the trick.\n",
    "            # TODO: maybe assume we have enough memory and just do readlines()?\n",
    "            n_cpu = mpp.cpu_count()\n",
    "            # eventually, we might need to batch this.\n",
    "            P = mpp.Pool(n_cpu)\n",
    "            self.headers = headers\n",
    "            results = P.map_async(self.process_row, fin, chunksize=100)\n",
    "            P.close()\n",
    "            data = results.get()\n",
    "            #\n",
    "            del results, P\n",
    "            \n",
    "            #all_the_data = fin.readlines(max_rows)\n",
    "            #\n",
    "#             data = []\n",
    "#             #for j,rw in enumerate(fin):\n",
    "#             for j,rw in enumerate(all_the_data[0:(len(all_the_data) or max_rows)]):\n",
    "#                 #\n",
    "#                 if rw[0:10] == headers_rw[0:10]:\n",
    "#                     continue\n",
    "#                 #\n",
    "#                 #n_rws += 1\n",
    "#                 ##if not (max_rows is None or max_rows<0) and j>max_rows:\n",
    "#                 #if not (max_rows is None or max_rows<0) and n_rws>max_rows:\n",
    "#                 #    break\n",
    "\n",
    "#                 #print('*** DEBUG: ', rw)\n",
    "#                 data += [rw.split(delim)[:-1]]\n",
    "#                 #print('* * DEBUG: ', data[-1])\n",
    "#                 #\n",
    "#                 data[-1] = [None if vl=='' else types_dict.get(col,str)(vl)\n",
    "#                     for k,(col,vl) in enumerate(zip(active_headers, data[-1]))]\n",
    "#                 ##for k, (col,vl) in enumerate(zip(headers, data[-1])):\n",
    "#                 #for k, (col,vl) in enumerate(zip(active_headers, data[-1])):\n",
    "#                 #    #print('j,k [{}:{}]: {}, {}'.format(col,vl, j,k))\n",
    "#                 #    #data[-1][k]=types_dict[col](vl)\n",
    "#                 #    data[-1][k]=types_dict.get(col, str)(vl)\n",
    "#                 #\n",
    "                #\n",
    "            #\n",
    "#            del all_the_data, rw, k, cvol, vl\n",
    "            #\n",
    "            if verbose:\n",
    "                print('** len: ', len(data))\n",
    "            #\n",
    "            self.data=data\n",
    "            # TODO: asrecarray()?\n",
    "            self.data_df = pandas.DataFrame(data, columns=active_headers)\n",
    "        #\n",
    "    def process_row(self, rw):\n",
    "        # use this for MPP processing:\n",
    "        #rws = rw.split(delim)\n",
    "        return [None if vl=='' else self.types_dict.get(col,str)(vl)\n",
    "                    for k,(col,vl) in enumerate(zip(self.headers, rw.split(self.delim)[:-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacct_data = SACCT_data_handler(data_file_name=data_file_name, delim='|', max_rows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# unique completion states:\n",
    "data_df = sacct_data.data_df\n",
    "#\n",
    "#states = sorted(list(set([rw[RH['State']]  for rw in data])))\n",
    "states = sorted(list(set(data_df['State'])))\n",
    "for s in states:\n",
    "    print('** {}'.format(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_groups = data_df.groupby(by=numpy.unique([s.split('.')[0] for s in data_df['JobID']]) )\n",
    "#my_groups = data_df.groupby(by=[s.split('.')[0] for s in data_df['JobID']] )\n",
    "ix_user_jobs = numpy.array([not ('.batch' in s or '.extern' in s) for s in data_df['JobID']])\n",
    "\n",
    "#group_index = {s:numpy.array([]) for s in numpy.unique([s.split('.')[0] for s in data_df['JobID'][ix_user_jobs] ])}\n",
    "job_ID_index = {s:[] for s in numpy.unique([s.split('.')[0] \n",
    "                                            for s in data_df['JobID'][ix_user_jobs] ])}\n",
    "\n",
    "\n",
    "for k,s in enumerate(data_df['JobID']):\n",
    "    job_ID_index[s.split('.')[0]] += [k]\n",
    "    #group_index[s.split('.')[0]] = numpy.append(group_index[s.split('.')[0]], [k])\n",
    "for ky,vl in job_ID_index.items():\n",
    "    job_ID_index[ky] = numpy.array(sorted(vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Start':str2date, 'End':str2date, 'Submit':str2date, 'Elapsed'\n",
    "\n",
    "jobs_summary = numpy.recarray(shape=(len(job_ID_index), len(sacct_data.data_df.dtypes)), dtype=data_df.dtypes)\n",
    "for k, (j_id, ks) in enumerate(job_ID_index.items()):\n",
    "    jobs_summary[k]=data_df[numpy.min(ks)]\n",
    "    jobs_summary[k]['End'] = numpy.max(data_df['End'][ks])\n",
    "    jobs_summary[k]['Start'] = numpy.max(data_df['Start'][ks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(my_groups))\n",
    "#print('** ', my_groups.count)\n",
    "#\n",
    "print('** ', len(group_index))\n",
    "print('** ', len(data_df['JobID']))\n",
    "#\n",
    "print('** ** ')\n",
    "\n",
    "for k, (ky,vl) in enumerate(group_index.items()):\n",
    "    print('* * ', ky, vl)\n",
    "    if k>10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('** {}\\n'.format(headers))\n",
    "# for rw in data[0:100]:\n",
    "#     if rw[RH['JobID']].split('.')[0]=='48057259_930':\n",
    "#         print('** {}\\n'.format(rw['Job']))\n",
    "#\n",
    "# this_jid = '48057259_930'\n",
    "this_jid = '48693050'\n",
    "this_jid = '48538697'\n",
    "for rw in data:\n",
    "    #print('** ', rw)\n",
    "    #print('* * ', rw[RH['JobID']])\n",
    "    #print('** ', rw[RH['JobID']].split('.'))\n",
    "    #if rw[RH['JobID']].split('.')[0]==this_jid:\n",
    "    if rw[RH['JobID']].startswith(this_jid) or rw[RH['JobID']] == this_jid:\n",
    "        #print('** {}'.format(rw[RH['JobID']]))\n",
    "        jid = rw[RH['JobID']]\n",
    "        #jid_0, jid_1 = rw[RH['JobID']].split('_')\n",
    "        # 'batch', 'extern', \n",
    "        if len(jid.split('.'))>=2 and jid.split('.')[1] in ('batch', 'extern', 'bogus_place_holder'):\n",
    "            continue\n",
    "        print('** {}\\n'.format([rw[RH[s]] for s in ('User', 'Group', 'JobID', 'Submit', 'Timelimit',\n",
    "                                  'Eligible', 'Start', 'End', 'Elapsed', 'NCPUS')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active CPUs:\n",
    "- TODO: this is overcounting cpus, because there is a row for each step in the job, but each step shows the full start/stop time.\n",
    "- We need to consolidate unique jobIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "### how 'bout a time series of sum(ncpu) RUNNING?\n",
    "#\n",
    "# TODO: consider states COMPLETED, FAILED, RUNNING, TIMEOUT, PENDING. These all represent workload; only exclude\n",
    "#. canceled jobs.\n",
    "#ix = numpy.array([rw[RH['State']] in ['COMPLETED', 'RUNNING'] for rw in data])\n",
    "#ix = numpy.logical_or( data_df['State']=='COMPLETED', data_df['State']=='RUNNING')\n",
    "#ix = numpy.ones(len(data_df)).astype(bool)\n",
    "ix = numpy.array(['.batch' not in s and '.extern' not in s for s in data_df['JobID']])\n",
    "ix = numpy.array(['.' not in s for s in data_df['JobID']])\n",
    "ix_run = numpy.array(data_df['State']!='PENDING')\n",
    "ix = numpy.logical_and(ix, ix_run)\n",
    "#\n",
    "#\n",
    "t_now = mpd.date2num(dtm.datetime.now())\n",
    "t_start = mpd.date2num(data_df['Start'][ix])\n",
    "#\n",
    "t_end = data_df['End'][ix].to_numpy()\n",
    "t_end[t_end is None]=t_now\n",
    "t_end=mpd.date2num(t_end)\n",
    "#\n",
    "n_cpu = data_df['NCPUS'][ix].to_numpy()\n",
    "\n",
    "start_end = numpy.array([t_start, t_end]).T\n",
    "#\n",
    "# sort on t_start:\n",
    "start_end = start_end[numpy.argsort(start_end[:,0])]\n",
    "\n",
    "t_min = numpy.min(numpy.append(t_start, [x for x in t_end if not x is None] ))\n",
    "t_max = numpy.max(numpy.append(t_start, [x for x in t_end if not x is None] ))\n",
    "#\n",
    "# can also create X sequence like, for minute resolution\n",
    "# X = numpy.arange(t_min, t_max, 1./(24*60))\n",
    "X = numpy.linspace(t_min, t_max, 10000)\n",
    "#\n",
    "Ns = numpy.zeros(len(X))\n",
    "#\n",
    "Ns_cpu = numpy.zeros(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is many 100's of times faster than its cousin below:\n",
    "#Ns2 = numpy.zeros(len(X))\n",
    "#\n",
    "for j, (t_1, t_2, n) in enumerate(zip(t_start, t_end, n_cpu)):\n",
    "    #if j%25000==0:\n",
    "    #    print('j:: {}'.format(j))\n",
    "    #\n",
    "    # TODO: get more of the data set; we appear to be counting more cpus than exist?\n",
    "    # 1) check partition info, 2) maybe we're creating some sort of sampling error?\n",
    "    #.\n",
    "    ix = numpy.logical_and( X>=t_1, X<t_2)\n",
    "    Ns[ix] += 1\n",
    "    Ns_cpu[ix] += n\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = numpy.random.random(10)\n",
    "x2 = numpy.random.random(10)\n",
    "#\n",
    "print('x1: ', x1)\n",
    "print('x2: ', x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try another, should be more compute efficint way of computing running jobs/cpus:\n",
    "#. compute a sequence of started_jobs and a sequence of ended_jobs, then subtract.\n",
    "# to do the subtraction, interpolate onto a common time-axis.\n",
    "#\n",
    "# TODO: this general strategy is probably smarter for creating these working arrays, but for now just\n",
    "#. use th eexisting working arrays.\n",
    "# ix_active = numpy.logical_or(data_df['State']=='COMPLETED', data_df['State']=='RUNNING')\n",
    "# #\n",
    "# starts = numpy.array([(mpd.date2num(data_df[ix_active])['Start']), (data_df[ix_active])['NCPUS']])\n",
    "# #starts.sor\n",
    "# stops  = numpy.array([(mpd.date2num(data_df[ix_active])['End']), (data_df[ix_active])['NCPUS']])\n",
    "#\n",
    "import scipy.interpolate\n",
    "#\n",
    "ix_start = numpy.argsort(t_start)\n",
    "ix_end   = numpy.argsort(t_end)\n",
    "#\n",
    "ns_start = numpy.array([t_start[ix_start], numpy.cumsum(n_cpu[ix_start])])\n",
    "ns_end   = numpy.array([t_end[ix_end], numpy.cumsum(n_cpu[ix_end])])\n",
    "#\n",
    "f_start = scipy.interpolate.interp1d(ns_start[0], ns_start[1], fill_value=\"extrapolate\", bounds_error=False)\n",
    "f_end   = scipy.interpolate.interp1d(ns_end[0], ns_end[1], fill_value=\"extrapolate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(12, 10))\n",
    "ax1 = plt.subplot('211')\n",
    "ax1.grid()\n",
    "#\n",
    "ax2 = plt.subplot('223')\n",
    "ax2.grid()\n",
    "ax2.set_title('cpu starts')\n",
    "#\n",
    "ax3 = plt.subplot('224')\n",
    "ax3.grid()\n",
    "ax3.set_title('cpu completes')\n",
    "#\n",
    "#\n",
    "ax2.plot(*ns_start, ls='-', marker='')\n",
    "ax3.plot(*ns_end, ls='-', marker='')\n",
    "#\n",
    "X = numpy.linspace(numpy.min(numpy.array([t_start[ix_start], t_end[ix_end]])),\n",
    "                       numpy.max(numpy.array([t_start[ix_start], t_end[ix_end]])), 10000)\n",
    "ax2.plot(X, f_start(X), ls='--')\n",
    "ax3.plot(X, f_end(X), ls='--')\n",
    "#\n",
    "\n",
    "ax1.plot(X[:-1000], (f_start(X)-f_end(X))[:-1000], ls='-')\n",
    "ax1.set_title('Active CPUs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(12,10))\n",
    "ax1 = plt.subplot('311')\n",
    "ax2 = plt.subplot('312', sharex=ax1)\n",
    "ax3 = plt.subplot('313', sharex=ax1)\n",
    "n_ave = 100\n",
    "#\n",
    "ln, = ax1.plot(X,Ns, ls='-', lw=1.5, marker='', alpha=.3)\n",
    "clr = ln.get_color()\n",
    "#\n",
    "Ns_sm = (numpy.cumsum(Ns)[n_ave:]-numpy.cumsum(Ns)[:-n_ave])/float(n_ave)\n",
    "ax1.plot(X[n_ave:], Ns_sm, color=clr,\n",
    "        ls='--', lw=2.)\n",
    "\n",
    "ln, = ax2.plot(X, Ns_cpu, ls='-', lw=3., marker='', alpha=.3)\n",
    "clr = ln.get_color()\n",
    "Ns_cpu_sm = (numpy.cumsum(Ns_cpu)[n_ave:]-numpy.cumsum(Ns_cpu)[:-n_ave])/float(n_ave)\n",
    "ax2.plot(X[n_ave:], Ns_cpu_sm, color=clr,\n",
    "        ls='--', lw=2.)\n",
    "\n",
    "ax3.plot(X[n_ave:], Ns_cpu_sm/Ns_sm, color=clr,\n",
    "        ls='--', lw=2.)\n",
    "\n",
    "#\n",
    "ax1.set_title('jobs running')\n",
    "ax2.set_title('cpu running')\n",
    "ax3.set_title('cpu/job')\n",
    "#\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "ax3.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "86*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# delta_ts_all = numpy.array([elapsed_time_2_sec(rw[RH['Elapsed']]) for rw in data])\n",
    "# delta_ts_completed = numpy.array([elapsed_time_2_sec(rw[RH['Elapsed']]) \n",
    "#                                   for rw in data if rw[RH['State']]=='COMPLETED'])\n",
    "#\n",
    "delta_ts_all = numpy.array([rw[RH['Elapsed']] for rw in data])\n",
    "delta_ts_completed = numpy.array([rw[RH['Elapsed']] \n",
    "                                  for rw in data if rw[RH['State']]=='COMPLETED'])\n",
    "#\n",
    "print('*** lens: {}, {}'.format(len(delta_ts_all), len(delta_ts_completed)))\n",
    "#\n",
    "print(delta_ts_all[0:10])\n",
    "#\n",
    "fg = plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot('111')\n",
    "#\n",
    "do_normed = True\n",
    "do_log    = False\n",
    "do_cum    = True\n",
    "h_type  = 'step'\n",
    "time_factor=1.0/(24.*3600)\n",
    "#\n",
    "hh_all  = ax.hist(delta_ts_all*time_factor, bins=50, log=do_log, cumulative=do_cum, histtype=h_type, normed=do_normed,\n",
    "                  label='all jobs')\n",
    "hh_comp = ax.hist(delta_ts_completed*time_factor, bins=50, log=do_log, cumulative=do_cum, histtype=h_type, normed=do_normed, \n",
    "                  label='completed')\n",
    "#\n",
    "ax.legend(loc=0)\n",
    "#\n",
    "ax.set_ylim(.99,1.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time delayed plots. aka, probability (or counts) of job times given jobs > t_min\n",
    "#\n",
    "fg = plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot('111')\n",
    "#\n",
    "do_normed = False\n",
    "do_log    = False\n",
    "do_cum    = True\n",
    "h_type  = 'step'\n",
    "time_factor=1.0/(24.*3600)\n",
    "#\n",
    "delta_ts_completed = numpy.array([rw[RH['Elapsed']]\n",
    "                                      for rw in data if rw[RH['State']]=='COMPLETED'])\n",
    "for k,t_min in enumerate([0., 3600., 36000., 24.*3600, 5.*24.*3600, 7.*24.*3600]):\n",
    "    if k<2: continue\n",
    "\n",
    "    #delta_ts_all = numpy.array([elapsed_time_2_sec(rw[RH['Elapsed']]) for rw in data])\n",
    "    \n",
    "    #\n",
    "    #print('*** lens: {}, {}'.format(len(delta_ts_all), len(delta_ts_completed)))\n",
    "    #\n",
    "    #print(delta_ts[0:10])\n",
    "    #\n",
    "    #\n",
    "    #hh_all  = ax.hist(delta_ts_all*time_factor, bins=50, log=do_log, cumulative=do_cum, histtype=h_type, normed=do_normed,\n",
    "    #                  label='all jobs')\n",
    "    Y,X, patches = ax.hist(delta_ts_completed[delta_ts_completed>= t_min]*time_factor, bins=50,\n",
    "                           log=do_log, cumulative=do_cum, histtype=h_type, normed=do_normed, \n",
    "                      label='$t_{{min}}={}'.format(t_min*time_factor,))\n",
    "    #\n",
    "    # fill bewteen (approximate) dt=7 intersection and y.\n",
    "    k0 = numpy.argmin( (X-7.)**2. )\n",
    "    y0 = Y[k0]\n",
    "    clr = patches[0].get_edgecolor()\n",
    "    ix = X[:-1]>7.\n",
    "    #\n",
    "    X_prime = [X[k0]]\n",
    "    Y_prime = [Y[k0]]\n",
    "    for j, (x,y1,y2) in enumerate(zip(X[k0+1:], Y[k0:-1], Y[k0+1:])):\n",
    "        X_prime += [x, x]\n",
    "        Y_prime += [y1, y2]\n",
    "    #\n",
    "    #ax.fill_between(numpy.array(X[:-1])[ix], y0, Y[ix], color=clr, alpha=.1)\n",
    "    ax.fill_between(X_prime, y0, Y_prime, color=clr, alpha=.1)\n",
    "    #print('** ** ', len(ax.lines))\n",
    "    #\n",
    "ax.legend(loc=0)\n",
    "ax.grid()\n",
    "ax.plot([7., 7.], ax.get_ylim(), marker='.', ls='--', lw=2., color='m')\n",
    "ax.plot(ax.get_xlim(), [.9, .9], marker='', ls='--', lw=2., color='m')\n",
    "#\n",
    "ax.set_title('Cumulative Distribution of Job Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time delayed plots. aka, probability (or counts) of job times given jobs > t_min\n",
    "#\n",
    "fg = plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot('111')\n",
    "#\n",
    "do_normed = False\n",
    "do_log    = False\n",
    "do_cum    = True\n",
    "h_type  = 'step'\n",
    "time_factor=1.0/(24.*3600)\n",
    "#\n",
    "# usage index: dt*ncpus\n",
    "usage_index_completed = numpy.array([rw[RH['Elapsed']]*float(rw[RH['NCPUS']])\n",
    "                                      for rw in data if rw[RH['State']]=='COMPLETED'])\n",
    "for k,t_min in enumerate([0., 3600., 36000., 24.*3600, 5.*24.*3600, 7.*24.*3600]):\n",
    "    if k<1: continue\n",
    "\n",
    "    #delta_ts_all = numpy.array([elapsed_time_2_sec(rw[RH['Elapsed']]) for rw in data])\n",
    "    \n",
    "    #\n",
    "    #print('*** lens: {}, {}'.format(len(delta_ts_all), len(delta_ts_completed)))\n",
    "    #\n",
    "    #print(delta_ts[0:10])\n",
    "    #\n",
    "    #\n",
    "    #hh_all  = ax.hist(delta_ts_all*time_factor, bins=50, log=do_log, cumulative=do_cum, histtype=h_type, normed=do_normed,\n",
    "    #                  label='all jobs')\n",
    "    Y,X, patches = ax.hist(usage_index_completed[delta_ts_completed>= t_min]*time_factor, bins=50,\n",
    "                           log=do_log, cumulative=do_cum, histtype=h_type, normed=do_normed, \n",
    "                      label='$t_{{min}}={}$'.format(t_min*time_factor,))\n",
    "    #\n",
    "    # fill bewteen (approximate) dt=7 intersection and y.\n",
    "    k0 = numpy.argmin( (X-7.)**2. )\n",
    "    y0 = Y[k0]\n",
    "    clr = patches[0].get_edgecolor()\n",
    "    ix = X[:-1]>7.\n",
    "    #\n",
    "    X_prime = [X[k0]]\n",
    "    Y_prime = [Y[k0]]\n",
    "    for j, (x,y1,y2) in enumerate(zip(X[k0+1:], Y[k0:-1], Y[k0+1:])):\n",
    "        X_prime += [x, x]\n",
    "        Y_prime += [y1, y2]\n",
    "    #\n",
    "    #ax.fill_between(numpy.array(X[:-1])[ix], y0, Y[ix], color=clr, alpha=.1)\n",
    "    ax.fill_between(X_prime, y0, Y_prime, color=clr, alpha=.1)\n",
    "    #print('** ** ', len(ax.lines))\n",
    "    #\n",
    "ax.legend(loc=0)\n",
    "ax.grid()\n",
    "ax.plot([7., 7.], ax.get_ylim(), marker='.', ls='--', lw=2., color='m')\n",
    "ax.plot(ax.get_xlim(), [.9, .9], marker='', ls='--', lw=2., color='m')\n",
    "#\n",
    "ax.set_title('Cumulative Distribution of Usage-index ($\\Delta t \\cdot N_{cpu}$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How 'bout some data on long jobs...\n",
    "#\n",
    "long_jobs = [rw for rw in data if rw[RH['Elapsed']]>7*24.*3600]\n",
    "#\n",
    "long_jobs_df = data_df[data_df['Elapsed']>7*24.*3600]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitions using long jobs:\n",
    "print(\"Long Job Partitons: \")\n",
    "print(\"len: \", len(long_jobs))\n",
    "print('partitions: ')\n",
    "#for rw in list(set([s[RH['Partition']] for s in long_jobs])):\n",
    "for rw in list(set(long_jobs_df['Partition'])):\n",
    "    print('** ', rw)\n",
    "    #\n",
    "#\n",
    "# partitions using long jobs:\n",
    "print(\"Long Job Users: \")\n",
    "print(\"len: \", len(long_jobs))\n",
    "print('Users: ')\n",
    "for rw in list(set(long_jobs_df['User'])):\n",
    "    print('** ', rw)\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compute_vol = numpy.sum(data_df['NCPUS']*data_df['Elapsed'])\n",
    "ix = data_df['Elapsed']>(7.*24.*3600.)\n",
    "long_compute_vol = numpy.sum(data_df['NCPUS'][ix]*data_df['Elapsed'][ix])\n",
    "#\n",
    "print('** jobs: ', len(data), len(data_df[ix]), sum(ix), len(data_df[ix])/len(data))\n",
    "print('** compute tinme: ', numpy.sum(data_df['Elapsed']), numpy.sum(data_df['Elapsed'][ix]),\n",
    "      numpy.sum(data_df['Elapsed'][ix])/numpy.sum(data_df['Elapsed']))\n",
    "print('** compute volumnes: ', total_compute_vol, long_compute_vol, long_compute_vol/total_compute_vol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ts = numpy.arange(1,40)\n",
    "lcvs = numpy.zeros(len(ts))\n",
    "mean_n_cpus = numpy.zeros(len(ts))\n",
    "stdev_n_cpus = numpy.zeros(len(ts))\n",
    "max_n_cpus = numpy.zeros(len(ts))\n",
    "median_n_cpus = numpy.zeros(len(ts))\n",
    "\n",
    "for k,t in enumerate(ts):\n",
    "    ix = data_df['Elapsed']>(t*24.*3600.)\n",
    "    lcv = numpy.sum(data_df['NCPUS'][ix]*data_df['Elapsed'][ix])\n",
    "    #\n",
    "    lcvs[k] = numpy.sum(numpy.sum(data_df['NCPUS'][ix]*data_df['Elapsed'][ix]))/total_compute_vol\n",
    "    #\n",
    "    mean_n_cpus[k]  = numpy.mean(data_df['NCPUS'][ix])\n",
    "    stdev_n_cpus[k] = numpy.std(data_df['NCPUS'][ix])\n",
    "    max_n_cpus[k] = numpy.max(data_df['NCPUS'][ix])\n",
    "    median_n_cpus[k] = numpy.median(data_df['NCPUS'][ix])\n",
    "    #\n",
    "#\n",
    "fg = plt.figure(figsize=(12,10))\n",
    "ax1 = plt.subplot('211')\n",
    "ax1.grid()\n",
    "ax2 = plt.subplot('212')\n",
    "ax2.grid()\n",
    "#\n",
    "ax1.plot(ts, lcvs, ls='-', marker='o', lw=3)\n",
    "ln, ax2.plot(ts, mean_n_cpus, ls='-', marker='o', lw=3)\n",
    "clr = ln.get_color()\n",
    "ax2.fill_between(ts, numpy.max([numpy.ones(len(ts)), mean_n_cpus-stdev_n_cpus], axis=0), mean_n_cpus+stdev_n_cpus, alpha=.2, color=clr)\n",
    "ax2.plot(ts, max_n_cpus, ls='--', marker='.', lw=2., alpha=.5)\n",
    "ax2.plot(ts, median_n_cpus, ls='--', marker='.', lw=2., alpha=.7, color=clr)\n",
    "#\n",
    "#\n",
    "n = 7\n",
    "ax1.plot([0., n], [lcvs[n-1], lcvs[n-1]], ls='--', lw=2., color='b')\n",
    "ax1.plot([n, n], [0., lcvs[n-1]], ls='--', lw=2., color='b')\n",
    "#\n",
    "n=14\n",
    "ax1.plot([0., n], [lcvs[n-1], lcvs[n-1]], ls='--', lw=2., color='c')\n",
    "ax1.plot([n, n], [0., lcvs[n-1]], ls='--', lw=2., color='c')\n",
    "#\n",
    "ax1.set_ylim(-.01, .3)\n",
    "#\n",
    "ax1.set_title('Fraction of jobs $\\Delta t > t$')\n",
    "ax1.set_ylabel('Fraction of long jobs')\n",
    "ax1.set_xlabel('Job length $t$ [days]')\n",
    "\n",
    "ax2.set_title('Number of CPU cores')\n",
    "ax2.set_ylabel('Number of CPUs $N_{cpu}$')\n",
    "ax2.set_xlabel('Job length $t$ [days]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job wait times\n",
    "- Compute stats (mean, median, stdev) for wait times, as a function of the number of cores, $n_{cpu}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait times as a funciton of CPU.\n",
    "#\n",
    "delta_ts = mpd.date2num(data_df['Start']) - mpd.date2num(data_df['Submit'])\n",
    "#\n",
    "# do some binning and stats:\n",
    "#Ns_cpus = numpy.unique(dat_df['NCPUS'])\n",
    "Ns_cpu = numpy.arange(max(data_df['NCPUS']))+1\n",
    "# so we could spin through this and group all the data into bins, like:\n",
    "# 1: (x10, x11, x12..), 2:(x20, x21, x22...)\n",
    "# ... or we could sort the data and use a find_in_sorted() type function, but for\n",
    "#. now, i think it will be sufficient to just spin through the list a few times and use an index:\n",
    "#\n",
    "wait_stats = numpy.core.records.fromarrays(numpy.zeros((len(Ns_cpu), 6)).T, dtype=[('n', '>f8'), ('mean', '>f8'), \n",
    "                                                                ('median', '>f8'),  ('stdev', '>f8'),\n",
    "                                                                ('min', '>f8'),  ('max', '>f8')])\n",
    "#\n",
    "for k in Ns_cpu:\n",
    "    #x_prime = delta_ts[data_df['NCPUS']==k]\n",
    "    x_prime = delta_ts[numpy.logical_and(data_df['NCPUS']==k, delta_ts>=0.)]\n",
    "    #wait_stats[k-1]=[[k, numpy.mean(x_prime), numpy.median(x_prime), numpy.std(x_prime), \n",
    "    #                 numpy.min(x_prime), numpy.max(x_prime)]]\n",
    "    wait_stats[k-1][0] = k\n",
    "    if len(x_prime)==0:\n",
    "        continue\n",
    "    \n",
    "    for j,f in zip(range(1, 6), [numpy.mean, numpy.median, numpy.std, numpy.min, numpy.max]):\n",
    "        #\n",
    "        wait_stats[k-1][j]=f(x_prime)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('** ', 1./(24*60) )\n",
    "print('** ', .0007*24.*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# make a step-function of median wait times:\n",
    "#step_meds = [[0,0]]\n",
    "step_meds=[]\n",
    "for x,y in zip(Ns_cpu, wait_stats['median']):\n",
    "    #x0 = step_meds[-1][0]\n",
    "    step_meds += [[x-1, 0], [x-1, y], [x,y], [x,0]]\n",
    "step_meds = numpy.array(step_meds)\n",
    "#\n",
    "fg = plt.figure(figsize=(12,8))\n",
    "ax1 = plt.subplot('211')\n",
    "#ax1 = plt.subplot('111')\n",
    "ax2 = plt.subplot('212', sharex=ax1)\n",
    "#\n",
    "#ln, = ax1.plot(Ns_cpu, wait_stats['mean'])\n",
    "clr = ln.get_color()\n",
    "#ax1.plot(Ns_cpu, wait_stats['median'], color=clr, ls='-')\n",
    "#ax1.plot(Ns_cpu, wait_stats['mean'], color=None, ls='--')\n",
    "#ax1.fill_between(Ns_cpu, wait_stats['median'], wait_stats['median']+wait_stats['stdev'], color=clr, alpha=.2)\n",
    "ax1.plot(*step_meds.T, ls='-', marker='')\n",
    "#\n",
    "ix = wait_stats['mean']>0.\n",
    "ln2, = ax2.plot(Ns_cpu[ix], 24.*wait_stats['median'][ix], marker='.', ls='-')\n",
    "clr2=ln2.get_color()\n",
    "#\n",
    "ix = wait_stats['mean']>0.\n",
    "ax2.errorbar((Ns_cpu)[ix], 24.*(wait_stats['median'])[ix], 24.*(wait_stats['stdev'])[ix],\n",
    "             color=clr2, ls='', alpha=.2)\n",
    "#\n",
    "#ax2.plot(data_df['NCPUS'], delta_ts, ls='', marker='.')\n",
    "#\n",
    "ax1.grid()\n",
    "ax1.set_xlabel('Number of Cores $N_{cpu}$')\n",
    "ax1.set_ylabel('Wait time to start (in days)')\n",
    "#ax1.set_ylim(0., .0005)\n",
    "ax1.set_yscale('log')\n",
    "#ax2.set_yscale('log')\n",
    "#\n",
    "ax2.set_ylim(-.2, 6.0)\n",
    "ax2.grid()\n",
    "ax2.set_ylabel('wait time, hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = pandas.DataFrame(sorted(sacct_data.data, key=lambda rw: rw[sacct_data.RH['JobID']]), columns=active_headers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = ['.' in s and not ( '.batch' in s or '.extern' in s) for s in data_sorted['JobID']]\n",
    "print('** :\\n', (data_sorted.iloc[ix, [RH[s] \n",
    "                                        for s in ['JobName', 'JobID', 'JobIDRaw',\n",
    "                                                  'Elapsed']]])[0:20] )\n",
    "# , 'CPUTimeRAW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ix = numpy.array([s.startswith('48057259') \n",
    "ix = numpy.array(['48270204' in s \n",
    "                                         and not ('.batch' in s \n",
    "                                         or '.extern' in s) for s in data_sorted['JobID']])\n",
    "\n",
    "print(data_sorted['JobID'][ix])\n",
    "#print(data_sorted['JobID'][ numpy.array(['48057259' in s for s in data_sorted['JobID']])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sorted.loc[ix, ['JobID','NCPUS', 'Start', 'End', 'Elapsed','TotalCPU', 'State'] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime as dtm\n",
    "import pytz\n",
    "import multiprocessing as mpp\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "#import lmod\n",
    "# lmod.load('system')\n",
    "# lmod.load('texlive')\n",
    "# lmod.\n",
    "#\n",
    "# TODO: phase out unreferenced hpc_lib calls...\n",
    "import hpc_lib\n",
    "import hpc_reports\n",
    "#\n",
    "def running_mean(X,n=10):\n",
    "    return (numpy.cumsum(numpy.insert(X,0,0))[n:] - numpy.cumsum(numpy.insert(X,0,0))[:-n])/n\n",
    "#\n",
    "#data_file_name = 'data/sacct_serc_20200622.out'\n",
    "#data_file_name = 'data/sacct_serc_20200724.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_pickle=True\n",
    "# pkl_name = \"{}.pkl\".format(os.path.splitext(data_file_name)[0])\n",
    "# #\n",
    "# if load_pickle:\n",
    "#     with open(pkl_name, 'rb') as fin:\n",
    "#         sacct_mazama=pickle.load(fin)\n",
    "#     #\n",
    "# else:\n",
    "#     sacct_mazama = hpc_lib.SACCT_data_handler(data_file_name=data_file_name)\n",
    "#     #\n",
    "    \n",
    "#     with open(pkl_name, 'wb') as fout:\n",
    "#             #out_pkl = pickle.dump(sacct_demo.jobs_summary, fout)\n",
    "#             out_pkl = pickle.dump(sacct_mazama, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** n_cpus:  1\n"
     ]
    }
   ],
   "source": [
    "# n_cpus:\n",
    "n_cpus = 1\n",
    "if 'SLURM_JOB_ID' in os.environ.keys():\n",
    "    # running in SLURM...\n",
    "    if 'SLURM_JOB_CPUS_PER_NODE' in os.environ.keys():\n",
    "        n_cpus = max( int(os.environ['SLURM_JOB_CPUS_PER_NODE']), 1)\n",
    "    else:\n",
    "        n_cpus = 1\n",
    "else:\n",
    "    n_cpus = min(8, mpp.cpu_count())\n",
    "#\n",
    "# or specify here:\n",
    "#n_cpus = 4\n",
    "#\n",
    "print('** n_cpus: ', n_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** dates: 2021-09-01 - 2022-02-28\n",
      "*** Creating SACCT object from HDF5 sacct_data/serc_sacct_20220228.h5\n",
      "**  ('index', 'User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'ReqTRES', 'AllocTRES', 'JobID_parent')\n"
     ]
    }
   ],
   "source": [
    "#end_dtm = dtm.datetime.now()+dtm.timedelta(days=1)\n",
    "end_dtm = dtm.datetime(2022,2,28)\n",
    "sacct_h5_file = f'sacct_data/serc_sacct_{end_dtm.year:04}{end_dtm.month:02}{end_dtm.day:02}.h5'\n",
    "#start_date = '2021-04-15'\n",
    "#end_date   = '2021-05-15'\n",
    "end_date = end_dtm.date()\n",
    "start_date = end_date - dtm.timedelta(days=180)\n",
    "print('*** dates: {} - {}'.format(start_date, end_date))\n",
    "#\n",
    "# NOTE: additional options can be passed in the more_options=[] arrary, or just as sacct_{option-name}={val}\n",
    "#\n",
    "if os.path.isfile(sacct_h5_file):\n",
    "    print('*** Creating SACCT object from HDF5 {}'.format(sacct_h5_file))\n",
    "    SACCT_obj = hpc_lib.SACCT_data_from_h5(sacct_h5_file, keep_raw_data=False, n_cpu=n_cpus)\n",
    "    #\n",
    "    \n",
    "else:\n",
    "    print('*** Fetching SACCT data directly')\n",
    "    SACCT_obj = hpc_lib.SACCT_data_direct(group=None, partition='serc', start_date=str(start_date),\n",
    "                                          n_cpu=n_cpus,\n",
    "                                          end_date=str(end_date), keep_raw_data=False)\n",
    "    print(f'** writing HDF5: {sacct_h5_file}')\n",
    "    SACCT_obj.write_hdf5(sacct_h5_file)\n",
    "    #\n",
    "#\n",
    "print('** ', SACCT_obj.jobs_summary.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** max_submit: 2022-02-28 16:07:17+00:00, max_start: 2022-02-27 23:37:16+00:00\n",
      "** cols:  ('index', 'User', 'Group', 'GID', 'JobName', 'JobID', 'JobIDRaw', 'Partition', 'State', 'Timelimit', 'NCPUS', 'NNodes', 'Submit', 'Eligible', 'Start', 'End', 'Elapsed', 'SystemCPU', 'UserCPU', 'TotalCPU', 'NTasks', 'CPUTimeRAW', 'Suspended', 'ReqTRES', 'AllocTRES', 'JobID_parent')\n"
     ]
    }
   ],
   "source": [
    "max_submit, max_start = [mpd.num2date(numpy.nanmax(SACCT_obj.jobs_summary[cl])) for cl in ['Start', 'Submit'] ]\n",
    "print('*** max_submit: {}, max_start: {}'.format(*[mpd.num2date(numpy.nanmax(SACCT_obj.jobs_summary[cl]))\n",
    "                                                   for cl in ['Start', 'Submit'] ]))\n",
    "print('** cols: ', SACCT_obj.jobs_summary.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: add this to the __init__. would be nice to refactor the code a bit to better handle inheritance.\n",
    "if not 'chunk_size' in SACCT_obj.__dict__.keys():\n",
    "    print('*** assigning chunk_size: ', chunk_size)\n",
    "    SACCT_obj.chunk_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** DEBUG: 5000, None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "cpu_usage = SACCT_obj.active_jobs_cpu()\n",
    "#\n",
    "bin_size=7\n",
    "cpu_weekly = SACCT_obj.active_jobs_cpu(bin_size=bin_size, t_min=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', len(cpu_usage))\n",
    "print('** \\n', cpu_usage[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(10,10))\n",
    "ax1 = fg.add_subplot('211')\n",
    "ax2 = fg.add_subplot('212', sharex=ax1)\n",
    "for ax in (ax1, ax2):\n",
    "    ax.grid()\n",
    "#\n",
    "ax1.plot(cpu_usage['time'], cpu_usage['N_jobs'], ls='-', lw=2., marker='')\n",
    "ax1.plot(cpu_weekly['time'], cpu_weekly['N_jobs'], ls='-', lw=2., marker='.')\n",
    "#\n",
    "ax2.plot(cpu_usage['time'], cpu_usage['N_cpu'], ls='-', lw=2., marker='')\n",
    "ax2.plot(cpu_weekly['time'], cpu_weekly['N_cpu'], ls='-', lw=2., marker='.')\n",
    "#\n",
    "\n",
    "ax1.set_title('Jobs', size=16)\n",
    "ax1.set_ylabel('$N_{jobs}$', size=16)\n",
    "#\n",
    "ax2.set_title('CPUs', size=16)\n",
    "ax2.set_ylabel('$N_{CPU}$', size=16)\n",
    "\n",
    "fg.canvas.draw()\n",
    "#\n",
    "# set ax3 labels to dates:\n",
    "# now format the datestrings...\n",
    "# fix_to_ascii(s)\n",
    "lbls = [hpc_lib.simple_date_string(mpd.num2date(float(hpc_lib.fix_to_ascii(s.get_text()))) ) for s in ax1.get_xticklabels()]\n",
    "#print('*** ', lbls)\n",
    "#\n",
    "ax2.set_xticklabels(lbls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def time_bin_aggregates(XY, bin_mod=24, qs=[.25, .5, .74]):\n",
    "#     XY=numpy.array(XY)\n",
    "#     if XY.shape[0]==2:\n",
    "#         X = XY[0,:]\n",
    "#         Y = XY[1:]\n",
    "#     else:\n",
    "#         X = XY[:,0]\n",
    "#         Y = XY[:,1]\n",
    "#     #\n",
    "#     #X_mod = ((X*bin_mod)%bin_mod).astype(int)\n",
    "#     X_mod = ((X%1.)*bin_mod).astype(int)\n",
    "#     #\n",
    "#     stats_output=[]\n",
    "#     for x in numpy.unique(X_mod):\n",
    "#         ix = X_mod==x\n",
    "#         this_Y = Y[ix]\n",
    "#         stats_output += [numpy.append([x, numpy.mean(this_Y), numpy.std(this_Y)],\n",
    "#                                       numpy.quantile(this_Y, qs))]\n",
    "#     #\n",
    "#     return numpy.core.records.fromarrays(numpy.array(stats_output).T, dtype=[('x', '>f8'), ('mean', '>f8'),\n",
    "#                                                         ('stdev', '>f8')] + \n",
    "#                                          [('q_{}'.format(q), '>f8') for q in qs])\n",
    "# #\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [.5, .75, .95]\n",
    "#\n",
    "comp_vol_submit = SACCT_obj.get_submit_compute_vol_timeofday(qs=qs)\n",
    "comp_vol_start = SACCT_obj.get_submit_compute_vol_timeofday(time_col='Start', qs=qs)\n",
    "#\n",
    "fg = plt.figure(figsize=(12,6))\n",
    "ax1 = fg.add_subplot('121')\n",
    "ax2 = fg.add_subplot('122')\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "#\n",
    "\n",
    "# N = numpy.sum(comp_vol_submit['cpu-time'])\n",
    "N = 1.\n",
    "ax1.plot(comp_vol_submit['time'], comp_vol_submit['cpu-time']/N,\n",
    "         ls='-', marker='o', lw=2., label='submit')\n",
    "#print('*** ', numpy.sum(comp_vol_submit['cpu-time']/numpy.sum(comp_vol_submit['cpu-time'])))\n",
    "#\n",
    "# N = numpy.sum(comp_vol_start['cpu-time'])\n",
    "N = 1 \n",
    "ax1.plot(comp_vol_start['time'], comp_vol_start['cpu-time']/N,\n",
    "         ls='-', marker='o', lw=2., label='start')\n",
    "#print('*** ', numpy.sum(comp_vol_start['cpu-time']/N))\n",
    "#\n",
    "#N=numpy.sum(comp_vol_submit['cpus'])\n",
    "N=1.\n",
    "ax2.plot(comp_vol_submit['time'], comp_vol_submit['cpus']/N, ls='-', marker='o', lw=2., label='submit')\n",
    "\n",
    "#N=numpy.sum(comp_vol_start['cpus'])\n",
    "N=1\n",
    "ax2.plot(comp_vol_start['time'], comp_vol_start['cpus']/N, ls='-', marker='o', lw=2., label='start')\n",
    "#\n",
    "#for k,cl in enumerate(comp_vol_tod.dtype.names[2:]):\n",
    "#    ax1.plot(comp_vol_tod['time'], comp_vol_tod[cl], ls='-', marker='o', lw=2., label='$q={}$'.format(qs[k]))\n",
    "#    break\n",
    "#\n",
    "ax1.legend(loc=0, numpoints=1)\n",
    "ax1.set_title('Compute Volume Requested, \\n$N_{CPU} \\cdot \\Delta t_{limit}$')\n",
    "ax1.set_xlabel('Hour of day')\n",
    "ax2.set_ylabel('Compute Vol.')\n",
    "\n",
    "ax2.set_title('CPUs Requested')\n",
    "ax2.set_xlabel('Hour of day')\n",
    "ax2.legend(loc=1, numpoints=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(10,10))\n",
    "ax1 = fg.add_subplot(2,2,1)\n",
    "ax2 = fg.add_subplot(2,2,2)\n",
    "ax3 = fg.add_subplot(2,2,3)\n",
    "ax4 = fg.add_subplot(2,2,4)\n",
    "axs = [ax1, ax2, ax3, ax4]\n",
    "[ax.grid() for ax in axs]\n",
    "#\n",
    "cpu_hourly = hpc_lib.time_bin_aggregates(XY=numpy.array([cpu_usage['time'], cpu_usage['N_cpu']]).T)\n",
    "jobs_hourly = hpc_lib.time_bin_aggregates(XY=numpy.array([cpu_usage['time'], cpu_usage['N_jobs']]).T)\n",
    "\n",
    "hh1 = ax1.hist(sorted(cpu_usage['N_jobs'])[0:int(1.0*len(cpu_usage))], bins=25, cumulative=False)\n",
    "ax2.plot(jobs_hourly['x'], jobs_hourly['mean'], ls='-', marker='o')\n",
    "\n",
    "hh3 = ax3.hist(cpu_usage['N_cpu'], bins=25)\n",
    "ax4.plot(cpu_hourly['x'], cpu_hourly['mean'], ls='-', marker='o')\n",
    "\n",
    "#ax1.set_ylim(-5., 200)\n",
    "ax1.set_title('$N_{jobs}$ Histogrm', size=16)\n",
    "ax2.set_title('Hour-of-day job counts', size=16)\n",
    "ax3.set_title('$N_{cpu}$ Histogram', size=16)\n",
    "ax4.set_title('Hour-of-day CPU counts', size=16)\n",
    "#\n",
    "plt.suptitle('Instantaneous Usage', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hours = SACCT_obj.get_cpu_hours(bin_size=7, n_points=5000)\n",
    "\n",
    "daily_hours = SACCT_obj.get_cpu_hours(bin_size=1, n_points=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.gca()\n",
    "ax1.grid()\n",
    "#\n",
    "ax1.plot(weekly_hours['time'], weekly_hours['cpu_hours']/7., ls='-', marker='.', label='bins=7 day', zorder=11)\n",
    "ax1.plot(daily_hours['time'], daily_hours['cpu_hours'], ls='-', marker='.', label='bins=1 day', zorder=5)\n",
    "#\n",
    "#ax1.plot( daily_hours['time'][0::(len(daily_hours['time'])-1)], numpy.ones(2)*14*24*24, ls='--', lw=3.)\n",
    "#ax1.plot( daily_hours['time'][0::(len(daily_hours['time'])-1)], numpy.ones(2)*12*24*24, ls='--', lw=3.)\n",
    "#\n",
    "n_cpus_serc = 32*104 + 128*8 + 24*12\n",
    "n_cpus_serc_gpus = n_cpus_serc + 128*6 + 24*2\n",
    "ax1.plot( daily_hours['time'][0::(len(daily_hours['time'])-1)], numpy.ones(2)*24*n_cpus_serc, ls='--', lw=3.,\n",
    "         label='CPU capacity')\n",
    "#ax1.plot( daily_hours['time'][0::(len(daily_hours['time'])-1)], numpy.ones(2)*24*n_cpus_serc_gpus, ls='--', lw=3.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "fg.canvas.draw()\n",
    "#\n",
    "# set ax3 labels to dates:\n",
    "# now format the datestrings...\n",
    "lbls = [hpc_lib.simple_date_string(mpd.num2date(float(hpc_lib.fix_to_ascii(s.get_text()))) ) for s in ax1.get_xticklabels()]\n",
    "#print('*** ', lbls)\n",
    "#\n",
    "ax1.set_xticklabels(lbls)\n",
    "ax1.set_xlabel('Time $t$')\n",
    "ax1.set_ylabel('Daily CPU hours')\n",
    "ax1.set_title('Daily CPU hours')\n",
    "#\n",
    "ax1.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** dtype:\\n', SACCT_obj.jobs_summary.dtype)\n",
    "#\n",
    "#user_cols  = ['Timelimit', 'NCPUS', 'NNodes', 'TotalCPU','NTasks']\n",
    "#user_types = ['>f8', '>i8', '>i8', '>f8', '>i8', '>f8']\n",
    "\n",
    "user_cols = ['NCPUS', 'NNodes', 'Elapsed', 'Comp_Vol']\n",
    "user_types = ['>i8', '>i8', '>f8', '>f8']\n",
    "#\n",
    "user_aggs = numpy.zeros( shape=[len(numpy.unique(SACCT_obj.jobs_summary['User'])),],\n",
    "                        dtype={'names':user_cols, 'formats':user_types })\n",
    "#\n",
    "ix = {usr:numpy.where(SACCT_obj.jobs_summary['User']==usr)[0] \n",
    "      for usr in numpy.unique(SACCT_obj.jobs_summary['User'])}\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** ', user_aggs.shape, user_aggs.dtype)\n",
    "print('*** ', user_cols)\n",
    "user_names = numpy.unique(SACCT_obj.jobs_summary['User'])\n",
    "for k,user in enumerate(user_names):\n",
    "    #print('** ', [numpy.sum(SACCT_obj.jobs_summary[cl][ix[user]]) for cl in user_cols])\n",
    "    #\n",
    "    vals = [numpy.sum( numpy.max([numpy.zeros(len(ix[user])), SACCT_obj.jobs_summary[cl][ix[user]]], axis=0) ) \n",
    "                      for cl in user_cols[:-1]]\n",
    "    vals += [numpy.sum(SACCT_obj.jobs_summary['Elapsed'][ix[user]]*24. * SACCT_obj.jobs_summary['NCPUS'][ix[user]])]\n",
    "    #print('** ix: ', ix[user])\n",
    "    #vals = [numpy.sum( (SACCT_obj.jobs_summary[cl][ numpy.array(ix[user])] ) )\n",
    "    #        for cl in user_cols]\n",
    "    #print('** vals: ', vals)\n",
    "    \n",
    "    #user_aggs[user_cols][k] = tuple([numpy.sum(SACCT_obj.jobs_summary[cl][ix[user]]) for cl in user_cols])\n",
    "    \n",
    "    user_aggs[user_cols][k] = tuple( vals )\n",
    "    #user_aggs[user_cols][k] = tuple(vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('** user, {}'.format(user_cols ) )\n",
    "# for nm,rw in zip(user_names, user_aggs):\n",
    "#     print('** {}: {}, '.format(nm,rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = plt.figure(figsize=(10,8))\n",
    "ax = plt.gca()\n",
    "ax.grid()\n",
    "for k,usr in enumerate(user_names[numpy.argsort(user_aggs['Comp_Vol'])[::-1]]):\n",
    "    cpuh = SACCT_obj.get_cpu_hours(bin_size=1.0, jobs_summary=SACCT_obj.jobs_summary[ix[usr]])\n",
    "    ax.plot(cpuh['time'], cpuh['cpu_hours'], label=('{}: {}'.format(usr, numpy.sum(cpuh['cpu_hours']) )\n",
    "                                                    if k<5 else None) )\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of day(-like) compute volume requests.\n",
    "\n",
    "qs = [.5, .75, .95]\n",
    "#\n",
    "comp_vol_submit = SACCT_obj.get_submit_compute_vol_timeofday(qs=qs)\n",
    "comp_vol_start = SACCT_obj.get_submit_compute_vol_timeofday(time_col='Start', qs=qs)\n",
    "#\n",
    "fg = plt.figure(figsize=(12,6))\n",
    "ax1 = fg.add_subplot('121')\n",
    "ax2 = fg.add_subplot('122')\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "#\n",
    "\n",
    "ax1.plot(comp_vol_submit['time'], comp_vol_submit['cpu-time']/numpy.sum(comp_vol_submit['cpu-time']),\n",
    "         ls='-', marker='o', lw=2., label='submit')\n",
    "#print('*** ', numpy.sum(comp_vol_submit['cpu-time']/numpy.sum(comp_vol_submit['cpu-time'])))\n",
    "\n",
    "ax1.plot(comp_vol_start['time'], comp_vol_start['cpu-time']/numpy.sum(comp_vol_start['cpu-time']),\n",
    "         ls='-', marker='o', lw=2., label='start')\n",
    "#print('*** ', numpy.sum(comp_vol_start['cpu-time']/numpy.sum(comp_vol_start['cpu-time'])))\n",
    "#\n",
    "ax2.plot(comp_vol_submit['time'], comp_vol_submit['cpus']/numpy.sum(comp_vol_submit['cpus']), ls='-', marker='o', lw=2., label='submit')\n",
    "ax2.plot(comp_vol_start['time'], comp_vol_start['cpus']/numpy.sum(comp_vol_start['cpus']), ls='-', marker='o', lw=2., label='start')\n",
    "#\n",
    "#for k,cl in enumerate(comp_vol_tod.dtype.names[2:]):\n",
    "#    ax1.plot(comp_vol_tod['time'], comp_vol_tod[cl], ls='-', marker='o', lw=2., label='$q={}$'.format(qs[k]))\n",
    "#    break\n",
    "#\n",
    "ax1.legend(loc=0, numpoints=1)\n",
    "ax1.set_title('Compute Volume Requested, \\n$N_{CPU} \\cdot \\Delta t_{limit}$')\n",
    "ax1.set_xlabel('Hour of day')\n",
    "ax2.set_ylabel('Percent')\n",
    "\n",
    "ax2.set_title('CPUs Requested')\n",
    "ax2.legend(loc=1, numpoints=1)\n",
    "\n",
    "#ax1.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute some distributions:\n",
    "print('*** ', SACCT_obj.jobs_summary.dtype.names)\n",
    "run_times = SACCT_obj.get_run_times()\n",
    "\n",
    "#\n",
    "fg = plt.figure(figsize=(12,8))\n",
    "ax1 = fg.add_subplot(2,2,1)\n",
    "ax2 = fg.add_subplot(2,2,2)\n",
    "#ax1a.set_yscale('log')\n",
    "ax3 = fg.add_subplot(2,2,3)\n",
    "ax4 = fg.add_subplot(2,2,4)\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "ax3.grid()\n",
    "ax4.grid()\n",
    "#\n",
    "hh = ax1.hist(run_times, bins=50, cumulative=False, density=True, log=True)\n",
    "hh1c = ax2.hist(run_times, bins=50, cumulative=True, density=True, log=True, histtype='bar')\n",
    "#hh1c = ax2.plot(sorted(run_times), numpy.array(numpy.linspace(1./len(run_times), 1.,\n",
    "#                                                             len(run_times))), lw=3.0, zorder=11)\n",
    "#\n",
    "hh2 = ax3.hist(SACCT_obj.jobs_summary['NCPUS'], bins=50, density=True, log=True)\n",
    "hh22 = ax4.hist(SACCT_obj.jobs_summary['NCPUS'], bins=50, density=True, log=True,\n",
    "                 cumulative=True, histtype='bar', lw=3, zorder=11)\n",
    "#hh22 = ax4.hist(sacct_mazama.jobs_summary['NCPUS'], bins=50, normed=True, log=True,\n",
    "#                 cumulative=True, histtype='step', lw=3, zorder=11)\n",
    "#ax1.plot(run_times, ls='', marker='.')\n",
    "#print('*** ', run_times[0:20]*24, len(run_times), run_times.shape)\n",
    "#\n",
    "ax1.set_title('Run-time Distribution (days)')\n",
    "ax3.set_title('NCPUs Distribution')\n",
    "ax2.set_xlabel('Time (days)')\n",
    "ax4.set_xlabel('CPUS $N_{cpus}$')\n",
    "#\n",
    "ax2.set_title(\"(Cumulative)\")\n",
    "#ax4.set_title(\"(Cumulative)\")\n",
    "\n",
    "#ax2.set_xlim(-1,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', len(run_times), len(SACCT_obj.jobs_summary['NCPUS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ix_rt = numpy.argsort(run_times)\n",
    "run_times_sorted = run_times.copy()\n",
    "run_times_sorted.sort()\n",
    "#\n",
    "k_2 = numpy.searchsorted(run_times_sorted, 2.0)\n",
    "k_7 = numpy.searchsorted(run_times_sorted, 7.0)\n",
    "k_14 = numpy.searchsorted(run_times_sorted, 14.0)\n",
    "\n",
    "#\n",
    "# k_2 = numpy.searchsorted(run_times[ix_rt], 2.0)\n",
    "# #k_7 = numpy.searchsorted(run_times[ix_rt], 7.0)\n",
    "# k_7 = k_2 + numpy.searchsorted( (run_times[ix_rt])[k_2:], 7.0)\n",
    "# k_14 = k_7 + numpy.searchsorted( (run_times[ix_rt])[k_7:], 14.0)\n",
    "#\n",
    "N=float(len(run_times))\n",
    "print('*** quantiles for t=2,7,14 days: {}, {}, {}'.format(float(k_2)/N, k_7/N, k_14/N))\n",
    "#\n",
    "# percent of jobs that use N<24 cores (aka, can run on a single node). Note that these are especially\n",
    "#. eligible for GCP.\n",
    "N_24 = numpy.sum(SACCT_obj.jobs_summary['NCPUS']<25)\n",
    "print('*** N_24/N={}'.format(N_24/N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_stats = SACCT_obj.get_wait_stats(qs=[.25, .5, .75, .9])\n",
    "#\n",
    "fg = plt.figure(figsize=(10,10))\n",
    "ax1 = plt.gca()\n",
    "ax1.grid()\n",
    "#\n",
    "ax1.plot(wait_stats['ncpus'], wait_stats['mean']*60.*24, ls='-', label='mean')\n",
    "ax1.plot(wait_stats['ncpus'], wait_stats['median']*60.*24, ls='-', label='median')\n",
    "#\n",
    "ax1.set_ylabel('Wait time (minutes)')\n",
    "ax1.set_xlabel('$N_{CPUS}$', size=16)\n",
    "#ax1.set_ylim(-.1, .5)\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     with open(data_file_name, 'r') as fin:\n",
    "#         header_rw = fin.readline()\n",
    "#         print('*** ', header_rw)\n",
    "\n",
    "#     headers = header_rw.split('|')\n",
    "#     k_group = headers.index('Group')\n",
    "#     k_gid = headers.index('GID')\n",
    "#     #\n",
    "    \n",
    "#     with open(data_file_name, 'r') as fin:\n",
    "#         for k,rw in enumerate(fin):\n",
    "#             #if 'dunham' in rw: print('** ', rw)\n",
    "#             #if 'dunham' in rw:\n",
    "#             #    rws = rw.split('|')\n",
    "#             #    print('** ', rws[k_group], rws[k_gid])\n",
    "#             if 'Partition_Limit' in rw:\n",
    "#                 print('** ', rw)\n",
    "#                 k+=1\n",
    "#                 if k>10: break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ',SACCT_obj.jobs_summary.dtype.names)\n",
    "print('** ', mpd.num2date(SACCT_obj.jobs_summary['Submit'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drafts of some reports\n",
    "- We have a couple reports already made. Let's start with those.\n",
    "- Even for those reports, we'll want to modify the inputs, for example to specify all Earth users/groups, not juust look at the `serc` partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report(s)\n",
    "#\n",
    "# first, we need to make groups.\n",
    "# groups who can access serc:\n",
    "# (base) [myoder96@sh01-ln01 login ~]$ scontrol show partition serc | grep Allow\n",
    "#   AllowGroups=sh_s-ees,sh_sysadm AllowAccounts=ALL AllowQos=normal,high_p,system\n",
    "#\n",
    "# so all permissions are consolidated to the sh_s-ees group\n",
    "# to get users in a group:\n",
    "# USERS=`getent passwd | awk -F: -v g=$(id -g $PI_SUNET) '$4==g {print $1}'`\n",
    "#. but this does not seem to work for the sh_s-ees group\n",
    "# we can use id to get secondary groups (we'll need to pares...), for example,\n",
    "# $ (base) [myoder96@sh01-ln01 login ~/Codes/cees_scripts]$ id myoder96\n",
    "# uid=362778(myoder96) gid=32264(ruthm) groups=99001(sh_users),98013(sh_sw-stata),98008(sh_sw-schrodinger),97002(sh_s-ees),97005(sh_s-hns),1007582(oak_p-cees-backup),1008811(oak_s-ees),32264(ruthm)\n",
    "# this should do it:\n",
    "# getent group sh_s-ees\n",
    "# then maybe just parse in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PI_groups = hpc_lib.get_PI_groups_from_groups(groups='sh_s-ees')\n",
    "# #\n",
    "# for ky,vl in PI_groups.items():\n",
    "#     print('** {}, {}'.format(ky,vl))\n",
    "#     #\n",
    "# #\n",
    "# SACCT = hpc_lib.SACCT_data_direct(partition='serc', start_date='2021-01-01', end_date='2021-06-03',\n",
    "#                                   keep_raw_data=False)\n",
    "# #\n",
    "# rpt = hpc_lib.SACCT_groups_analyzer_report(Short_title='SERC HPC Analytics', \n",
    "#                 Full_title='HPC Analytics Breakdown for Stanford Earth, Sherlock SERC',\n",
    "#                 out_path='output/SERC_analytics_20210602', tex_filename='SERC_HPC_analytics.tex',\n",
    "#                                            groups=PI_groups, SACCT_obj=SACCT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEES Sherlock report(s)\n",
    "- Let's just get into this... What we need:\n",
    "- Standard(ish) report of SERC (and other) partition(s) usage\n",
    "- Mostly same standard report for each active PI group\n",
    "\n",
    "#### Standard report:\n",
    "- jobs,cpus\n",
    "- cpu-hours\n",
    "- Time-of-Day, Day-of-week usage\n",
    "- \n",
    "\n",
    "#### Basic workflow:\n",
    "- Get full SACCT data for `serc` partition\n",
    "- Run report on partition usage\n",
    "- Get active groups (group name + users?)\n",
    "- Fetch new SACCT_obj for each group\n",
    "- Run report for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SACCT_obj.write_hdf5('sacct_data/serc_2021_12_01_2022_01_31.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SACCT_obj = hpc_lib.SACCT_data_direct(partition='serc', start_date='2021-12-01', end_date='2022-01-31',\n",
    "#                                   keep_raw_data=False)\n",
    "# SACCT_obj.write_hdf5('sacct_data/serc_2021_12_01_2022_01_31.h5')\n",
    "\n",
    "serc_report = hpc_lib.SACCT_groups_analyzer_report(Short_title='SERC Analytics', Full_title='SERC HPC analytics',\n",
    "                                                  SACCT_obj=SACCT_obj,\n",
    "                                                  out_path='output/SERC_analytics_20220121',\n",
    "                                                   tex_filename='SERC_HPC_analytics.tex'\n",
    "                                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How to make reports with hpc_lib.SACCT_groups_analyzer_report_handler()\n",
    "#\n",
    "serc_report_h = hpc_lib.SACCT_groups_analyzer_report_handler(Short_title='SERC Analytics', Full_title='SERC HPC analytics',\n",
    "                                                  SACCT_obj=SACCT_obj,\n",
    "                                                  out_path='output/SERC_analytics_20220121_h',\n",
    "                                                   tex_filename='SERC_HPC_analytics.tex'\n",
    "                                                  )\n",
    "z = serc_report_h.standard_reports_slides(group_name='serc')\n",
    "\n",
    "#z = serc_report_h.standard_reports_slides(ix=(SACCT_obj.jobs_summary['Group']==b'biondo'), group_name='Biondo')\n",
    "\n",
    "zz = serc_report_h.HPC_tex_obj.render()\n",
    "#ix_g_ruthm = numpy.where([SACCT_obj.jobs_summary['User'] ])\n",
    "#ix_g_ruthm = (SACCT_obj.jobs_summary['Group']=='ruthm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SACCT_obj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hourlies = SACCT_obj.get_cpu_hours(bin_size=1./24., d_t=1/24., IX=None, verbose=0)\n",
    "# NOTE: because we have 1/24 d_t and bin_size, this should be equivalent to active_cpus, but it is not\n",
    "#. actually computed quite the same way.\n",
    "hourlies = hpc_lib.get_cpu_hours(bin_size=7., d_t=1/24., verbose=0, jobs_summary=SACCT_obj.jobs_summary)\n",
    "\n",
    "#hourlies = hpc_lib.get_cpu_hours(bin_size=1./24., n_points=None, d_t=1./24., verbose=0, jobs_summary=SACCT_obj.jobs_summary)\n",
    "\n",
    "print('** hourlies: ', hourlies['cpu_hours'][0:20]*24.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlies_cpu_job = hpc_lib.active_jobs_cpu(bin_size=1./24., jobs_summary=SACCT_obj.jobs_summary)\n",
    "hourlies_cpu_job['N_jobs'][numpy.isnan(hourlies_cpu_job['N_jobs'])] = 0.\n",
    "hourlies_cpu_job['N_cpu'][numpy.isnan(hourlies_cpu_job['N_cpu'])] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', hourlies_cpu_job.dtype)\n",
    "print(numpy.isnan(hourlies_cpu_job['N_jobs']).any() )\n",
    "print(numpy.isnan(hourlies_cpu_job['N_cpu']).any() )\n",
    "print(numpy.isnan(hourlies_cpu_job['time']).any() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know why these are not the same length... something to do with how cpu-hours are computed.\n",
    "print('lens: ', len(hourlies), len(hourlies_cpu_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: num2date(0) is a thursday, so datetime.weekday() == (t+3)%7 or (t-4)%7, so t%7 = {2,3} are sat, sun.\n",
    "ix_wknd   = numpy.logical_and( (hourlies['time'])%7 >= 2., (hourlies['time'])%7 <= 3. )\n",
    "ix_afters = numpy.logical_or(hourlies['time']%1.<.3 , hourlies['time']%1>.8)\n",
    "ix_dollies = numpy.logical_and(numpy.invert(ix_wknd), numpy.invert(ix_afters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_w   = numpy.logical_and( (hourlies_cpu_job['time'])%7 >= 2., (hourlies_cpu_job['time'])%7 <= 3. )\n",
    "#ix_a   = numpy.logical_or(hourlies_cpu_job['time']%1.<.3 , hourlies_cpu_job['time']%1>.8)\n",
    "ix_a   = numpy.logical_and( numpy.logical_or(hourlies_cpu_job['time']%1.<.33 , hourlies_cpu_job['time']%1>.75),\n",
    "                           numpy.invert(ix_w))\n",
    "ix_d   = numpy.logical_and(numpy.invert(ix_w), numpy.invert(ix_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', hourlies['cpu_hours'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('*** ', SACCT_obj.jobs_summary[0:10])\n",
    "# print('*** ', SACCT_obj.jobs_summary.dtype)\n",
    "# print('*** ', SACCT_obj.jobs_summary['Start'][0:10])\n",
    "#\n",
    "# what do dates look like? Turns out that the SLURM dates are TZ naive, but in local (PST) time.\n",
    "# so matplotlib just treats them like UTC, so our treatment should be fine...\n",
    "dt_str = '2021-12-08T11:05:18'\n",
    "dt_num = mpd.datestr2num(dt_str)\n",
    "print(f'** dt_num: {dt_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute running averages over a full week for the components:\n",
    "# dollies (9 to 5)\n",
    "# after hours (weekdays, not 9 to 5)\n",
    "# weekends.\n",
    "ave_bins = 7*24\n",
    "#\n",
    "hourlies_cake_total = running_mean(hourlies_cpu_job['N_cpu'], ave_bins)\n",
    "#\n",
    "X = hourlies_cpu_job['N_cpu'].copy()\n",
    "X[numpy.invert(ix_d)] = 0.\n",
    "hourlies_cake_9to5 = running_mean(X,ave_bins)\n",
    "#\n",
    "X = hourlies_cpu_job['N_cpu'].copy()\n",
    "X[numpy.invert(ix_w)] = 0.\n",
    "hourlies_cake_wknd = running_mean(X, ave_bins)\n",
    "#\n",
    "X = hourlies_cpu_job['N_cpu'].copy()\n",
    "X[numpy.invert(ix_a)] = 0.\n",
    "hourlies_cake_afters = running_mean(X, ave_bins)\n",
    "#\n",
    "\n",
    "# print('** ', hourlies['cpu_hours'][0:10]*24.)\n",
    "# print('** ', hourlies_cake_afters[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dtm.datetime.now()\n",
    "dt_64 = numpy.datetime64(dt)\n",
    "#\n",
    "print('** ', dt.weekday())\n",
    "print('** ', int((mpd.date2num(dt)+3)%7))\n",
    "#\n",
    "print('** ', isinstance(numpy.datetime64(dtm.datetime.now()), dtm.datetime))\n",
    "#\n",
    "print('*** ', mpd.date2num(dt), mpd.date2num(dt_64))\n",
    "#\n",
    "print('*** ', numpy.mod(numpy.arange(10),2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_steps = numpy.array([.25, .5, .75, .9])\n",
    "qs = numpy.array([.25, .5, .75, .9])\n",
    "print('** ', numpy.sum(hourlies_cpu_job['N_cpu']), numpy.sum(hourlies_cpu_job['N_cpu'][ix_d]),\n",
    "      numpy.sum(hourlies_cpu_job['N_cpu'][ix_a]), numpy.sum(hourlies_cpu_job['N_cpu'][ix_w]))\n",
    "#\n",
    "dtype_qs = [('time', '>i8')] + [(f'q{k+1}', '>f8') for k,x in enumerate(qs)] \n",
    "print('** dtype: ', dtype_qs)\n",
    "periodic_hourly_weekdays_cpus = numpy.zeros((24,), dtype=dtype_qs)\n",
    "periodic_hourly_weekdays_cpus['time'] = numpy.arange(24)\n",
    "print('** ', periodic_hourly_weekdays_cpus)\n",
    "print('** ', periodic_hourly_weekdays_cpus['time'])\n",
    "#\n",
    "for k in periodic_hourly_weekdays_cpus['time']:\n",
    "    # we are really interested in 9to5 activity, but since we're resolving hours, we might as well show \n",
    "    #. them. SO, we want to just exclude weekends, then do quantiles on daily slices.\n",
    "    q_vals = numpy.nanquantile(hourlies_cpu_job['N_cpu'][numpy.logical_and(numpy.invert(ix_w),\n",
    "                                    (24*(hourlies_cpu_job['time']%1)).astype(int)==k)], qs)\n",
    "    #print(f'** qs[{k}]: {q_vals}')\n",
    "    #\n",
    "    # what is this syntax????\n",
    "    #periodic_hourly_925_cpus[('q1', 'q2', 'q3')] = q_vals[:]\n",
    "    #if numpy.isnan(q_vals):\n",
    "    #    continue\n",
    "    for j,q in enumerate(q_vals):\n",
    "        periodic_hourly_weekdays_cpus[f'q{j+1}'][k]=q\n",
    "print('** **\\n', periodic_hourly_weekdays_cpus)\n",
    "\n",
    "periodic_daily_925_cpus = numpy.zeros((7,), dtype=dtype_qs)\n",
    "periodic_daily_cpus = numpy.zeros((7,), dtype=dtype_qs)\n",
    "periodic_daily_925_cpus['time'] = numpy.arange(7)\n",
    "periodic_daily_cpus['time'] = numpy.arange(7)\n",
    "#\n",
    "# daily usage:\n",
    "for k in periodic_daily_cpus['time']:\n",
    "    # here, we want do do quantiles on daily slices. Again, our principal interest is\n",
    "    #. 9to5 activity, so exclude after-hours (t_am > t < t_pm), but include weekends. \n",
    "#    q_vals_925 = numpy.nanquantile(hourlies_cpu_job['N_cpu'][numpy.logical_and(numpy.invert(ix_d),\n",
    "#                                    (((hourlies_cpu_job['time']+3)%7)).astype(int)==k)], qs)\n",
    "    # ix_a   = numpy.logical_and( numpy.logical_or(hourlies_cpu_job['time']%1.<.33 , hourlies_cpu_job['time']%1>.75),\n",
    "    #                       numpy.invert(ix_w))\n",
    "    ix_925 = numpy.logical_and(numpy.logical_and((hourlies_cpu_job['time']%1. >= .33),\n",
    "                                             (hourlies_cpu_job['time']%1 <= .75)),\n",
    "                           (((hourlies_cpu_job['time']+3)%7).astype(int)==k)\n",
    "                          )\n",
    "    #X = hourlies_cpu_job['N_cpu'][ix]\n",
    "    #print('*** sum(ix): ', numpy.sum(ix) )\n",
    "    q_vals_925 = numpy.nanquantile(hourlies_cpu_job['N_cpu'][ix_925], qs)\n",
    "    #\n",
    "#    q_vals     = numpy.nanquantile(hourlies_cpu_job['N_cpu'][numpy.logical_and(numpy.invert(ix_a),\n",
    "#                                    (((hourlies_cpu_job['time']+3)%7)).astype(int)==k)], qs)\n",
    "    q_vals     = numpy.nanquantile(hourlies_cpu_job['N_cpu'][(((hourlies_cpu_job['time']+3)%7)).astype(int)==k], qs)\n",
    "\n",
    "    #                                                             \n",
    "    #print(f'** ** q_vals_925[{k}]: {q_vals_925}')\n",
    "    for j,q in enumerate(q_vals):\n",
    "        periodic_daily_cpus[f'q{j+1}'][k]=q\n",
    "    for j,q in enumerate(q_vals_925):\n",
    "        periodic_daily_925_cpus[f'q{j+1}'][k]=q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('** ', ix_dollies[0:100])\n",
    "fg = plt.figure(figsize=(14,15))\n",
    "n_cols = 2\n",
    "n_rws  = 4\n",
    "ax1 = plt.subplot(n_rws,1,1)\n",
    "ax2 = plt.subplot(n_rws,1,2, sharex=ax1)\n",
    "ax3 = plt.subplot(n_rws,n_cols,5)\n",
    "ax4 = plt.subplot(n_rws,n_cols,6)\n",
    "ax5 = plt.subplot(n_rws,n_cols,7, projection='polar')\n",
    "ax6 = plt.subplot(n_rws,n_cols,8, projection='polar')\n",
    "#\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "ax3.grid()\n",
    "ax4.grid()\n",
    "ax5.grid()\n",
    "ax6.grid()\n",
    "#\n",
    "fg.suptitle('CPU Usage Report', size=16)\n",
    "ax1.set_title('Active CPUs')\n",
    "ax2.set_title('Active CPUs layer cake')\n",
    "ax3.set_title('CPUs Active: Hourly, weekdays')\n",
    "ax4.set_title('CPUs Active: DoW, 9-5')\n",
    "#\n",
    "# ix=numpy.logical_and( T%1.>.3 , T%1<.8, (T-4)%7)\n",
    "# ax.plot(hourlies['time'], hourlies['cpu_hours'])\n",
    "# ax.plot(hourlies['time'][ix], hourlies['cpu_hours'][ix], ls='', marker='.')\n",
    "ax1.plot(hourlies_cpu_job['time'], hourlies_cpu_job['N_cpu'], color='b', lw=1.)\n",
    "#ax1.fill_between(hourlies_cpu_job['time'], 0., hourlies_cpu_job['N_cpu'], color='b', alpha=.2, zorder=1)\n",
    "X = hourlies_cpu_job['N_cpu'].copy()\n",
    "X[numpy.invert(ix_d)]=0.\n",
    "ax1.fill_between(hourlies_cpu_job['time'], 0., X, alpha=.2, label='9 to 5')\n",
    "#\n",
    "X = hourlies_cpu_job['N_cpu'].copy()\n",
    "X[numpy.invert(ix_w)]=0.\n",
    "ax1.fill_between(hourlies_cpu_job['time'], 0., X, alpha=.2, label='weekends')\n",
    "#\n",
    "X = hourlies_cpu_job['N_cpu'].copy()\n",
    "X[numpy.invert(ix_a)]=0.\n",
    "X[ix_w]=0.\n",
    "ax1.fill_between(hourlies_cpu_job['time'], 0., X, alpha=.2, label='after-hours')\n",
    "# x_lbls = [mpd.num2date(x) for x in ax1.get_xticklabels.text]\n",
    "# ax1.set_xticklabels([f'{x.year}-{x.month}-{x.day}' for x in x_lbls])\n",
    "ax1.legend(loc=0)\n",
    "#\n",
    "\n",
    "# ax1.plot(hourlies_cpu_job['time'][ix_d], hourlies_cpu_job['N_cpu'][ix_d], ls='', marker='.')\n",
    "# ax1.plot(hourlies_cpu_job['time'][ix_w], hourlies_cpu_job['N_cpu'][ix_w], ls='', marker='.')\n",
    "# ax1.plot(hourlies_cpu_job['time'][ix_a], hourlies_cpu_job['N_cpu'][ix_a], ls='', marker='.')\n",
    "#\n",
    "T = hourlies_cpu_job['time']\n",
    "ln, = ax2.plot(T[ave_bins-1:], hourlies_cake_total, ls='-', color='r', label='total')\n",
    "#ax2.plot(T, hourlies['cpu_hours'], ls='-', marker='')\n",
    "X = hourlies_cake_wknd.copy()\n",
    "ln, = ax2.plot(T[ave_bins-1:], X, ls='--', label='weekends')\n",
    "clr = ln.get_color()\n",
    "ax2.fill_between(T[ave_bins-1:], 0., X, color=clr, alpha=.2 )\n",
    "#\n",
    "X0 = X.copy()\n",
    "X += hourlies_cake_afters\n",
    "ln, = ax2.plot(T[ave_bins-1:], X, ls='--', label='after-hours')\n",
    "clr = ln.get_color()\n",
    "ax2.fill_between(T[ave_bins-1:], X0, X, color=clr, alpha=.2 )\n",
    "#\n",
    "X0 = X.copy()\n",
    "X += hourlies_cake_9to5\n",
    "ln, = ax2.plot(T[ave_bins-1:], X, ls='--', marker='', label='9to5')\n",
    "clr = ln.get_color()\n",
    "ax2.fill_between(T[ave_bins-1:], X0, X, color=clr, alpha=.2 )\n",
    "#\n",
    "ax2.legend(loc=0)\n",
    "fg.canvas.draw()\n",
    "#print('** ** DEBUG: ', [s.get_text() for s in ax2.get_xticklabels()])\n",
    "lbls = [hpc_lib.simple_date_string(mpd.num2date(max(1, float(s.get_text())) ) ) for s in ax2.get_xticklabels()]\n",
    "ax2.set_xticklabels(lbls)\n",
    "#\n",
    "ln, = ax3.plot(periodic_hourly_weekdays_cpus['time'], periodic_hourly_weekdays_cpus['q2'], lw=3)\n",
    "clr = ln.get_color()\n",
    "ax3.fill_between(periodic_hourly_weekdays_cpus['time'], periodic_hourly_weekdays_cpus['q2'],\n",
    "                 periodic_hourly_weekdays_cpus['q3'], color=clr, alpha=.15 )\n",
    "ax3.fill_between(periodic_hourly_weekdays_cpus['time'], periodic_hourly_weekdays_cpus['q2'],\n",
    "                 periodic_hourly_weekdays_cpus['q4'], color=clr, alpha=.15 )\n",
    "\n",
    "xx = numpy.min(numpy.append([1000], periodic_hourly_weekdays_cpus['q1']))\n",
    "ax3.set_ylim(xx, 5000)\n",
    "#\n",
    "X = periodic_daily_925_cpus.copy()\n",
    "ax4.plot(X['time'], X['q2'], lw=3, color='b', label='9to5')\n",
    "ax4.fill_between(X['time'], X['q2'], X['q3'], color='b', alpha=.15 )\n",
    "ax4.fill_between(X['time'], X['q1'], X['q4'], color='b', alpha=.15 )\n",
    "X = periodic_daily_cpus\n",
    "ax4.plot(X['time'], X['q1'], lw=3, color='m', ls='--', label='all')\n",
    "ax4.fill_between(X['time'], X['q2'], X['q4'], color='m', alpha=.08)\n",
    "ax4.legend(loc=0)\n",
    "#\n",
    "ax5.set_theta_direction(-1)\n",
    "ax5.set_theta_offset(math.pi/2.0)\n",
    "X = periodic_hourly_weekdays_cpus['time'].copy()*scipy.constants.pi*2.0/float(len(periodic_hourly_weekdays_cpus))\n",
    "ln, = ax5.plot(X, periodic_hourly_weekdays_cpus['q2'], lw=3, marker='.')\n",
    "clr = ln.get_color()\n",
    "ax5.plot(numpy.linspace(0., math.pi*2., 100), numpy.ones(100)*numpy.mean(periodic_hourly_weekdays_cpus['q2']),\n",
    "         ls='--', lw=2., color=clr, alpha=.7, label='mean, 50th' )\n",
    "ax5.set_xticks(numpy.arange(0., math.pi*2., math.pi*2/24.))\n",
    "ax5.set_xticklabels(numpy.arange(0,24, 1.))\n",
    "y_min = .5*(numpy.max(periodic_hourly_weekdays_cpus['q1'])+numpy.min(periodic_hourly_weekdays_cpus['q2']))\n",
    "ax5.set_ylim(y_min, )\n",
    "ax5.grid()\n",
    "ax5.legend(loc='upper right')\n",
    "#\n",
    "# TODO: figure out phase of DoW circle-plot.\n",
    "ax6.set_theta_direction(-1)\n",
    "ax6.set_theta_offset(math.pi/2.0)\n",
    "X = periodic_daily_925_cpus.copy()\n",
    "ln, = ax6.plot(X['time']*math.pi*2.0/float(len(X)+1.), X['q2'], lw=3, marker='o', label='9to5')\n",
    "clr = ln.get_color()\n",
    "ax6.plot(numpy.linspace(0., math.pi*2., 100), numpy.ones(100)*numpy.mean(X['q2']), ls='--', lw=2., \n",
    "         color=clr, alpha=.7 , label='mean, 50th')\n",
    "ax6.set_xticks=numpy.arange(0., math.pi*2., math.pi*2./7.)\n",
    "ax6.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "y_min = .5*(numpy.max(X['q1']) + numpy.min(X['q2']))\n",
    "ax6.set_ylim(y_min, )\n",
    "ax6.grid()\n",
    "ax6.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', hourlies['cpu_hours'][0:10])\n",
    "print('** ', hourlies_cake_9to5[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in hourlies['time'][0:250]:\n",
    "#     d = mpd.num2date(t)\n",
    "#     print('** time: {}: hour: {}, weekday: {}/{}/{}::{}'.format(t, d.hour, d.weekday(), int(t%7), \n",
    "#                                                                 int(t+3)%7, int(t-4)%7 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "out_path = 'output/SERC_analytics'\n",
    "#\n",
    "rpt = hpc_reports.SACCT_report_handler(SACCT_obj=SACCT_obj, out_path='output/SERC_analytics')\n",
    "#\n",
    "fpath_cpu_activity_serc = rpt.cpu_hourly_activity_report(fout_path_name='')\n",
    "fpath_wait_stats = rpt.wait_stats_report(fout_path_name='')\n",
    "#\n",
    "# TODO: need to get relative paths correct here. render() defaults to render from the the loal path.\n",
    "#. use os.path.relpath(path_to, path_from) and os.lpath.abspath() to code the paths into the \n",
    "#. report definition.\n",
    "rpt.HPC_tex_obj.add_fig_slide(fig_title='CPU Activity: SERC', fig_path=fpath_cpu_activity_serc)\n",
    "rpt.HPC_tex_obj.add_fig_slide(fig_title='Wait Stats: SERC', fig_path=fpath_wait_stats)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** ', fpath_cpu_activity_serc, os.path.abspath(fpath_cpu_activity_serc))\n",
    "print('*** ', fpath_wait_stats)\n",
    "print('*** ', os.path.join('abc', 'def', 'ghi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt.HPC_tex_obj.render()\n",
    "#rpt.HPC_tex_obj.save_presentation_tex(os.path.join(out_path, 'SERC_usage.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait Stats figure/report:\n",
    "- Do a wait-time timeseries (just to see how it looks), maybe with some averaging.\n",
    "- Wait time quantiles, again by day, hour, 9to5, and for a few ranges of n_cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait_stats      = SACCT_obj.get_wait_stats(qs=[.25, .5, .75, .9])\n",
    "# for col in wait_stats.dtype.names[1:]:\n",
    "#     wait_stats[col]*=24.\n",
    "#wait_stats_ncpu = SACCT_obj.get_wait_times_per_ncpu()\n",
    "\n",
    "wait_times = 60.*24.*(SACCT_obj.jobs_summary['Start'] - SACCT_obj.jobs_summary['Submit'])\n",
    "# wait_time_units = 'hours'\n",
    "# wait_times *= 60.\n",
    "wait_time_units='minutes'\n",
    "#\n",
    "\n",
    "wait_times_per_dow = hpc_lib.day_of_week_distribution(time=SACCT_obj.jobs_summary['Submit'], Y=wait_times)\n",
    "wait_times_per_hod = hpc_lib.hour_of_day_distribution(time=SACCT_obj.jobs_summary['Submit'], Y=wait_times)\n",
    "\n",
    "#\n",
    "qs_ps = [.25, .5, .76, .9]\n",
    "qs_waittimes = numpy.nanquantile(wait_times,qs_ps)\n",
    "print('*** Wait-time quantiles: {}'.format(qs_waittimes))\n",
    "ix_q4 = numpy.logical_and(numpy.isnan(wait_times)==False, wait_times<qs_waittimes[-1])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait stats on serc are basically nothing. Let's see if we can get something interesting for\n",
    "# ncpus: <=10, 10<n<100, n>100\\\n",
    "#\n",
    "# kx_wait_10 = SACCT_obj.jobs_summary['NCPUS']<=10\n",
    "# kx_wait_100 = numpy.logical_and(SACCT_obj.jobs_summary['NCPUS']>10,  SACCT_obj.jobs_summary['NCPUS']<=100)\n",
    "# kx_wait_101 = SACCT_obj.jobs_summary['NCPUS']>100\n",
    "#\n",
    "fg = plt.figure(figsize=(12,10))\n",
    "ax1a = plt.subplot(3,2,1, projection='polar')\n",
    "ax1a.set_theta_direction(-1)\n",
    "ax1a.set_theta_offset(math.pi/2.0)\n",
    "ax1a.set_title('Wait-Time DoW quantiles (hours)')\n",
    "#\n",
    "ax1b = plt.subplot(3,2,2, projection='polar')\n",
    "ax1b.set_theta_direction(-1)\n",
    "ax1b.set_theta_offset(math.pi/2.0)\n",
    "ax1b.set_title('Wait-Time ToD quantiles (hours)')\n",
    "#\n",
    "ax2a = plt.subplot(3,2,3)\n",
    "ax2a.set_title('Wait-Time DoW (clock plot)')\n",
    "ax2b = plt.subplot(3,2,4)\n",
    "ax2b.set_title('Wait-Time ToD (clock plit)')\n",
    "\n",
    "ax3 = plt.subplot(3,2,5)\n",
    "ax4 = plt.subplot(3,2,6)\n",
    "\n",
    "#\n",
    "X_dow = wait_times_per_dow['DoW']*math.pi*2./7.\n",
    "ln, = ax1a.plot(X_dow, wait_times_per_dow['q2'], ls='-', marker='o')\n",
    "clr = ln.get_color()\n",
    "ax1a.fill_between(X_dow, wait_times_per_dow['q2'], wait_times_per_dow['q3'], color=clr, alpha=.2)\n",
    "ax1a.set_xticks(X_dow)\n",
    "ax1a.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "#\n",
    "X_hod = wait_times_per_hod['hour']*math.pi*2./float(len(wait_times_per_hod))\n",
    "ln, = ax1b.plot(X_hod, wait_times_per_hod['q2'], ls='-', marker='o')\n",
    "clr = ln.get_color()\n",
    "ax1b.fill_between(X_hod, wait_times_per_hod['q2'], wait_times_per_hod['q3'], color=clr, alpha=.2)\n",
    "ax1b.set_xticks(X_hod )\n",
    "ax1b.set_xticklabels(numpy.arange(0,24,1))\n",
    "#\n",
    "X_dow = wait_times_per_dow['DoW']\n",
    "ln, = ax2a.plot(X_dow, wait_times_per_dow['q2'], ls='-', marker='o')\n",
    "clr = ln.get_color()\n",
    "ax2a.fill_between(X_dow, wait_times_per_dow['q2'], wait_times_per_dow['q3'], color=clr, alpha=.2)\n",
    "ax2a.set_xticks(numpy.arange(0, 7) )\n",
    "ax2a.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "ax2a.set_ylabel(wait_time_units.capitalize())\n",
    "#\n",
    "X_hod = wait_times_per_hod['hour']\n",
    "ln, = ax2b.plot(X_hod, wait_times_per_hod['q2'], ls='-', marker='o')\n",
    "clr = ln.get_color()\n",
    "ax2b.fill_between(X_hod, wait_times_per_hod['q2'], wait_times_per_hod['q3'], color=clr, alpha=.2)\n",
    "\n",
    "qs_wait = numpy.quantile(wait_times[numpy.invert(numpy.isnan(wait_times))], qs_ps)\n",
    "print('*** ', qs_wait)\n",
    "print('*** wait time quantiles: ')\n",
    "for q_p, q_v in zip(qs_ps, qs_wait):\n",
    "    print(f'** q[{q_p}]: {q_v*60.} ')\n",
    "print('*** mean: {}'.format(numpy.mean(wait_times[numpy.invert(numpy.isnan(wait_times))])))\n",
    "#\n",
    "\n",
    "h1 = ax3.hist(wait_times[ix_q4], bins=1000, density=True)\n",
    "#ax3.set_xlim(0., 1.)\n",
    "ax3.set_title(f'Wait time PDF of {qs_ps[3]:.2f} quantile')\n",
    "ax3.set_xlabel(wait_time_units.capitalize())\n",
    "ax3.set_ylabel('Percent')\n",
    "#\n",
    "h2 = ax4.hist(wait_times[ix_q4], bins=100, cumulative=True, density=True, histtype='step')\n",
    "h2 = ax4.hist(wait_times[ix_q4], bins=100, cumulative=True, density=True, histtype='stepfilled', alpha=.2)\n",
    "#ax4.set_xlim(0, 1.)\n",
    "ax4.set_title(f'Wait time CDF of {qs_ps[3]:.2f} quantile')\n",
    "ax4.set_xlabel(wait_time_units.capitalize())\n",
    "ax4.set_ylabel('Percent')\n",
    "#\n",
    "for ax in (ax2a, ax2b, ax3, ax4):\n",
    "    ax.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just write explicit functions for periodic usage of cpus, jobs, other-stuff-too?\n",
    "# We could use the existing tools to modulus-bin time-series, but I think it won't cost that much\n",
    "# to just do it explicitly and (more) exactly. Trying to match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('** ', wait_times[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', wait_stats[0:10])\n",
    "print('** ', wait_stats.dtype.names)\n",
    "print('** ', wait_stats.qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
